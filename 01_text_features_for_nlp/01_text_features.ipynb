{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01.Text Vectorization (Draft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 차례\n",
    "* Frequency Vectors\n",
    "* One-Hot Encoding\n",
    "* Term Frequency-Inverse Document Frequency\n",
    "* Distributed Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://nbviewer.jupyter.org/github/psygrammer/psyml/blob/master/nlp_ml/ch01/figures/cap1.3.png\" width=600 />\n",
    "<img src=\"https://nbviewer.jupyter.org/github/psygrammer/psyml/blob/master/nlp_ml/ch01/figures/cap1.4.png\" width=600 />\n",
    "\n",
    "* 출처 - Applied Text Analysis with Python: Enabling Language-Aware Data Products with Machine Learning - https://www.amazon.com/Applied-Text-Analysis-Python-Language-Aware/dp/1491963042/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/main_nlp_01.png\" width=600 />\n",
    "<img src=\"figures/main_nlp_02.png\" width=600 />\n",
    "<img src=\"figures/main_nlp_03.png\" width=600 />\n",
    "<img src=\"figures/main_nlp_04.png\" width=600 />\n",
    "<img src=\"figures/main_nlp_05.png\" width=600 />\n",
    "\n",
    "* 출처 - Natural Language Processing - https://www.coursera.org/learn/language-processing - Main approaches in NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://nbviewer.jupyter.org/github/psygrammer/psyml/blob/master/nlp_ml/ch04/figures/cap01.png\" width=600 />\n",
    "\n",
    "* 출처 - Applied Text Analysis with Python: Enabling Language-Aware Data Products with Machine Learning - https://www.amazon.com/Applied-Text-Analysis-Python-Language-Aware/dp/1491963042/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://slideplayer.com/slide/5270778/17/images/10/Distributed+Word+Representation.jpg\" width=600 />\n",
    "\n",
    "* 출처 - https://slideplayer.com/slide/5270778/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frequency Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://nbviewer.jupyter.org/github/psygrammer/psyml/blob/master/nlp_ml/ch04/figures/cap02.png\" width=600 />\n",
    "\n",
    "* 출처 - Applied Text Analysis with Python: Enabling Language-Aware Data Products with Machine Learning - https://www.amazon.com/Applied-Text-Analysis-Python-Language-Aware/dp/1491963042/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gensim을 이용하려면 문서 목록인 documents를 corpus 클래스로 바꿔야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가상의 문서 4개\n",
    "documents_en = [\n",
    "    \"a b c a\",\n",
    "    \"c b c\",\n",
    "    \"b b a\",\n",
    "    \"a c c\",\n",
    "    \"c b a\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['a', 'b', 'c', 'a'],\n",
      " ['c', 'b', 'c'],\n",
      " ['b', 'b', 'a'],\n",
      " ['a', 'c', 'c'],\n",
      " ['c', 'b', 'a']]\n"
     ]
    }
   ],
   "source": [
    "# 단어(토큰) 단위로 분할\n",
    "def tokenize(x) :\n",
    "    for token in x.split() :\n",
    "        yield token\n",
    "        \n",
    "corpus_en = [list(tokenize(doc)) for doc in  documents_en]\n",
    "\n",
    "pprint(corpus_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary 객체. 구체적으로는 단어에 id 할당 (그 외에도 여러가지 기능은 튜토리얼 참조)\n",
    "dictionary_en = gensim.corpora.Dictionary(corpus_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.corpora.dictionary.Dictionary at 0x1a173c9048>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 0, 'b': 1, 'c': 2}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary_en.token2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = [dictionary_en.doc2bow(doc) for doc in corpus_en]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 2), (1, 1), (2, 1)],\n",
      " [(1, 1), (2, 2)],\n",
      " [(0, 1), (1, 2)],\n",
      " [(0, 1), (2, 2)],\n",
      " [(0, 1), (1, 1), (2, 1)]]\n"
     ]
    }
   ],
   "source": [
    "pprint(vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://nbviewer.jupyter.org/github/psygrammer/psyml/blob/master/nlp_ml/ch04/figures/cap03.png\" width=600 />\n",
    "\n",
    "* 출처 - Applied Text Analysis with Python: Enabling Language-Aware Data Products with Machine Learning - https://www.amazon.com/Applied-Text-Analysis-Python-Language-Aware/dp/1491963042/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['a', 'b', 'c', 'a'],\n",
       " ['c', 'b', 'c'],\n",
       " ['b', 'b', 'a'],\n",
       " ['a', 'c', 'c'],\n",
       " ['c', 'b', 'a']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 2), (1, 1), (2, 1)],\n",
       " [(1, 1), (2, 2)],\n",
       " [(0, 1), (1, 2)],\n",
       " [(0, 1), (2, 2)],\n",
       " [(0, 1), (1, 1), (2, 1)]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[dictionary_en.doc2bow(doc) for doc in corpus_en]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 2), (1, 1), (2, 1)],\n",
       " [(1, 1), (2, 2)],\n",
       " [(0, 1), (1, 2)],\n",
       " [(0, 1), (2, 2)],\n",
       " [(0, 1), (1, 1), (2, 1)]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[token for token in dictionary_en.doc2bow(doc)] \n",
    "                         for doc in corpus_en]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 2), (1, 1), (2, 1)],\n",
       " [(1, 1), (2, 2)],\n",
       " [(0, 1), (1, 2)],\n",
       " [(0, 1), (2, 2)],\n",
       " [(0, 1), (1, 1), (2, 1)]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[(token[0], token[1]) for token in dictionary_en.doc2bow(doc)] \n",
    "                         for doc in corpus_en]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = [[(token[0], 1) for token in dictionary_en.doc2bow(doc)] \n",
    "                         for doc in corpus_en]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors  = np.array([\n",
    "        [(token[0], 1) for token in dictionary_en.doc2bow(doc)]\n",
    "        for doc in corpus_en\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list([(0, 1), (1, 1), (2, 1)]), list([(1, 1), (2, 1)]),\n",
       "       list([(0, 1), (1, 1)]), list([(0, 1), (2, 1)]),\n",
       "       list([(0, 1), (1, 1), (2, 1)])], dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Term Frequency-Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://nbviewer.jupyter.org/github/psygrammer/psyml/blob/master/nlp_ml/ch04/figures/cap04.png\" width=600 />\n",
    "\n",
    "* 출처 - Applied Text Analysis with Python: Enabling Language-Aware Data Products with Machine Learning - https://www.amazon.com/Applied-Text-Analysis-Python-Language-Aware/dp/1491963042/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a b c a', 'c b c', 'b b a', 'a c c', 'c b a']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['a', 'b', 'c', 'a'],\n",
       " ['c', 'b', 'c'],\n",
       " ['b', 'b', 'a'],\n",
       " ['a', 'c', 'c'],\n",
       " ['c', 'b', 'a']]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary 생성\n",
    "dictionary_en = gensim.corpora.Dictionary(corpus_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.tfidfmodel.TfidfModel at 0x1a173e7550>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tfidf Model 생성\n",
    "tfidf_en = gensim.models.TfidfModel(dictionary=dictionary_en, normalize=True)\n",
    "tfidf_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 0.816496580927726), (1, 0.408248290463863), (2, 0.408248290463863)],\n",
       " [(1, 0.447213595499958), (2, 0.894427190999916)],\n",
       " [(0, 0.447213595499958), (1, 0.894427190999916)],\n",
       " [(0, 0.447213595499958), (2, 0.894427190999916)],\n",
       " [(0, 0.5773502691896257), (1, 0.5773502691896257), (2, 0.5773502691896257)]]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors = [tfidf_en[dictionary_en.doc2bow(vector)] for vector in corpus_en]\n",
    "vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary와 tfidf model을 저장해 놓으면 나중에 다시 로드해서 쓸 수 있어 편하다. \n",
    "dictionary_en.save_as_text('test_en.txt')\n",
    "tfidf_en.save('tfidf_en.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_en.txt\r\n"
     ]
    }
   ],
   "source": [
    "%ls test_en.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfidf_en.pkl\r\n"
     ]
    }
   ],
   "source": [
    "%ls tfidf_en.pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 한국어 예제 실습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 출처 - 나만의 웹 크롤러 만들기 with Requests/BeautifulSoup - https://beomi.github.io/2017/01/20/HowToMakeWebCrawler/\n",
    "* 출처 - https://www.slideshare.net/kimhyunjoonglovit/pycon2017-koreannlp\n",
    "* 출처 - https://github.com/lovit/soynlp/blob/master/tutorials/nounextractor-v2_usage.ipynb\n",
    "* 출처 - https://wikidocs.net/24603"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 데이터 수집 - 간단한 웹 스크래핑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#url = \"https://gasazip.com/view.html?no=614736\"\n",
    "#url = \"https://gasazip.com/view.html?no=636135\"\n",
    "url = \"http://gasazip.com/view.html?no=2276458\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTTP GET Request\n",
    "req = requests.get(url)\n",
    "# HTML 소스 가져오기\n",
    "html = req.text\n",
    "# BeautifulSoup으로 html소스를 python객체로 변환하기\n",
    "# 첫 인자는 html소스코드, 두 번째 인자는 어떤 parser를 이용할지 명시.\n",
    "# 여기서는 Python 내장 html.parser를 이용했다.\n",
    "soup = BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics = []\n",
    "for txt in soup.find_all('div', attrs={'class': 'col-md-8'}) :\n",
    "    lines = txt.get_text().split('\\n')\n",
    "    for line in lines :\n",
    "        lyrics.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['작은 것들을 위한 시 (Boy With Luv)Feat.Halsey',\n",
       " '',\n",
       " '모든 게 궁금해',\n",
       " 'How’s your day',\n",
       " 'Oh tell me',\n",
       " '뭐가 널 행복하게 하는지',\n",
       " 'Oh text me',\n",
       " 'Your every picture',\n",
       " '내 머리맡에 두고 싶어',\n",
       " 'oh bae',\n",
       " 'Come be my teacher',\n",
       " '네 모든 걸 다 가르쳐줘',\n",
       " 'Your 1 your 2',\n",
       " 'Listen my my baby 나는',\n",
       " '저 하늘을 높이 날고 있어',\n",
       " '그때 니가 내게 줬던 두 날개로',\n",
       " '이제 여긴 너무 높아',\n",
       " '난 내 눈에 널 맞추고 싶어',\n",
       " 'Yeah you makin’ me a boy with luv',\n",
       " 'Oh my my my oh my my my',\n",
       " \"I've waited all my life\",\n",
       " '네 전부를 함께하고 싶어',\n",
       " 'Oh my my my oh my my my',\n",
       " 'Looking for something right',\n",
       " '이제 조금은 나 알겠어',\n",
       " 'I want something stronger',\n",
       " 'Than a moment',\n",
       " 'than a moment love',\n",
       " 'I have waited longer',\n",
       " 'For a boy with',\n",
       " 'For a boy with luv',\n",
       " '널 알게 된 이후 ya',\n",
       " '내 삶은 온통 너 ya',\n",
       " '사소한 게 사소하지 않게',\n",
       " '만들어버린 너라는 별',\n",
       " '하나부터 열까지 모든 게 특별하지',\n",
       " '너의 관심사 걸음걸이 말투와',\n",
       " '사소한 작은 습관들까지',\n",
       " '다 말하지 너무 작던',\n",
       " '내가 영웅이 된 거라고',\n",
       " 'Oh nah',\n",
       " '난 말하지 운명 따윈',\n",
       " '처음부터 내 게 아니었다고',\n",
       " 'Oh nah',\n",
       " '세계의 평화',\n",
       " 'No way',\n",
       " '거대한 질서',\n",
       " 'No way',\n",
       " '그저 널 지킬 거야 난',\n",
       " 'Boy with luv',\n",
       " 'Listen my my baby 나는',\n",
       " '저 하늘을 높이 날고 있어',\n",
       " '그때 니가 내게 줬던 두 날개로',\n",
       " '이제 여긴 너무 높아',\n",
       " '난 내 눈에 널 맞추고 싶어',\n",
       " 'Yeah you makin’ me a boy with luv',\n",
       " 'Oh my my my oh my my my',\n",
       " 'You got me high so fast',\n",
       " '네 전부를 함께하고 싶어',\n",
       " 'Oh my my my oh my my my',\n",
       " 'You got me fly so fast',\n",
       " '이제 조금은 나 알겠어',\n",
       " 'Love is nothing stronger',\n",
       " 'Than a boy with luv',\n",
       " 'Love is nothing stronger',\n",
       " 'Than a boy with luv',\n",
       " '툭 까놓고 말할게',\n",
       " '나도 모르게 힘이 들어가기도 했어',\n",
       " '높아버린 sky',\n",
       " '커져버린 hall',\n",
       " '때론 도망치게 해달라며 기도했어',\n",
       " 'But 너의 상처는 나의 상처',\n",
       " '깨달았을 때 나 다짐했던걸',\n",
       " '니가 준 이카루스의 날개로',\n",
       " '태양이 아닌 너에게로',\n",
       " 'Let me fly',\n",
       " 'Oh my my my oh my my my',\n",
       " \"I've waited all my life\",\n",
       " '네 전부를 함께하고 싶어',\n",
       " 'Oh my my my oh my my my',\n",
       " 'Looking for something right',\n",
       " '이제 조금은 나 알겠어',\n",
       " 'I want something stronger',\n",
       " 'Than a moment',\n",
       " 'than a moment love',\n",
       " 'Love is nothing stronger',\n",
       " 'Than a boy with luv',\n",
       " '']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하나로는 좀 부족한 듯. 가사를 여럿 모아보자.\n",
    "\n",
    "import collections\n",
    " \n",
    "BTS = collections.OrderedDict()\n",
    "BTS['No More Dream'] = 638198\n",
    "BTS['I NEED U'] = 576129\n",
    "BTS['쩔어'] = 628809\n",
    "BTS['봄날'] = 644668\n",
    "BTS['DNA'] = 1235594\n",
    "BTS['피 땀 눈물'] = 630845\n",
    "BTS['불타오르네'] = 615362\n",
    "BTS['IDOL'] = 2262224\n",
    "BTS['작은 것들을 위한 시'] = 2276458"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['http://gasazip.com/view.html?no=638198',\n",
      " 'http://gasazip.com/view.html?no=576129',\n",
      " 'http://gasazip.com/view.html?no=628809',\n",
      " 'http://gasazip.com/view.html?no=644668',\n",
      " 'http://gasazip.com/view.html?no=1235594',\n",
      " 'http://gasazip.com/view.html?no=630845',\n",
      " 'http://gasazip.com/view.html?no=615362',\n",
      " 'http://gasazip.com/view.html?no=2262224',\n",
      " 'http://gasazip.com/view.html?no=2276458']\n"
     ]
    }
   ],
   "source": [
    "lyrics_ids = BTS.values()\n",
    "url_root = \"http://gasazip.com/view.html?no=\"\n",
    "\n",
    "urls = [url_root + str(lid) for lid in lyrics_ids]\n",
    "\n",
    "pprint(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "for url in urls :\n",
    "    req = requests.get(url)\n",
    "    html = req.text\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    doc = []\n",
    "    for txt in soup.find_all('div', attrs={'class': 'col-md-8'}) :\n",
    "        doc.append(txt.get_text())\n",
    "    \n",
    "    docs.append(\",\".join(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['No More Dream,\\n얌마 네 꿈은 뭐니\\n얌마 네 꿈은 뭐니\\n얌마 네 꿈은 뭐니\\n네 꿈은 겨우 그거니\\n\\nI wanna big house, big cars & big rings\\nBut 사실은 I dun have any big dreams\\n하하 난 참 편하게 살어\\n꿈 따위 안 꿔도 아무도 뭐라 안 하잖어\\n전부 다다다 똑같이 나처럼 생각하고 있어\\n새까까까맣게 까먹은 꿈 많던 어린 시절\\n대학은 걱정 마 멀리라도 갈 거니까\\n알았어 엄마 지금 독서실 간다니까\\n\\n네가 꿈꿔 온 네 모습이 뭐야\\n지금 네 거울 속엔 누가 보여, I gotta say\\n너의 길을 가라고\\n단 하루를 살아도\\n뭐라도 하라고\\n나약함은 담아 둬\\n\\n왜 말 못하고 있어? 공부는 하기 싫다면서\\n학교 때려 치기는 겁나지? 이거 봐 등교할 준비하네 벌써\\n철 좀 들어 제발 좀, 너 입만 살아가지고 인마 유리 멘탈 boy\\n(Stop!) 자신에게 물어봐 언제 네가 열심히 노력했냐고\\n\\n얌마 네 꿈은 뭐니\\n얌마 네 꿈은 뭐니\\n얌마 네 꿈은 뭐니\\n네 꿈은 겨우 그거니\\n\\n거짓말이야 you such a liar\\nSee me see me ya 넌 위선자여\\n왜 자꾸 딴 길을 가래 야 너나 잘해\\n제발 강요하진 말아 줘\\n(La la la la la) 네 꿈이 뭐니 네 꿈이 뭐니 뭐니\\n(La la la la la) 고작 이거니 고작 이거니 거니\\n\\n지겨운 same day, 반복되는 매일에\\n어른들과 부모님은 틀에 박힌 꿈을 주입해\\n장래희망 넘버원... 공무원?\\n강요된 꿈은 아냐, 9회말 구원투수\\n시간 낭비인 야자에 돌직구를 날려\\n지옥 같은 사회에 반항해, 꿈을 특별 사면\\n자신에게 물어봐 네 꿈의 profile\\n억압만 받던 인생 네 삶의 주어가 되어 봐\\n\\n네가 꿈꿔 온 네 모습이 뭐여\\n지금 네 거울 속엔 누가 보여, I gotta say\\n너의 길을 가라고\\n단 하루를 살아도\\n뭐라도 하라고\\n나약함은 담아 둬\\n\\n얌마 네 꿈은 뭐니\\n얌마 네 꿈은 뭐니\\n얌마 네 꿈은 뭐니\\n네 꿈은 겨우 그거니\\n\\n거짓말이야 you such a liar\\nSee me see me ya 넌 위선자여\\n왜 자꾸 딴 길을 가래 야 너나 잘해\\n제발 강요하진 말아 줘\\n(La la la la la) 네 꿈이 뭐니 네 꿈이 뭐니 뭐니\\n(La la la la la) 고작 이거니 고작 이거니 거니\\n\\n살아가는 법을 몰라\\n날아가는 법을 몰라\\n결정하는 법을 몰라\\n이젠 꿈꾸는 법도 몰라\\n\\n눈을 눈을 눈을 떠라 다 이제\\n춤을 춤을 춤을 춰 봐 자 다시\\n꿈을 꿈을 꿈을 꿔 봐 다\\n너 꾸물대지 마 우물쭈물 대지 마 wussup!\\n\\n거짓말이야 you such a liar\\nSee me see me ya 넌 위선자여\\n왜 자꾸 딴 길을 가래 야 너나 잘해\\n제발 강요하진 말아 줘\\n(La la la la la) 네 꿈이 뭐니 네 꿈이 뭐니 뭐니\\n(La la la la la) 고작 이거니 고작 이거니 거니\\n\\nTo all the youngsters without dreams.\\n', 'I NEED U,\\nFall Fall Fall 흩어지네\\nFall Fall Fall 떨어지네\\n\\n너 땜에 나 이렇게 망가져\\n그만할래 이제 너 안 가져\\n못하겠어 뭣 같아서\\n제발 핑계 같은 건 삼가줘\\n\\n니가 나한테 이럼 안 돼\\n니가 한 모든 말은 안대\\n진실을 가리고 날 찢어\\n날 찍어 나 미쳐 다 싫어\\n전부 가져가 난 니가 그냥 미워\\n\\nBut you’re my everything (You’re my)\\nEverything (You’re my)\\nEverything (You’re my)\\n제발 좀 꺼져 huh\\n\\n미안해 (I hate u)\\n사랑해 (I hate u)\\n용서해\\n\\nI need you girl\\n왜 혼자 사랑하고 혼자서만 이별해\\nI need you girl\\n왜 다칠 걸 알면서 자꾸 니가 필요해\\n\\nI need you girl 넌 아름다워\\nI need you girl 너무 차가워\\nI need you girl (I need you girl)\\nI need you girl (I need you girl)\\n\\nIt goes round & round 나 왜 자꾸 돌아오지\\nI go down & down 이쯤 되면 내가 바보지\\n나 무슨 짓을 해봐도 어쩔 수가 없다고\\n분명 내 심장, 내 마음, 내 가슴인데 왜 말을 안 듣냐고\\n또 혼잣말하네 (또 혼잣말하네)\\n또 혼잣말하네 (또 혼잣말하네)\\n넌 아무 말 안 해 아 제발 내가 잘할게\\n하늘은 또 파랗게 (하늘은 또 파랗게)\\n\\n하늘이 파래서 햇살이 빛나서\\n내 눈물이 더 잘 보이나 봐\\n왜 나는 너인지 왜 하필 너인지\\n왜 너를 떠날 수가 없는지\\n\\nI need you girl\\n왜 혼자 사랑하고 혼자서만 이별해\\nI need you girl\\n왜 다칠 걸 알면서 자꾸 니가 필요해\\n\\nI need you girl 넌 아름다워\\nI need you girl 너무 차가워\\nI need you girl (I need you girl)\\nI need you girl (I need you girl)\\n\\nGirl 차라리 차라리 헤어지자고 해줘\\nGirl 사랑이 사랑이 아니었다고 해줘\\n내겐 그럴 용기가 없어\\n내게 마지막 선물을 줘\\n더는 돌아갈 수 없도록\\n\\nI need you girl\\n왜 혼자 사랑하고 혼자서만 이별해\\nI need you girl\\n왜 다칠 걸 알면서 자꾸 니가 필요해\\n\\nI need you girl 넌 아름다워\\nI need you girl 너무 차가워\\nI need you girl (I need you girl)\\nI need you girl (I need you girl)\\n', \"쩔어,\\n어서 와 방탄은 처음이지?\\n\\nAyo ladies & gentleman\\n준비가 됐다면 부를게 yeah!\\n딴 녀석들과는 다르게\\n내 스타일로 내 내 내 내 스타일로 에오!\\n\\n밤새 일했지 everyday\\n니가 클럽에서 놀 때 yeah\\n자 놀라지 말고 들어 매일\\nI got a feel, I got a feel\\n난 좀 쩔어!\\n\\n아 쩔어 쩔어 쩔어 우리 연습실 땀내\\n봐 쩌렁 쩌렁 쩌렁한 내 춤이 답해\\n모두 비실이 찌질이 찡찡이 띨띨이들\\n나랑은 상관이 없어 cuz 난 희망이 쩔어 haha\\n\\nOk 우린 머리부터 발끝까지 전부 다 쩌 쩔어\\n하루의 절반을 작업에 쩌 쩔어\\n작업실에 쩔어 살어 청춘은 썩어가도\\n덕분에 모로 가도 달리는 성공가도\\n소녀들아 더 크게 소리질러 쩌 쩌렁\\n\\n밤새 일했지 everyday\\n니가 클럽에서 놀 때 yeah\\n딴 녀석들과는 다르게\\nI don't wanna say yes\\nI don't wanna say yes\\n\\n소리쳐봐 all right\\n몸이 타버리도록 all night (all night)\\nCause we got fire (fire!)\\nHigher (higher!)\\nI gotta make it, I gotta make it\\n쩔어!\\n\\n거부는 거부해\\n난 원래 너무해\\n모두 다 따라 해\\n쩔어\\n\\n거부는 거부해\\n전부 나의 노예\\n모두 다 따라 해\\n쩔어\\n\\n3포세대? 5포세대?\\n그럼 난 육포가 좋으니까 6포세대\\n언론과 어른들은 의지가 없다며 우릴 싹 주식처럼 매도해\\n왜 해보기도 전에 죽여 걔넨 enemy enemy enemy\\n왜 벌써부터 고개를 숙여 받아 energy energy energy\\n절대 마 포기 you know you not lonely\\n너와 내 새벽은 낮보다 예뻐\\nSo can I get a little bit of hope? (yeah)\\n잠든 청춘을 깨워 go\\n\\n밤새 일했지 everyday\\n니가 클럽에서 놀 때 yeah\\n딴 녀석들과는 다르게\\nI don't wanna say yes\\nI don't wanna say yes\\n\\n소리쳐봐 all right\\n몸이 타버리도록 all night (all night)\\nCause we got fire (fire!)\\nHigher (higher!)\\nI gotta make it, I gotta make it\\n쩔어!\\n\\n거부는 거부해\\n난 원래 너무해\\n모두 다 따라 해\\n쩔어\\n\\n거부는 거부해\\n전부 나의 노예\\n모두 다 따라 해\\n쩔어\\n\\n이런 게 방탄 스타일\\n거짓말 wack들과는 달라\\n매일이 hustle life\\nI gotta make it fire baby\\n\\n이런 게 방탄 스타일\\n거짓말 wack들과는 달라\\n매일이 hustle life\\nI gotta make it, I gotta make it\\n난 좀 쩔어!\\n\\nSay what!\\nSay wo~ wo~\\nSay what!\\n쩔어\\n\"]\n"
     ]
    }
   ],
   "source": [
    "print(docs[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_ko = docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"쩔어,\\n어서 와 방탄은 처음이지?\\n\\nAyo ladies & gentleman\\n준비가 됐다면 부를게 yeah!\\n딴 녀석들과는 다르게\\n내 스타일로 내 내 내 내 스타일로 에오!\\n\\n밤새 일했지 everyday\\n니가 클럽에서 놀 때 yeah\\n자 놀라지 말고 들어 매일\\nI got a feel, I got a feel\\n난 좀 쩔어!\\n\\n아 쩔어 쩔어 쩔어 우리 연습실 땀내\\n봐 쩌렁 쩌렁 쩌렁한 내 춤이 답해\\n모두 비실이 찌질이 찡찡이 띨띨이들\\n나랑은 상관이 없어 cuz 난 희망이 쩔어 haha\\n\\nOk 우린 머리부터 발끝까지 전부 다 쩌 쩔어\\n하루의 절반을 작업에 쩌 쩔어\\n작업실에 쩔어 살어 청춘은 썩어가도\\n덕분에 모로 가도 달리는 성공가도\\n소녀들아 더 크게 소리질러 쩌 쩌렁\\n\\n밤새 일했지 everyday\\n니가 클럽에서 놀 때 yeah\\n딴 녀석들과는 다르게\\nI don't wanna say yes\\nI don't wanna say yes\\n\\n소리쳐봐 all right\\n몸이 타버리도록 all night (all night)\\nCause we got fire (fire!)\\nHigher (higher!)\\nI gotta make it, I gotta make it\\n쩔어!\\n\\n거부는 거부해\\n난 원래 너무해\\n모두 다 따라 해\\n쩔어\\n\\n거부는 거부해\\n전부 나의 노예\\n모두 다 따라 해\\n쩔어\\n\\n3포세대? 5포세대?\\n그럼 난 육포가 좋으니까 6포세대\\n언론과 어른들은 의지가 없다며 우릴 싹 주식처럼 매도해\\n왜 해보기도 전에 죽여 걔넨 enemy enemy enemy\\n왜 벌써부터 고개를 숙여 받아 energy energy energy\\n절대 마 포기 you know you not lonely\\n너와 내 새벽은 낮보다 예뻐\\nSo can I get a little bit of hope? (yeah)\\n잠든 청춘을 깨워 go\\n\\n밤새 일했지 everyday\\n니가 클럽에서 놀 때 yeah\\n딴 녀석들과는 다르게\\nI don't wanna say yes\\nI don't wanna say yes\\n\\n소리쳐봐 all right\\n몸이 타버리도록 all night (all night)\\nCause we got fire (fire!)\\nHigher (higher!)\\nI gotta make it, I gotta make it\\n쩔어!\\n\\n거부는 거부해\\n난 원래 너무해\\n모두 다 따라 해\\n쩔어\\n\\n거부는 거부해\\n전부 나의 노예\\n모두 다 따라 해\\n쩔어\\n\\n이런 게 방탄 스타일\\n거짓말 wack들과는 달라\\n매일이 hustle life\\nI gotta make it fire baby\\n\\n이런 게 방탄 스타일\\n거짓말 wack들과는 달라\\n매일이 hustle life\\nI gotta make it, I gotta make it\\n난 좀 쩔어!\\n\\nSay what!\\nSay wo~ wo~\\nSay what!\\n쩔어\\n\""
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 현재 문서는 좀 번잡하다.형태소 분석을 이용해서 정리하고 싶다.\n",
    "doc = documents_ko[2]\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] use default predictors\n",
      "[Noun Extractor] num features: pos=1260, neg=1173, common=12\n"
     ]
    }
   ],
   "source": [
    "# 간단한 전처리를 해보자 \n",
    "from soynlp.noun import LRNounExtractor_v2\n",
    "\n",
    "noun_extractor = LRNounExtractor_v2(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 1076 from 9 sents. mem=0.117 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2727, mem=0.118 Gb\n",
      "[Noun Extractor] batch prediction was completed for 286 words\n",
      "[Noun Extractor] checked compounds. discovered 0 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 80 -> 80\n",
      "[Noun Extractor] postprocessing ignore_features : 80 -> 77\n",
      "[Noun Extractor] postprocessing ignore_NJ : 77 -> 77\n",
      "[Noun Extractor] 77 nouns (0 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.119 Gb                    \n",
      "[Noun Extractor] 19.80 % eojeols are covered\n"
     ]
    }
   ],
   "source": [
    "# 학습기반 단어(명사) 추출기\n",
    "nouns = noun_extractor.train_extract(documents_ko)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DNA': NounScore(frequency=5, score=0.8333333333333334),\n",
       " '거짓말': NounScore(frequency=5, score=1.0),\n",
       " '어른들': NounScore(frequency=2, score=1.0),\n",
       " '스타일': NounScore(frequency=4, score=1.0),\n",
       " '우리': NounScore(frequency=6, score=1.0),\n",
       " '운명': NounScore(frequency=8, score=1.0),\n",
       " '하늘': NounScore(frequency=4, score=1.0),\n",
       " '시간': NounScore(frequency=4, score=1.0),\n",
       " '청춘': NounScore(frequency=2, score=1.0),\n",
       " '겨울': NounScore(frequency=6, score=1.0),\n",
       " '벌써': NounScore(frequency=2, score=1.0),\n",
       " '처음': NounScore(frequency=1, score=0.5),\n",
       " '머리': NounScore(frequency=1, score=0.5),\n",
       " '우주': NounScore(frequency=3, score=1.0),\n",
       " '준비': NounScore(frequency=2, score=1.0),\n",
       " '모두': NounScore(frequency=6, score=1.0),\n",
       " '봄날': NounScore(frequency=3, score=0.75),\n",
       " '매일': NounScore(frequency=4, score=1.0),\n",
       " '사랑': NounScore(frequency=7, score=1.0),\n",
       " '눈물': NounScore(frequency=11, score=1.0),\n",
       " '새벽': NounScore(frequency=2, score=1.0),\n",
       " '함께': NounScore(frequency=3, score=0.5),\n",
       " '행복': NounScore(frequency=2, score=1.0),\n",
       " '아무': NounScore(frequency=2, score=1.0),\n",
       " '있어': NounScore(frequency=5, score=0.8333333333333334),\n",
       " '하루': NounScore(frequency=4, score=1.0),\n",
       " '멀리': NounScore(frequency=2, score=1.0),\n",
       " '거부': NounScore(frequency=8, score=1.0),\n",
       " '날개': NounScore(frequency=4, score=1.0),\n",
       " '상처': NounScore(frequency=2, score=1.0),\n",
       " '너무': NounScore(frequency=12, score=1.0),\n",
       " '달콤': NounScore(frequency=3, score=1.0),\n",
       " '이상': NounScore(frequency=2, score=1.0),\n",
       " '사소': NounScore(frequency=3, score=1.0),\n",
       " '쩌렁': NounScore(frequency=4, score=1.0),\n",
       " '특별': NounScore(frequency=2, score=1.0),\n",
       " '전부': NounScore(frequency=10, score=1.0),\n",
       " '자꾸': NounScore(frequency=8, score=1.0),\n",
       " '조금': NounScore(frequency=5, score=0.7142857142857143),\n",
       " '작업': NounScore(frequency=1, score=0.5),\n",
       " '춤': NounScore(frequency=7, score=1.0),\n",
       " '꿈': NounScore(frequency=28, score=1.0),\n",
       " '길': NounScore(frequency=7, score=1.0),\n",
       " '법': NounScore(frequency=4, score=1.0),\n",
       " '눈': NounScore(frequency=8, score=1.0),\n",
       " '숨': NounScore(frequency=4, score=1.0),\n",
       " '밤': NounScore(frequency=3, score=1.0),\n",
       " '손': NounScore(frequency=4, score=0.6666666666666666),\n",
       " '끝': NounScore(frequency=3, score=1.0),\n",
       " '말': NounScore(frequency=14, score=0.5),\n",
       " '보': NounScore(frequency=1, score=1.0),\n",
       " '내': NounScore(frequency=51, score=1.0),\n",
       " '네': NounScore(frequency=32, score=1.0),\n",
       " '수': NounScore(frequency=8, score=1.0),\n",
       " '많': NounScore(frequency=40, score=0.9512195121951219),\n",
       " '몸': NounScore(frequency=3, score=1.0),\n",
       " '높': NounScore(frequency=4, score=0.8),\n",
       " '독': NounScore(frequency=1, score=0.5),\n",
       " '힘': NounScore(frequency=1, score=0.5),\n",
       " '깊': NounScore(frequency=1, score=0.5),\n",
       " '너': NounScore(frequency=30, score=1.0),\n",
       " '못': NounScore(frequency=4, score=1.0),\n",
       " '꿔': NounScore(frequency=2, score=1.0),\n",
       " '뭣': NounScore(frequency=2, score=1.0),\n",
       " '것': NounScore(frequency=4, score=1.0),\n",
       " '둘': NounScore(frequency=6, score=1.0),\n",
       " '취': NounScore(frequency=3, score=1.0),\n",
       " '더': NounScore(frequency=13, score=1.0),\n",
       " '원': NounScore(frequency=9, score=1.0),\n",
       " '잘': NounScore(frequency=5, score=0.625),\n",
       " '욕': NounScore(frequency=2, score=1.0),\n",
       " '꺼': NounScore(frequency=2, score=0.3333333333333333),\n",
       " '생': NounScore(frequency=2, score=0.4),\n",
       " '때': NounScore(frequency=10, score=0.7142857142857143),\n",
       " '모': NounScore(frequency=10, score=1.0),\n",
       " '삶': NounScore(frequency=2, score=1.0),\n",
       " '열': NounScore(frequency=1, score=0.5)}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토크나이저 \n",
    "from soynlp.tokenizer import MaxScoreTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DNA': 0.8333333333333334,\n",
       " '거짓말': 1.0,\n",
       " '어른들': 1.0,\n",
       " '스타일': 1.0,\n",
       " '우리': 1.0,\n",
       " '운명': 1.0,\n",
       " '하늘': 1.0,\n",
       " '시간': 1.0,\n",
       " '청춘': 1.0,\n",
       " '겨울': 1.0,\n",
       " '벌써': 1.0,\n",
       " '처음': 0.5,\n",
       " '머리': 0.5,\n",
       " '우주': 1.0,\n",
       " '준비': 1.0,\n",
       " '모두': 1.0,\n",
       " '봄날': 0.75,\n",
       " '매일': 1.0,\n",
       " '사랑': 1.0,\n",
       " '눈물': 1.0,\n",
       " '새벽': 1.0,\n",
       " '함께': 0.5,\n",
       " '행복': 1.0,\n",
       " '아무': 1.0,\n",
       " '있어': 0.8333333333333334,\n",
       " '하루': 1.0,\n",
       " '멀리': 1.0,\n",
       " '거부': 1.0,\n",
       " '날개': 1.0,\n",
       " '상처': 1.0,\n",
       " '너무': 1.0,\n",
       " '달콤': 1.0,\n",
       " '이상': 1.0,\n",
       " '사소': 1.0,\n",
       " '쩌렁': 1.0,\n",
       " '특별': 1.0,\n",
       " '전부': 1.0,\n",
       " '자꾸': 1.0,\n",
       " '조금': 0.7142857142857143,\n",
       " '작업': 0.5,\n",
       " '춤': 1.0,\n",
       " '꿈': 1.0,\n",
       " '길': 1.0,\n",
       " '법': 1.0,\n",
       " '눈': 1.0,\n",
       " '숨': 1.0,\n",
       " '밤': 1.0,\n",
       " '손': 0.6666666666666666,\n",
       " '끝': 1.0,\n",
       " '말': 0.5,\n",
       " '보': 1.0,\n",
       " '내': 1.0,\n",
       " '네': 1.0,\n",
       " '수': 1.0,\n",
       " '많': 0.9512195121951219,\n",
       " '몸': 1.0,\n",
       " '높': 0.8,\n",
       " '독': 0.5,\n",
       " '힘': 0.5,\n",
       " '깊': 0.5,\n",
       " '너': 1.0,\n",
       " '못': 1.0,\n",
       " '꿔': 1.0,\n",
       " '뭣': 1.0,\n",
       " '것': 1.0,\n",
       " '둘': 1.0,\n",
       " '취': 1.0,\n",
       " '더': 1.0,\n",
       " '원': 1.0,\n",
       " '잘': 0.625,\n",
       " '욕': 1.0,\n",
       " '꺼': 0.3333333333333333,\n",
       " '생': 0.4,\n",
       " '때': 0.7142857142857143,\n",
       " '모': 1.0,\n",
       " '삶': 1.0,\n",
       " '열': 0.5}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단어별 스코어 \n",
    "scores = {w:s.score for w, s in nouns.items()}\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토크나이저 생성\n",
    "tokenizer = MaxScoreTokenizer(scores=scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------- 원문 --------------\n",
      "\n",
      "No More Dream,\n",
      "얌마 네 꿈은 뭐니\n",
      "얌마 네 꿈은 뭐니\n",
      "얌마 네 꿈은 뭐니\n",
      "네 꿈은 겨우 그거니\n",
      "\n",
      "I wanna big house, big cars & big rings\n",
      "But 사실은 I dun have any big dreams\n",
      "하하 난 참 편하게 살어\n",
      "꿈 따위 안 꿔도 아무도 뭐라 안 하잖어\n",
      "전부 다다다 똑같이 나처럼 생각하고 있어\n",
      "새까까까맣게 까먹은 꿈 많던 어린 시절\n",
      "대학은 걱정 마 멀리라도 갈 거니까\n",
      "알았어 엄마 지금 독서실 간다니까\n",
      "\n",
      "네가 꿈꿔 온 네 모습이 뭐야\n",
      "지금 네 거울 속엔 누가 보여, I gotta say\n",
      "너의 길을 가라고\n",
      "단 하루를 살아도\n",
      "뭐라도 하라고\n",
      "나약함은 담아 둬\n",
      "\n",
      "왜 말 못하고 있어? 공부는 하기 싫다면서\n",
      "학교 때려 치기는 겁나지? 이거 봐 등교할 준비하네 벌써\n",
      "철 좀 들어 제발 좀, 너 입만 살아가지고 인마 유리 멘탈 boy\n",
      "(Stop!) 자신에게 물어봐 언제 네가 열심히 노력했냐고\n",
      "\n",
      "얌마 네 꿈은 뭐니\n",
      "얌마 네 꿈은 뭐니\n",
      "얌마 네 꿈은 뭐니\n",
      "네 꿈은 겨우 그거니\n",
      "\n",
      "거짓말이야 you such a liar\n",
      "See me see me ya 넌 위선자여\n",
      "왜 자꾸 딴 길을 가래 야 너나 잘해\n",
      "제발 강요하진 말아 줘\n",
      "(La la la la la) 네 꿈이 뭐니 네 꿈이 뭐니 뭐니\n",
      "(La la la la la) 고작 이거니 고작 이거니 거니\n",
      "\n",
      "지겨운 same day, 반복되는 매일에\n",
      "어른들과 부모님은 틀에 박힌 꿈을 주입해\n",
      "장래희망 넘버원... 공무원?\n",
      "강요된 꿈은 아냐, 9회말 구원투수\n",
      "시간 낭비인 야자에 돌직구를 날려\n",
      "지옥 같은 사회에 반항해, 꿈을 특별 사면\n",
      "자신에게 물어봐 네 꿈의 profile\n",
      "억압만 받던 인생 네 삶의 주어가 되어 봐\n",
      "\n",
      "네가 꿈꿔 온 네 모습이 뭐여\n",
      "지금 네 거울 속엔 누가 보여, I gotta say\n",
      "너의 길을 가라고\n",
      "단 하루를 살아도\n",
      "뭐라도 하라고\n",
      "나약함은 담아 둬\n",
      "\n",
      "얌마 네 꿈은 뭐니\n",
      "얌마 네 꿈은 뭐니\n",
      "얌마 네 꿈은 뭐니\n",
      "네 꿈은 겨우 그거니\n",
      "\n",
      "거짓말이야 you such a liar\n",
      "See me see me ya 넌 위선자여\n",
      "왜 자꾸 딴 길을 가래 야 너나 잘해\n",
      "제발 강요하진 말아 줘\n",
      "(La la la la la) 네 꿈이 뭐니 네 꿈이 뭐니 뭐니\n",
      "(La la la la la) 고작 이거니 고작 이거니 거니\n",
      "\n",
      "살아가는 법을 몰라\n",
      "날아가는 법을 몰라\n",
      "결정하는 법을 몰라\n",
      "이젠 꿈꾸는 법도 몰라\n",
      "\n",
      "눈을 눈을 눈을 떠라 다 이제\n",
      "춤을 춤을 춤을 춰 봐 자 다시\n",
      "꿈을 꿈을 꿈을 꿔 봐 다\n",
      "너 꾸물대지 마 우물쭈물 대지 마 wussup!\n",
      "\n",
      "거짓말이야 you such a liar\n",
      "See me see me ya 넌 위선자여\n",
      "왜 자꾸 딴 길을 가래 야 너나 잘해\n",
      "제발 강요하진 말아 줘\n",
      "(La la la la la) 네 꿈이 뭐니 네 꿈이 뭐니 뭐니\n",
      "(La la la la la) 고작 이거니 고작 이거니 거니\n",
      "\n",
      "To all the youngsters without dreams.\n",
      "\n",
      "\n",
      "---------------- 처리 후 --------------\n",
      "\n",
      "['No', 'More', 'Dream,', '얌마', '네', '꿈은', '뭐니', '얌마', '네', '꿈은', '뭐니', '얌마', '네', '꿈은', '뭐니', '네', '꿈은', '겨우', '그거니', 'I', 'wanna', 'big', 'house,', 'big', 'cars', '&', 'big', 'rings', 'But', '사실은', 'I', 'dun', 'have', 'any', 'big', 'dreams', '하하', '난', '참', '편하게', '살어', '꿈', '따위', '안', '꿔도', '아무', '도', '뭐라', '안', '하잖어', '전부', '다다다', '똑같이', '나처럼', '생각하고', '있어', '새까까까맣게', '까먹은', '꿈', '많던', '어린', '시절', '대학은', '걱정', '마', '멀리', '라도', '갈', '거니까', '알았어', '엄마', '지금', '독서실', '간다니까', '네가', '꿈꿔', '온', '네', '모습이', '뭐야', '지금', '네', '거울', '속엔', '누가', '보여,', 'I', 'gotta', 'say', '너의', '길을', '가라고', '단', '하루', '를', '살아도', '뭐라도', '하라고', '나약함은', '담아', '둬', '왜', '말', '못하고', '있어', '?', '공부는', '하기', '싫다면서', '학교', '때려', '치기는', '겁나지?', '이거', '봐', '등교할', '준비', '하네', '벌써', '철', '좀', '들어', '제발', '좀,', '너', '입만', '살아가지고', '인마', '유리', '멘탈', 'boy', '(Stop!)', '자신에게', '물어봐', '언제', '네가', '열심히', '노력했냐고', '얌마', '네', '꿈은', '뭐니', '얌마', '네', '꿈은', '뭐니', '얌마', '네', '꿈은', '뭐니', '네', '꿈은', '겨우', '그거니', '거짓말', '이야', 'you', 'such', 'a', 'liar', 'See', 'me', 'see', 'me', 'ya', '넌', '위선자여', '왜', '자꾸', '딴', '길을', '가래', '야', '너나', '잘해', '제발', '강요하진', '말아', '줘', '(La', 'la', 'la', 'la', 'la)', '네', '꿈이', '뭐니', '네', '꿈이', '뭐니', '뭐니', '(La', 'la', 'la', 'la', 'la)', '고작', '이거니', '고작', '이거니', '거니', '지겨운', 'same', 'day,', '반복되는', '매일', '에', '어른들', '과', '부모님은', '틀에', '박힌', '꿈을', '주입해', '장래희망', '넘버원...', '공무원?', '강요된', '꿈은', '아냐,', '9회말', '구원투수', '시간', '낭비인', '야자에', '돌직구를', '날려', '지옥', '같은', '사회에', '반항해,', '꿈을', '특별', '사면', '자신에게', '물어봐', '네', '꿈의', 'profile', '억압만', '받던', '인생', '네', '삶의', '주어가', '되어', '봐', '네가', '꿈꿔', '온', '네', '모습이', '뭐여', '지금', '네', '거울', '속엔', '누가', '보여,', 'I', 'gotta', 'say', '너의', '길을', '가라고', '단', '하루', '를', '살아도', '뭐라도', '하라고', '나약함은', '담아', '둬', '얌마', '네', '꿈은', '뭐니', '얌마', '네', '꿈은', '뭐니', '얌마', '네', '꿈은', '뭐니', '네', '꿈은', '겨우', '그거니', '거짓말', '이야', 'you', 'such', 'a', 'liar', 'See', 'me', 'see', 'me', 'ya', '넌', '위선자여', '왜', '자꾸', '딴', '길을', '가래', '야', '너나', '잘해', '제발', '강요하진', '말아', '줘', '(La', 'la', 'la', 'la', 'la)', '네', '꿈이', '뭐니', '네', '꿈이', '뭐니', '뭐니', '(La', 'la', 'la', 'la', 'la)', '고작', '이거니', '고작', '이거니', '거니', '살아가는', '법을', '몰라', '날아가는', '법을', '몰라', '결정하는', '법을', '몰라', '이젠', '꿈꾸는', '법도', '몰라', '눈을', '눈을', '눈을', '떠라', '다', '이제', '춤을', '춤을', '춤을', '춰', '봐', '자', '다시', '꿈을', '꿈을', '꿈을', '꿔', '봐', '다', '너', '꾸물대지', '마', '우물쭈물', '대지', '마', 'wussup!', '거짓말', '이야', 'you', 'such', 'a', 'liar', 'See', 'me', 'see', 'me', 'ya', '넌', '위선자여', '왜', '자꾸', '딴', '길을', '가래', '야', '너나', '잘해', '제발', '강요하진', '말아', '줘', '(La', 'la', 'la', 'la', 'la)', '네', '꿈이', '뭐니', '네', '꿈이', '뭐니', '뭐니', '(La', 'la', 'la', 'la', 'la)', '고작', '이거니', '고작', '이거니', '거니', 'To', 'all', 'the', 'youngsters', 'without', 'dreams.']\n",
      "\n",
      "---------------- 원문 --------------\n",
      "\n",
      "I NEED U,\n",
      "Fall Fall Fall 흩어지네\n",
      "Fall Fall Fall 떨어지네\n",
      "\n",
      "너 땜에 나 이렇게 망가져\n",
      "그만할래 이제 너 안 가져\n",
      "못하겠어 뭣 같아서\n",
      "제발 핑계 같은 건 삼가줘\n",
      "\n",
      "니가 나한테 이럼 안 돼\n",
      "니가 한 모든 말은 안대\n",
      "진실을 가리고 날 찢어\n",
      "날 찍어 나 미쳐 다 싫어\n",
      "전부 가져가 난 니가 그냥 미워\n",
      "\n",
      "But you’re my everything (You’re my)\n",
      "Everything (You’re my)\n",
      "Everything (You’re my)\n",
      "제발 좀 꺼져 huh\n",
      "\n",
      "미안해 (I hate u)\n",
      "사랑해 (I hate u)\n",
      "용서해\n",
      "\n",
      "I need you girl\n",
      "왜 혼자 사랑하고 혼자서만 이별해\n",
      "I need you girl\n",
      "왜 다칠 걸 알면서 자꾸 니가 필요해\n",
      "\n",
      "I need you girl 넌 아름다워\n",
      "I need you girl 너무 차가워\n",
      "I need you girl (I need you girl)\n",
      "I need you girl (I need you girl)\n",
      "\n",
      "It goes round & round 나 왜 자꾸 돌아오지\n",
      "I go down & down 이쯤 되면 내가 바보지\n",
      "나 무슨 짓을 해봐도 어쩔 수가 없다고\n",
      "분명 내 심장, 내 마음, 내 가슴인데 왜 말을 안 듣냐고\n",
      "또 혼잣말하네 (또 혼잣말하네)\n",
      "또 혼잣말하네 (또 혼잣말하네)\n",
      "넌 아무 말 안 해 아 제발 내가 잘할게\n",
      "하늘은 또 파랗게 (하늘은 또 파랗게)\n",
      "\n",
      "하늘이 파래서 햇살이 빛나서\n",
      "내 눈물이 더 잘 보이나 봐\n",
      "왜 나는 너인지 왜 하필 너인지\n",
      "왜 너를 떠날 수가 없는지\n",
      "\n",
      "I need you girl\n",
      "왜 혼자 사랑하고 혼자서만 이별해\n",
      "I need you girl\n",
      "왜 다칠 걸 알면서 자꾸 니가 필요해\n",
      "\n",
      "I need you girl 넌 아름다워\n",
      "I need you girl 너무 차가워\n",
      "I need you girl (I need you girl)\n",
      "I need you girl (I need you girl)\n",
      "\n",
      "Girl 차라리 차라리 헤어지자고 해줘\n",
      "Girl 사랑이 사랑이 아니었다고 해줘\n",
      "내겐 그럴 용기가 없어\n",
      "내게 마지막 선물을 줘\n",
      "더는 돌아갈 수 없도록\n",
      "\n",
      "I need you girl\n",
      "왜 혼자 사랑하고 혼자서만 이별해\n",
      "I need you girl\n",
      "왜 다칠 걸 알면서 자꾸 니가 필요해\n",
      "\n",
      "I need you girl 넌 아름다워\n",
      "I need you girl 너무 차가워\n",
      "I need you girl (I need you girl)\n",
      "I need you girl (I need you girl)\n",
      "\n",
      "\n",
      "---------------- 처리 후 --------------\n",
      "\n",
      "['I', 'NEED', 'U,', 'Fall', 'Fall', 'Fall', '흩어지네', 'Fall', 'Fall', 'Fall', '떨어지네', '너', '땜에', '나', '이렇게', '망가져', '그만할래', '이제', '너', '안', '가져', '못하겠어', '뭣', '같아서', '제발', '핑계', '같은', '건', '삼가줘', '니가', '나한테', '이럼', '안', '돼', '니가', '한', '모든', '말은', '안대', '진실을', '가리고', '날', '찢어', '날', '찍어', '나', '미쳐', '다', '싫어', '전부', '가져가', '난', '니가', '그냥', '미워', 'But', 'you’re', 'my', 'everything', '(You’re', 'my)', 'Everything', '(You’re', 'my)', 'Everything', '(You’re', 'my)', '제발', '좀', '꺼져', 'huh', '미안해', '(I', 'hate', 'u)', '사랑', '해', '(I', 'hate', 'u)', '용서해', 'I', 'need', 'you', 'girl', '왜', '혼자', '사랑', '하고', '혼자서만', '이별해', 'I', 'need', 'you', 'girl', '왜', '다칠', '걸', '알면서', '자꾸', '니가', '필요해', 'I', 'need', 'you', 'girl', '넌', '아름다워', 'I', 'need', 'you', 'girl', '너무', '차가워', 'I', 'need', 'you', 'girl', '(I', 'need', 'you', 'girl)', 'I', 'need', 'you', 'girl', '(I', 'need', 'you', 'girl)', 'It', 'goes', 'round', '&', 'round', '나', '왜', '자꾸', '돌아오지', 'I', 'go', 'down', '&', 'down', '이쯤', '되면', '내가', '바보지', '나', '무슨', '짓을', '해봐도', '어쩔', '수가', '없다고', '분명', '내', '심장,', '내', '마음,', '내', '가슴인데', '왜', '말을', '안', '듣냐고', '또', '혼잣말하네', '(또', '혼잣말하네)', '또', '혼잣말하네', '(또', '혼잣말하네)', '넌', '아무', '말', '안', '해', '아', '제발', '내가', '잘할게', '하늘', '은', '또', '파랗게', '(', '하늘', '은', '또', '파랗게)', '하늘', '이', '파래서', '햇살이', '빛나서', '내', '눈물', '이', '더', '잘', '보이나', '봐', '왜', '나는', '너인지', '왜', '하필', '너인지', '왜', '너를', '떠날', '수가', '없는지', 'I', 'need', 'you', 'girl', '왜', '혼자', '사랑', '하고', '혼자서만', '이별해', 'I', 'need', 'you', 'girl', '왜', '다칠', '걸', '알면서', '자꾸', '니가', '필요해', 'I', 'need', 'you', 'girl', '넌', '아름다워', 'I', 'need', 'you', 'girl', '너무', '차가워', 'I', 'need', 'you', 'girl', '(I', 'need', 'you', 'girl)', 'I', 'need', 'you', 'girl', '(I', 'need', 'you', 'girl)', 'Girl', '차라리', '차라리', '헤어지자고', '해줘', 'Girl', '사랑', '이', '사랑', '이', '아니었다고', '해줘', '내겐', '그럴', '용기가', '없어', '내게', '마지막', '선물을', '줘', '더는', '돌아갈', '수', '없도록', 'I', 'need', 'you', 'girl', '왜', '혼자', '사랑', '하고', '혼자서만', '이별해', 'I', 'need', 'you', 'girl', '왜', '다칠', '걸', '알면서', '자꾸', '니가', '필요해', 'I', 'need', 'you', 'girl', '넌', '아름다워', 'I', 'need', 'you', 'girl', '너무', '차가워', 'I', 'need', 'you', 'girl', '(I', 'need', 'you', 'girl)', 'I', 'need', 'you', 'girl', '(I', 'need', 'you', 'girl)']\n"
     ]
    }
   ],
   "source": [
    "# 토크나이저 적용.\n",
    "for doc in documents_ko[:2] :\n",
    "    print(\"\\n---------------- 원문 --------------\\n\")\n",
    "    print(doc)\n",
    "    print(\"\\n---------------- 처리 후 --------------\\n\")\n",
    "    tokens = tokenizer.tokenize(doc)\n",
    "    print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# 그래도 뭔가 부족해보인다. 명사 위주로만 뽑자\n",
    "# 명사 추출기를 통해 해당 단어가 명사인지 아닌지를 알 수 있다.\n",
    "print(nouns.get('안녕'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------- 원문 --------------\n",
      "\n",
      "No More Dream,\n",
      "얌마 네 꿈은 뭐니\n",
      "얌마 네 꿈은 뭐니\n",
      "얌마 네 꿈은 뭐니\n",
      "네 꿈은 겨우 그거니\n",
      "\n",
      "I wanna big house, big cars & big rings\n",
      "But 사실은 I dun have any big dreams\n",
      "하하 난 참 편하게 살어\n",
      "꿈 따위 안 꿔도 아무도 뭐라 안 하잖어\n",
      "전부 다다다 똑같이 나처럼 생각하고 있어\n",
      "새까까까맣게 까먹은 꿈 많던 어린 시절\n",
      "대학은 걱정 마 멀리라도 갈 거니까\n",
      "알았어 엄마 지금 독서실 간다니까\n",
      "\n",
      "네가 꿈꿔 온 네 모습이 뭐야\n",
      "지금 네 거울 속엔 누가 보여, I gotta say\n",
      "너의 길을 가라고\n",
      "단 하루를 살아도\n",
      "뭐라도 하라고\n",
      "나약함은 담아 둬\n",
      "\n",
      "왜 말 못하고 있어? 공부는 하기 싫다면서\n",
      "학교 때려 치기는 겁나지? 이거 봐 등교할 준비하네 벌써\n",
      "철 좀 들어 제발 좀, 너 입만 살아가지고 인마 유리 멘탈 boy\n",
      "(Stop!) 자신에게 물어봐 언제 네가 열심히 노력했냐고\n",
      "\n",
      "얌마 네 꿈은 뭐니\n",
      "얌마 네 꿈은 뭐니\n",
      "얌마 네 꿈은 뭐니\n",
      "네 꿈은 겨우 그거니\n",
      "\n",
      "거짓말이야 you such a liar\n",
      "See me see me ya 넌 위선자여\n",
      "왜 자꾸 딴 길을 가래 야 너나 잘해\n",
      "제발 강요하진 말아 줘\n",
      "(La la la la la) 네 꿈이 뭐니 네 꿈이 뭐니 뭐니\n",
      "(La la la la la) 고작 이거니 고작 이거니 거니\n",
      "\n",
      "지겨운 same day, 반복되는 매일에\n",
      "어른들과 부모님은 틀에 박힌 꿈을 주입해\n",
      "장래희망 넘버원... 공무원?\n",
      "강요된 꿈은 아냐, 9회말 구원투수\n",
      "시간 낭비인 야자에 돌직구를 날려\n",
      "지옥 같은 사회에 반항해, 꿈을 특별 사면\n",
      "자신에게 물어봐 네 꿈의 profile\n",
      "억압만 받던 인생 네 삶의 주어가 되어 봐\n",
      "\n",
      "네가 꿈꿔 온 네 모습이 뭐여\n",
      "지금 네 거울 속엔 누가 보여, I gotta say\n",
      "너의 길을 가라고\n",
      "단 하루를 살아도\n",
      "뭐라도 하라고\n",
      "나약함은 담아 둬\n",
      "\n",
      "얌마 네 꿈은 뭐니\n",
      "얌마 네 꿈은 뭐니\n",
      "얌마 네 꿈은 뭐니\n",
      "네 꿈은 겨우 그거니\n",
      "\n",
      "거짓말이야 you such a liar\n",
      "See me see me ya 넌 위선자여\n",
      "왜 자꾸 딴 길을 가래 야 너나 잘해\n",
      "제발 강요하진 말아 줘\n",
      "(La la la la la) 네 꿈이 뭐니 네 꿈이 뭐니 뭐니\n",
      "(La la la la la) 고작 이거니 고작 이거니 거니\n",
      "\n",
      "살아가는 법을 몰라\n",
      "날아가는 법을 몰라\n",
      "결정하는 법을 몰라\n",
      "이젠 꿈꾸는 법도 몰라\n",
      "\n",
      "눈을 눈을 눈을 떠라 다 이제\n",
      "춤을 춤을 춤을 춰 봐 자 다시\n",
      "꿈을 꿈을 꿈을 꿔 봐 다\n",
      "너 꾸물대지 마 우물쭈물 대지 마 wussup!\n",
      "\n",
      "거짓말이야 you such a liar\n",
      "See me see me ya 넌 위선자여\n",
      "왜 자꾸 딴 길을 가래 야 너나 잘해\n",
      "제발 강요하진 말아 줘\n",
      "(La la la la la) 네 꿈이 뭐니 네 꿈이 뭐니 뭐니\n",
      "(La la la la la) 고작 이거니 고작 이거니 거니\n",
      "\n",
      "To all the youngsters without dreams.\n",
      "\n",
      "\n",
      "---------------- 처리 후 --------------\n",
      "\n",
      "['네', '네', '네', '네', '꿈', '아무', '전부', '있어', '꿈', '멀리', '네', '네', '하루', '말', '있어', '준비', '벌써', '너', '네', '네', '네', '네', '거짓말', '자꾸', '네', '네', '매일', '어른들', '시간', '특별', '네', '네', '네', '네', '하루', '네', '네', '네', '네', '거짓말', '자꾸', '네', '네', '꿔', '너', '거짓말', '자꾸', '네', '네']\n",
      "\n",
      "---------------- 원문 --------------\n",
      "\n",
      "I NEED U,\n",
      "Fall Fall Fall 흩어지네\n",
      "Fall Fall Fall 떨어지네\n",
      "\n",
      "너 땜에 나 이렇게 망가져\n",
      "그만할래 이제 너 안 가져\n",
      "못하겠어 뭣 같아서\n",
      "제발 핑계 같은 건 삼가줘\n",
      "\n",
      "니가 나한테 이럼 안 돼\n",
      "니가 한 모든 말은 안대\n",
      "진실을 가리고 날 찢어\n",
      "날 찍어 나 미쳐 다 싫어\n",
      "전부 가져가 난 니가 그냥 미워\n",
      "\n",
      "But you’re my everything (You’re my)\n",
      "Everything (You’re my)\n",
      "Everything (You’re my)\n",
      "제발 좀 꺼져 huh\n",
      "\n",
      "미안해 (I hate u)\n",
      "사랑해 (I hate u)\n",
      "용서해\n",
      "\n",
      "I need you girl\n",
      "왜 혼자 사랑하고 혼자서만 이별해\n",
      "I need you girl\n",
      "왜 다칠 걸 알면서 자꾸 니가 필요해\n",
      "\n",
      "I need you girl 넌 아름다워\n",
      "I need you girl 너무 차가워\n",
      "I need you girl (I need you girl)\n",
      "I need you girl (I need you girl)\n",
      "\n",
      "It goes round & round 나 왜 자꾸 돌아오지\n",
      "I go down & down 이쯤 되면 내가 바보지\n",
      "나 무슨 짓을 해봐도 어쩔 수가 없다고\n",
      "분명 내 심장, 내 마음, 내 가슴인데 왜 말을 안 듣냐고\n",
      "또 혼잣말하네 (또 혼잣말하네)\n",
      "또 혼잣말하네 (또 혼잣말하네)\n",
      "넌 아무 말 안 해 아 제발 내가 잘할게\n",
      "하늘은 또 파랗게 (하늘은 또 파랗게)\n",
      "\n",
      "하늘이 파래서 햇살이 빛나서\n",
      "내 눈물이 더 잘 보이나 봐\n",
      "왜 나는 너인지 왜 하필 너인지\n",
      "왜 너를 떠날 수가 없는지\n",
      "\n",
      "I need you girl\n",
      "왜 혼자 사랑하고 혼자서만 이별해\n",
      "I need you girl\n",
      "왜 다칠 걸 알면서 자꾸 니가 필요해\n",
      "\n",
      "I need you girl 넌 아름다워\n",
      "I need you girl 너무 차가워\n",
      "I need you girl (I need you girl)\n",
      "I need you girl (I need you girl)\n",
      "\n",
      "Girl 차라리 차라리 헤어지자고 해줘\n",
      "Girl 사랑이 사랑이 아니었다고 해줘\n",
      "내겐 그럴 용기가 없어\n",
      "내게 마지막 선물을 줘\n",
      "더는 돌아갈 수 없도록\n",
      "\n",
      "I need you girl\n",
      "왜 혼자 사랑하고 혼자서만 이별해\n",
      "I need you girl\n",
      "왜 다칠 걸 알면서 자꾸 니가 필요해\n",
      "\n",
      "I need you girl 넌 아름다워\n",
      "I need you girl 너무 차가워\n",
      "I need you girl (I need you girl)\n",
      "I need you girl (I need you girl)\n",
      "\n",
      "\n",
      "---------------- 처리 후 --------------\n",
      "\n",
      "['너', '너', '뭣', '전부', '사랑', '사랑', '자꾸', '너무', '자꾸', '내', '내', '내', '아무', '말', '하늘', '하늘', '하늘', '내', '눈물', '더', '잘', '사랑', '자꾸', '너무', '사랑', '사랑', '수', '사랑', '자꾸', '너무']\n"
     ]
    }
   ],
   "source": [
    "# 적용\n",
    "for doc in documents_ko[:2] :\n",
    "    print(\"\\n---------------- 원문 --------------\\n\")\n",
    "    print(doc)\n",
    "    print(\"\\n---------------- 처리 후 --------------\\n\")\n",
    "    tokens = tokenizer.tokenize(doc)\n",
    "    print([token for token in tokens if nouns.get(token)]) # 토큰 중에 명사인 것만 반영"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['네',\n",
      "  '네',\n",
      "  '네',\n",
      "  '네',\n",
      "  '꿈',\n",
      "  '아무',\n",
      "  '전부',\n",
      "  '있어',\n",
      "  '꿈',\n",
      "  '멀리',\n",
      "  '네',\n",
      "  '네',\n",
      "  '하루',\n",
      "  '말',\n",
      "  '있어',\n",
      "  '준비',\n",
      "  '벌써',\n",
      "  '너',\n",
      "  '네',\n",
      "  '네',\n",
      "  '네',\n",
      "  '네',\n",
      "  '거짓말',\n",
      "  '자꾸',\n",
      "  '네',\n",
      "  '네',\n",
      "  '매일',\n",
      "  '어른들',\n",
      "  '시간',\n",
      "  '특별',\n",
      "  '네',\n",
      "  '네',\n",
      "  '네',\n",
      "  '네',\n",
      "  '하루',\n",
      "  '네',\n",
      "  '네',\n",
      "  '네',\n",
      "  '네',\n",
      "  '거짓말',\n",
      "  '자꾸',\n",
      "  '네',\n",
      "  '네',\n",
      "  '꿔',\n",
      "  '너',\n",
      "  '거짓말',\n",
      "  '자꾸',\n",
      "  '네',\n",
      "  '네'],\n",
      " ['너',\n",
      "  '너',\n",
      "  '뭣',\n",
      "  '전부',\n",
      "  '사랑',\n",
      "  '사랑',\n",
      "  '자꾸',\n",
      "  '너무',\n",
      "  '자꾸',\n",
      "  '내',\n",
      "  '내',\n",
      "  '내',\n",
      "  '아무',\n",
      "  '말',\n",
      "  '하늘',\n",
      "  '하늘',\n",
      "  '하늘',\n",
      "  '내',\n",
      "  '눈물',\n",
      "  '더',\n",
      "  '잘',\n",
      "  '사랑',\n",
      "  '자꾸',\n",
      "  '너무',\n",
      "  '사랑',\n",
      "  '사랑',\n",
      "  '수',\n",
      "  '사랑',\n",
      "  '자꾸',\n",
      "  '너무']]\n"
     ]
    }
   ],
   "source": [
    "# 단어(토큰) 단위로 분할하는 함수 - 그 중에 명사만 반환한다.\n",
    "def tokenize(x, tokenizer) :\n",
    "    for token in tokenizer.tokenize(x) :\n",
    "        if nouns.get(token) :  # 명사만 선택한다.\n",
    "            yield token\n",
    "        \n",
    "        \n",
    "# 코퍼스를 만들자\n",
    "corpus_ko = [list(tokenize(doc, tokenizer)) for doc in documents_ko]\n",
    "\n",
    "pprint(corpus_ko[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 벡터 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary 생성\n",
    "dictionary_ko = gensim.corpora.Dictionary(corpus_ko)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tfidf Model 생성\n",
    "tfidf_ko = gensim.models.TfidfModel(dictionary=dictionary_ko, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 0.12107436527752928),\n",
       "  (1, 0.11791395471330296),\n",
       "  (2, 0.05895697735665148),\n",
       "  (3, 0.05895697735665148),\n",
       "  (4, 0.9685949222202342),\n",
       "  (5, 0.04035812175917643),\n",
       "  (6, 0.04035812175917643),\n",
       "  (7, 0.04035812175917643),\n",
       "  (8, 0.04035812175917643),\n",
       "  (9, 0.02947848867832574),\n",
       "  (10, 0.04035812175917643),\n",
       "  (11, 0.04035812175917643),\n",
       "  (12, 0.03154354402424213),\n",
       "  (13, 0.08843546603497723),\n",
       "  (14, 0.010879633080850683),\n",
       "  (15, 0.04035812175917643),\n",
       "  (16, 0.04035812175917643),\n",
       "  (17, 0.05895697735665148)],\n",
       " [(3, 0.1810555648125473),\n",
       "  (5, 0.12393889336758655),\n",
       "  (10, 0.12393889336758655),\n",
       "  (13, 0.3621111296250946),\n",
       "  (14, 0.0334111109613129),\n",
       "  (18, 0.1336444438452516),\n",
       "  (19, 0.14530428209412652),\n",
       "  (20, 0.12393889336758655),\n",
       "  (21, 0.048434760698042166),\n",
       "  (22, 0.1810555648125473),\n",
       "  (23, 0.7436333602055194),\n",
       "  (24, 0.09052778240627365),\n",
       "  (25, 0.12393889336758655),\n",
       "  (26, 0.3718166801027597)]]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors = [tfidf_ko[dictionary_ko.doc2bow(vector)] for vector in corpus_ko]\n",
    "vectors[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 벡터 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf-idf 기반 벡터 유사도 \n",
    "from gensim import similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 거리 구하는 함수\n",
    "def distance(a, b, dic) :\n",
    "    index = similarities.MatrixSimilarity([a],num_features=len(dic))\n",
    "    sim = index[b]\n",
    "    return sim[0]*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 해당 벡터가 어떤 곡에 대한 것인지를 출력하기 위해 제목 리스트만 따로 뽑음\n",
    "titles = list(BTS.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: \n",
      "No More Dream\n",
      "['네', '네', '네', '네', '꿈', '아무', '전부', '있어', '꿈', '멀리', '네', '네', '하루', '말', '있어', '준비', '벌써', '너', '네', '네', '네', '네', '거짓말', '자꾸', '네', '네', '매일', '어른들', '시간', '특별', '네', '네', '네', '네', '하루', '네', '네', '네', '네', '거짓말', '자꾸', '네', '네', '꿔', '너', '거짓말', '자꾸', '네', '네']\n",
      "[(0, 0.12107436527752928), (1, 0.11791395471330296), (2, 0.05895697735665148), (3, 0.05895697735665148), (4, 0.9685949222202342), (5, 0.04035812175917643), (6, 0.04035812175917643), (7, 0.04035812175917643), (8, 0.04035812175917643), (9, 0.02947848867832574), (10, 0.04035812175917643), (11, 0.04035812175917643), (12, 0.03154354402424213), (13, 0.08843546603497723), (14, 0.010879633080850683), (15, 0.04035812175917643), (16, 0.04035812175917643), (17, 0.05895697735665148)]\n",
      "-------------------\n",
      "B: \n",
      "봄날\n",
      "['봄날', '더', '있어', '너무', '시간', '우리', '우리', '겨울', '겨울', '시간', '손', '겨울', '봄날', '조금', '더', '수', '조금', '더', '겨울', '봄날', '더', '시간', '우리', '모두', '하루', '조금', '더', '겨울', '조금', '더', '겨울', '봄날', '더']\n",
      "[(9, 0.17977842153624138), (12, 0.0320620719751412), (17, 0.05992614051208046), (19, 0.0320620719751412), (21, 0.2244345038259884), (24, 0.05992614051208046), (30, 0.08204309595838546), (33, 0.17977842153624138), (38, 0.7191136861449655), (39, 0.47940912409664366), (40, 0.11985228102416091), (41, 0.32817238383354186)]\n",
      "===================\n",
      "0.98 % similar\n"
     ]
    }
   ],
   "source": [
    "# 두 벡터 비교 \n",
    "\n",
    "a = 0\n",
    "print(\"A: \")\n",
    "print(titles[a])\n",
    "print(corpus_ko[a])\n",
    "print(vectors[a])\n",
    "\n",
    "print(\"-------------------\")\n",
    "\n",
    "b = 3\n",
    "print(\"B: \")\n",
    "print(titles[b])\n",
    "print(corpus_ko[b])\n",
    "print(vectors[b])\n",
    "\n",
    "sim = distance(vectors[a], vectors[b], dictionary_ko)\n",
    "\n",
    "print(\"===================\")\n",
    "print(round(sim,2),'% similar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: \n",
      "봄날\n",
      "['봄날', '더', '있어', '너무', '시간', '우리', '우리', '겨울', '겨울', '시간', '손', '겨울', '봄날', '조금', '더', '수', '조금', '더', '겨울', '봄날', '더', '시간', '우리', '모두', '하루', '조금', '더', '겨울', '조금', '더', '겨울', '봄날', '더']\n",
      "[(9, 0.17977842153624138), (12, 0.0320620719751412), (17, 0.05992614051208046), (19, 0.0320620719751412), (21, 0.2244345038259884), (24, 0.05992614051208046), (30, 0.08204309595838546), (33, 0.17977842153624138), (38, 0.7191136861449655), (39, 0.47940912409664366), (40, 0.11985228102416091), (41, 0.32817238383354186)]\n",
      "-------------------\n",
      "B: \n",
      "봄날\n",
      "['봄날', '더', '있어', '너무', '시간', '우리', '우리', '겨울', '겨울', '시간', '손', '겨울', '봄날', '조금', '더', '수', '조금', '더', '겨울', '봄날', '더', '시간', '우리', '모두', '하루', '조금', '더', '겨울', '조금', '더', '겨울', '봄날', '더']\n",
      "[(9, 0.17977842153624138), (12, 0.0320620719751412), (17, 0.05992614051208046), (19, 0.0320620719751412), (21, 0.2244345038259884), (24, 0.05992614051208046), (30, 0.08204309595838546), (33, 0.17977842153624138), (38, 0.7191136861449655), (39, 0.47940912409664366), (40, 0.11985228102416091), (41, 0.32817238383354186)]\n",
      "===================\n",
      "100.0 % similar\n"
     ]
    }
   ],
   "source": [
    "a = 3\n",
    "print(\"A: \")\n",
    "print(titles[a])\n",
    "print(corpus_ko[a])\n",
    "print(vectors[a])\n",
    "\n",
    "print(\"-------------------\")\n",
    "\n",
    "b = 3\n",
    "print(\"B: \")\n",
    "print(titles[b])\n",
    "print(corpus_ko[b])\n",
    "print(vectors[b])\n",
    "\n",
    "sim = distance(vectors[a], vectors[b], dictionary_ko)\n",
    "\n",
    "print(\"===================\")\n",
    "print(round(sim,2),'% similar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: \n",
      "DNA\n",
      "['DNA', '내', 'DNA', '우리', '우주', '운명', '내', '내', '운명', '우주', '함께', '운명', 'DNA', '더', 'DNA', '우리', '자꾸', '이상', '사랑', '내', '운명', '우주', '함께', '운명', 'DNA', '운명', '우리', '함께', '운명', 'DNA']\n",
      "[(13, 0.05726670028364734), (18, 0.08454174074334084), (21, 0.030639201032861517), (23, 0.07840213546948255), (33, 0.17180010085094202), (42, 0.6872004034037681), (43, 0.34360020170188404), (44, 0.5488149482863779), (45, 0.07840213546948255), (46, 0.23520640640844767)]\n",
      "-------------------\n",
      "B: \n",
      "작은 것들을 위한 시\n",
      "['행복', '내', '머리', '네', '하늘', '있어', '날개', '너무', '내', '네', '전부', '함께', '조금', '내', '너', '사소', '사소', '특별', '사소', '너무', '운명', '처음', '내', '하늘', '있어', '날개', '너무', '내', '네', '전부', '함께', '조금', '상처', '상처', '때', '날개', '네', '전부', '함께', '조금']\n",
      "[(3, 0.0788862248590622), (4, 0.4320031332304548), (12, 0.08441243830039596), (14, 0.08734367534565451), (16, 0.1080007833076137), (18, 0.14557279224275751), (19, 0.12661865745059395), (26, 0.2160015666152274), (28, 0.1080007833076137), (29, 0.1080007833076137), (36, 0.1080007833076137), (41, 0.32400234992284116), (44, 0.1080007833076137), (46, 0.32400234992284116), (48, 0.32400234992284116), (53, 0.1080007833076137), (54, 0.47331734915437323), (55, 0.3155448994362488)]\n",
      "===================\n",
      "14.78 % similar\n"
     ]
    }
   ],
   "source": [
    "a = 4\n",
    "print(\"A: \")\n",
    "print(titles[a])\n",
    "print(corpus_ko[a])\n",
    "print(vectors[a])\n",
    "\n",
    "print(\"-------------------\")\n",
    "\n",
    "b = 8\n",
    "print(\"B: \")\n",
    "print(titles[b])\n",
    "print(corpus_ko[b])\n",
    "print(vectors[b])\n",
    "\n",
    "sim = distance(vectors[a], vectors[b], dictionary_ko)\n",
    "\n",
    "print(\"===================\")\n",
    "print(round(sim,2),'% similar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Document Embedding\n",
    "    - Doc2Vec\n",
    "* Word Embedding\n",
    "    - Word2Vec\n",
    "    - Glove\n",
    "    - FastText\n",
    "* Sentence Embeding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://nbviewer.jupyter.org/github/psygrammer/psyml/blob/master/nlp_ml/ch04/figures/cap05.png\" width=600 />\n",
    "\n",
    "* 출처 - Applied Text Analysis with Python: Enabling Language-Aware Data Products with Machine Learning - https://www.amazon.com/Applied-Text-Analysis-Python-Language-Aware/dp/1491963042/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Embedding\n",
    "* Doc2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import TaggedDocument, Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['네', '네', '네', '네', '꿈', '아무', '전부', '있어', '꿈', '멀리', '네', '네', '하루', '말', '있어', '준비', '벌써', '너', '네', '네', '네', '네', '거짓말', '자꾸', '네', '네', '매일', '어른들', '시간', '특별', '네', '네', '네', '네', '하루', '네', '네', '네', '네', '거짓말', '자꾸', '네', '네', '꿔', '너', '거짓말', '자꾸', '네', '네'], ['너', '너', '뭣', '전부', '사랑', '사랑', '자꾸', '너무', '자꾸', '내', '내', '내', '아무', '말', '하늘', '하늘', '하늘', '내', '눈물', '더', '잘', '사랑', '자꾸', '너무', '사랑', '사랑', '수', '사랑', '자꾸', '너무']]\n"
     ]
    }
   ],
   "source": [
    "# 코퍼스 확인\n",
    "print(corpus_ko[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs   = [ \n",
    "    TaggedDocument(words, [titles[idx]])\n",
    "        for idx, words in enumerate(corpus_ko)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TaggedDocument(words=['네', '네', '네', '네', '꿈', '아무', '전부', '있어', '꿈', '멀리', '네', '네', '하루', '말', '있어', '준비', '벌써', '너', '네', '네', '네', '네', '거짓말', '자꾸', '네', '네', '매일', '어른들', '시간', '특별', '네', '네', '네', '네', '하루', '네', '네', '네', '네', '거짓말', '자꾸', '네', '네', '꿔', '너', '거짓말', '자꾸', '네', '네'], tags=['No More Dream']), TaggedDocument(words=['너', '너', '뭣', '전부', '사랑', '사랑', '자꾸', '너무', '자꾸', '내', '내', '내', '아무', '말', '하늘', '하늘', '하늘', '내', '눈물', '더', '잘', '사랑', '자꾸', '너무', '사랑', '사랑', '수', '사랑', '자꾸', '너무'], tags=['I NEED U'])]\n"
     ]
    }
   ],
   "source": [
    "print(docs[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2v = Doc2Vec(docs, min_count=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = d2v.docvecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-7.8998436e-04  7.2224741e-04  7.5279403e-04  3.3110820e-03\n",
      " -1.6554599e-04 -5.9665216e-04 -2.2180914e-03 -5.0128563e-03\n",
      "  1.1621257e-03  2.0022118e-03 -4.8898552e-03  7.3794735e-04\n",
      "  4.1524386e-03 -4.3140757e-03 -4.0426860e-03  3.7782415e-04\n",
      "  2.1112207e-03  2.6266535e-03  2.0632350e-03  4.5367228e-03\n",
      " -1.2671099e-03  4.9808430e-03 -3.1151571e-03 -3.5587098e-03\n",
      "  1.4732762e-03 -4.1772700e-03 -1.4950533e-03 -2.4870571e-04\n",
      " -3.3947878e-04  2.2091547e-03  1.7591557e-03  3.6670095e-03\n",
      "  1.7021375e-03 -4.1979896e-03  2.9724082e-03 -4.3629901e-03\n",
      " -4.1979076e-03 -1.4266249e-03 -1.8863283e-03 -3.3406063e-03\n",
      "  4.7751768e-03 -4.0315883e-03 -1.0805968e-03  4.5295265e-03\n",
      " -4.2952015e-03  4.7686393e-03  1.2336487e-03 -2.6753121e-03\n",
      "  1.5505255e-03 -8.0554985e-04  2.1500492e-03 -4.8299232e-03\n",
      " -1.9262416e-03  1.4880367e-03 -3.5560040e-03  7.2937971e-04\n",
      " -4.7101676e-03 -1.6871268e-03 -4.3555470e-03  1.9187623e-03\n",
      "  3.0014194e-03  3.5826860e-03  1.5113975e-03  1.1298138e-04\n",
      "  1.9675798e-03  5.0486955e-03  3.7281148e-03  4.9514892e-03\n",
      "  1.6653826e-04 -8.6212158e-04 -3.5220277e-03 -7.1266823e-04\n",
      " -1.2839524e-03  2.4878862e-04 -1.6545451e-03 -2.8902467e-03\n",
      "  3.0122499e-03 -1.7452137e-04  2.4725806e-03  3.5558604e-03\n",
      " -3.9999932e-03  2.7950730e-03  3.7495613e-03 -2.7036096e-03\n",
      " -1.7613155e-03  5.9870385e-05 -1.9295951e-03 -1.7735887e-03\n",
      "  1.7448518e-03 -3.5504568e-03  5.2410695e-03  1.2768143e-03\n",
      "  3.4357600e-03  1.2233979e-03  1.9711624e-03  3.2813912e-03\n",
      "  1.2602648e-03 -5.3995539e-04 -3.8766033e-05  4.4148448e-03]\n",
      "[-8.9804374e-04 -2.6019005e-04 -1.5940175e-05  1.6910994e-03\n",
      " -4.4240695e-03 -1.6376215e-03 -1.7818178e-03  3.2514727e-03\n",
      "  2.1043676e-03  9.1995223e-04  3.6812867e-03 -1.9783948e-03\n",
      " -1.1994003e-03 -2.4375913e-03  3.5065596e-03  2.7465071e-03\n",
      "  1.2789252e-03 -6.4379798e-04 -4.3024397e-03 -4.1966834e-03\n",
      "  2.1654200e-03  6.1497289e-05  4.5088595e-03 -3.4810228e-03\n",
      "  4.4927178e-03 -3.1398851e-03 -3.2976744e-04  4.2752274e-03\n",
      " -3.4299465e-03  2.9241662e-03 -3.3628722e-03  1.9994457e-03\n",
      "  2.4566397e-03  1.4649295e-03  4.5519034e-04 -3.2281637e-04\n",
      "  2.6609660e-03 -4.9924194e-03  1.2999949e-03  4.5594783e-03\n",
      " -4.0286183e-03  2.4323617e-03  1.1593471e-03  4.8431032e-03\n",
      " -4.3815165e-04 -3.1435704e-03 -4.8614820e-03  1.0799424e-03\n",
      "  2.2438122e-03  2.1682675e-03  3.2614532e-03  2.5866656e-03\n",
      "  7.6546840e-04  3.9832466e-04  3.2226311e-05 -2.7671640e-03\n",
      " -2.2583418e-03 -4.8463312e-03 -2.5002544e-03  2.5331827e-03\n",
      "  4.6534408e-03 -1.9513414e-03  2.3444104e-03  4.9777273e-03\n",
      "  2.2629262e-03 -4.0155612e-03  3.5532389e-03  2.4686297e-03\n",
      "  2.6744278e-03 -1.1237761e-03 -2.3710511e-03 -1.3420244e-03\n",
      "  3.2661299e-03 -3.0587921e-03  3.9259996e-03 -4.6913341e-05\n",
      " -2.3125554e-03  2.7939538e-03 -1.0322972e-03 -2.6301092e-03\n",
      " -3.5538641e-03 -4.0838826e-03 -1.9770938e-03 -4.7334386e-05\n",
      " -2.8232739e-03 -4.2145411e-03  2.3113135e-03  1.8636333e-03\n",
      "  3.6770988e-03  1.8823927e-03  4.1268086e-03  1.6206753e-03\n",
      " -2.1897228e-03 -3.1935163e-03  1.1640277e-03  4.0001846e-03\n",
      "  2.3686001e-03 -2.0758440e-03 -6.4924949e-05 -3.2147893e-03]\n",
      "[ 1.8185569e-03  3.7705558e-03 -2.0004562e-03 -3.1059424e-03\n",
      "  1.7977755e-03 -3.2670045e-04  1.8348427e-03  4.9200811e-04\n",
      "  1.1589624e-03  3.8557851e-03 -2.8918730e-04 -4.6352402e-04\n",
      " -3.1999319e-03  1.3540634e-03 -1.0546729e-03 -2.2874009e-03\n",
      " -3.9425809e-03 -1.7236860e-03 -3.7720881e-03  1.2676620e-03\n",
      " -1.3517210e-03 -1.4404003e-03  5.0152079e-03  7.5909169e-04\n",
      " -2.9019343e-03  3.7331665e-03  4.7541796e-03 -2.1840415e-03\n",
      "  1.9557551e-03  4.7591277e-03  9.8450715e-04  2.6216421e-03\n",
      "  3.3734892e-03  3.2466038e-03 -1.4424449e-03  2.9778439e-03\n",
      "  3.9733038e-03 -3.4523837e-03  2.3085056e-03  1.2204387e-03\n",
      "  4.0241433e-05  4.0418081e-04 -2.1692379e-03 -2.9905366e-03\n",
      "  4.4628270e-03  1.5857520e-03  1.6627668e-03 -4.2101932e-03\n",
      " -1.6872541e-03  4.9037510e-04 -1.4793532e-03 -5.1989527e-03\n",
      " -4.9637598e-03 -1.8395677e-03  2.4055145e-03  4.5690956e-03\n",
      " -3.7974536e-03 -1.5666102e-03  3.2177758e-03 -1.1797964e-03\n",
      "  2.6419435e-03 -1.6575005e-03 -2.6554067e-03  1.5290244e-03\n",
      "  4.0342566e-03  4.1463617e-03  1.6411764e-03  2.7221011e-03\n",
      "  5.2496647e-03 -4.7250311e-03 -3.2586702e-03  4.2183273e-03\n",
      " -3.8930215e-03  2.0128514e-03  4.7455109e-03  2.3185024e-03\n",
      " -2.1655923e-03  2.9610365e-03  3.9390288e-03  4.2693974e-03\n",
      " -4.2048078e-03 -5.3506731e-03 -4.9336818e-03  2.8153236e-03\n",
      "  9.0022666e-05 -5.8364443e-05 -3.0097011e-03 -3.8990304e-03\n",
      "  3.4522489e-03  2.3957533e-03  2.0895437e-03 -1.1929049e-03\n",
      "  2.6344252e-04  5.0308486e-03  4.9372534e-03  4.7640926e-03\n",
      " -4.2308741e-03 -2.2670252e-03 -3.6132890e-03  1.2120195e-03]\n"
     ]
    }
   ],
   "source": [
    "for i in range(3) :\n",
    "    print(vectors[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc2vec기반 벡터 유사도 \n",
    "def distance(a_doctag, b_doctag, vectors) :\n",
    "    sim = vectors.similarity(a_doctag, b_doctag)\n",
    "    return sim*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: \n",
      "No More Dream\n",
      "['네', '네', '네', '네', '꿈', '아무', '전부', '있어', '꿈', '멀리', '네', '네', '하루', '말', '있어', '준비', '벌써', '너', '네', '네', '네', '네', '거짓말', '자꾸', '네', '네', '매일', '어른들', '시간', '특별', '네', '네', '네', '네', '하루', '네', '네', '네', '네', '거짓말', '자꾸', '네', '네', '꿔', '너', '거짓말', '자꾸', '네', '네']\n",
      "[-7.8998436e-04  7.2224741e-04  7.5279403e-04  3.3110820e-03\n",
      " -1.6554599e-04 -5.9665216e-04 -2.2180914e-03 -5.0128563e-03\n",
      "  1.1621257e-03  2.0022118e-03 -4.8898552e-03  7.3794735e-04\n",
      "  4.1524386e-03 -4.3140757e-03 -4.0426860e-03  3.7782415e-04\n",
      "  2.1112207e-03  2.6266535e-03  2.0632350e-03  4.5367228e-03\n",
      " -1.2671099e-03  4.9808430e-03 -3.1151571e-03 -3.5587098e-03\n",
      "  1.4732762e-03 -4.1772700e-03 -1.4950533e-03 -2.4870571e-04\n",
      " -3.3947878e-04  2.2091547e-03  1.7591557e-03  3.6670095e-03\n",
      "  1.7021375e-03 -4.1979896e-03  2.9724082e-03 -4.3629901e-03\n",
      " -4.1979076e-03 -1.4266249e-03 -1.8863283e-03 -3.3406063e-03\n",
      "  4.7751768e-03 -4.0315883e-03 -1.0805968e-03  4.5295265e-03\n",
      " -4.2952015e-03  4.7686393e-03  1.2336487e-03 -2.6753121e-03\n",
      "  1.5505255e-03 -8.0554985e-04  2.1500492e-03 -4.8299232e-03\n",
      " -1.9262416e-03  1.4880367e-03 -3.5560040e-03  7.2937971e-04\n",
      " -4.7101676e-03 -1.6871268e-03 -4.3555470e-03  1.9187623e-03\n",
      "  3.0014194e-03  3.5826860e-03  1.5113975e-03  1.1298138e-04\n",
      "  1.9675798e-03  5.0486955e-03  3.7281148e-03  4.9514892e-03\n",
      "  1.6653826e-04 -8.6212158e-04 -3.5220277e-03 -7.1266823e-04\n",
      " -1.2839524e-03  2.4878862e-04 -1.6545451e-03 -2.8902467e-03\n",
      "  3.0122499e-03 -1.7452137e-04  2.4725806e-03  3.5558604e-03\n",
      " -3.9999932e-03  2.7950730e-03  3.7495613e-03 -2.7036096e-03\n",
      " -1.7613155e-03  5.9870385e-05 -1.9295951e-03 -1.7735887e-03\n",
      "  1.7448518e-03 -3.5504568e-03  5.2410695e-03  1.2768143e-03\n",
      "  3.4357600e-03  1.2233979e-03  1.9711624e-03  3.2813912e-03\n",
      "  1.2602648e-03 -5.3995539e-04 -3.8766033e-05  4.4148448e-03]\n",
      "---------------------\n",
      "B: \n",
      "봄날\n",
      "['봄날', '더', '있어', '너무', '시간', '우리', '우리', '겨울', '겨울', '시간', '손', '겨울', '봄날', '조금', '더', '수', '조금', '더', '겨울', '봄날', '더', '시간', '우리', '모두', '하루', '조금', '더', '겨울', '조금', '더', '겨울', '봄날', '더']\n",
      "[-3.7882584e-03 -1.7288388e-03  2.0746761e-03 -2.5114072e-03\n",
      " -3.4897325e-03  1.8923049e-03  1.1728042e-03  3.6649787e-04\n",
      " -1.5953288e-03  5.3563365e-04 -2.2353516e-03  2.4905426e-03\n",
      "  2.5519875e-03 -3.2564029e-03 -4.8096594e-03 -2.9567881e-03\n",
      "  3.7387132e-03  1.1639760e-03 -4.1232174e-03  4.5650471e-03\n",
      " -2.7486365e-03 -1.7876422e-03  1.6164050e-03  2.8504920e-03\n",
      "  4.7876230e-03 -4.5712106e-03 -2.5195009e-03  2.4036004e-04\n",
      "  2.8574816e-03 -9.9775160e-04  1.5266036e-03  9.4945979e-05\n",
      " -3.2246055e-03 -4.4487547e-03  5.8567477e-04 -3.6685131e-03\n",
      "  3.0866382e-03 -3.7679628e-03  6.8011490e-04 -6.7939912e-04\n",
      " -1.2760557e-03 -8.3443732e-04  3.8418849e-03 -1.5382431e-03\n",
      " -4.6656071e-03 -2.3612375e-03 -9.1492292e-04  5.9326412e-04\n",
      " -4.7833077e-03 -2.6112140e-04 -4.5117880e-03  3.8066083e-03\n",
      " -4.4272309e-03 -1.2666084e-03 -2.5107788e-03 -3.3297569e-03\n",
      " -2.9788711e-03  5.1322590e-05 -3.0751615e-03  3.8424246e-03\n",
      " -4.2473576e-03 -2.2195748e-03 -3.0289595e-03  3.2748515e-03\n",
      " -4.2068991e-03  4.5537008e-03 -3.3340557e-03 -1.4640221e-03\n",
      " -4.1550398e-03 -1.1336937e-04 -2.7506705e-03  3.2971252e-03\n",
      "  1.8923190e-04  2.5725567e-03  3.9257393e-03 -4.4109087e-04\n",
      " -6.5472233e-04  2.1880111e-03 -1.1236900e-03  2.9885198e-03\n",
      "  4.0860851e-03 -3.8348574e-03  2.5136715e-03  2.4066048e-03\n",
      "  3.8678455e-03 -1.5143570e-03 -3.3197419e-03 -2.5779619e-03\n",
      " -4.4755903e-03  2.6223219e-03  1.2582382e-03  2.5528786e-03\n",
      "  1.7335679e-04  4.5177499e-03  2.8375883e-03 -4.5547369e-03\n",
      " -3.1846594e-03 -3.1775183e-03  7.6870149e-04 -1.4684054e-03]\n",
      "=====================\n",
      "5.28 % similar\n"
     ]
    }
   ],
   "source": [
    "a = 0\n",
    "a_doctag = vectors.index2entity[a]\n",
    "print(\"A: \")\n",
    "print(a_doctag)\n",
    "print(corpus_ko[a])\n",
    "print(vectors[a])\n",
    "\n",
    "print(\"---------------------\")\n",
    "\n",
    "b = 3\n",
    "b_doctag = vectors.index2entity[b]\n",
    "print(\"B: \")\n",
    "print(b_doctag)\n",
    "print(corpus_ko[b])\n",
    "print(vectors[b])\n",
    "\n",
    "print(\"=====================\")\n",
    "\n",
    "sim = distance(a_doctag, b_doctag, vectors)\n",
    "\n",
    "print(round(sim,2),'% similar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: \n",
      "봄날\n",
      "['봄날', '더', '있어', '너무', '시간', '우리', '우리', '겨울', '겨울', '시간', '손', '겨울', '봄날', '조금', '더', '수', '조금', '더', '겨울', '봄날', '더', '시간', '우리', '모두', '하루', '조금', '더', '겨울', '조금', '더', '겨울', '봄날', '더']\n",
      "[-3.7882584e-03 -1.7288388e-03  2.0746761e-03 -2.5114072e-03\n",
      " -3.4897325e-03  1.8923049e-03  1.1728042e-03  3.6649787e-04\n",
      " -1.5953288e-03  5.3563365e-04 -2.2353516e-03  2.4905426e-03\n",
      "  2.5519875e-03 -3.2564029e-03 -4.8096594e-03 -2.9567881e-03\n",
      "  3.7387132e-03  1.1639760e-03 -4.1232174e-03  4.5650471e-03\n",
      " -2.7486365e-03 -1.7876422e-03  1.6164050e-03  2.8504920e-03\n",
      "  4.7876230e-03 -4.5712106e-03 -2.5195009e-03  2.4036004e-04\n",
      "  2.8574816e-03 -9.9775160e-04  1.5266036e-03  9.4945979e-05\n",
      " -3.2246055e-03 -4.4487547e-03  5.8567477e-04 -3.6685131e-03\n",
      "  3.0866382e-03 -3.7679628e-03  6.8011490e-04 -6.7939912e-04\n",
      " -1.2760557e-03 -8.3443732e-04  3.8418849e-03 -1.5382431e-03\n",
      " -4.6656071e-03 -2.3612375e-03 -9.1492292e-04  5.9326412e-04\n",
      " -4.7833077e-03 -2.6112140e-04 -4.5117880e-03  3.8066083e-03\n",
      " -4.4272309e-03 -1.2666084e-03 -2.5107788e-03 -3.3297569e-03\n",
      " -2.9788711e-03  5.1322590e-05 -3.0751615e-03  3.8424246e-03\n",
      " -4.2473576e-03 -2.2195748e-03 -3.0289595e-03  3.2748515e-03\n",
      " -4.2068991e-03  4.5537008e-03 -3.3340557e-03 -1.4640221e-03\n",
      " -4.1550398e-03 -1.1336937e-04 -2.7506705e-03  3.2971252e-03\n",
      "  1.8923190e-04  2.5725567e-03  3.9257393e-03 -4.4109087e-04\n",
      " -6.5472233e-04  2.1880111e-03 -1.1236900e-03  2.9885198e-03\n",
      "  4.0860851e-03 -3.8348574e-03  2.5136715e-03  2.4066048e-03\n",
      "  3.8678455e-03 -1.5143570e-03 -3.3197419e-03 -2.5779619e-03\n",
      " -4.4755903e-03  2.6223219e-03  1.2582382e-03  2.5528786e-03\n",
      "  1.7335679e-04  4.5177499e-03  2.8375883e-03 -4.5547369e-03\n",
      " -3.1846594e-03 -3.1775183e-03  7.6870149e-04 -1.4684054e-03]\n",
      "---------------------\n",
      "B: \n",
      "봄날\n",
      "['봄날', '더', '있어', '너무', '시간', '우리', '우리', '겨울', '겨울', '시간', '손', '겨울', '봄날', '조금', '더', '수', '조금', '더', '겨울', '봄날', '더', '시간', '우리', '모두', '하루', '조금', '더', '겨울', '조금', '더', '겨울', '봄날', '더']\n",
      "[-3.7882584e-03 -1.7288388e-03  2.0746761e-03 -2.5114072e-03\n",
      " -3.4897325e-03  1.8923049e-03  1.1728042e-03  3.6649787e-04\n",
      " -1.5953288e-03  5.3563365e-04 -2.2353516e-03  2.4905426e-03\n",
      "  2.5519875e-03 -3.2564029e-03 -4.8096594e-03 -2.9567881e-03\n",
      "  3.7387132e-03  1.1639760e-03 -4.1232174e-03  4.5650471e-03\n",
      " -2.7486365e-03 -1.7876422e-03  1.6164050e-03  2.8504920e-03\n",
      "  4.7876230e-03 -4.5712106e-03 -2.5195009e-03  2.4036004e-04\n",
      "  2.8574816e-03 -9.9775160e-04  1.5266036e-03  9.4945979e-05\n",
      " -3.2246055e-03 -4.4487547e-03  5.8567477e-04 -3.6685131e-03\n",
      "  3.0866382e-03 -3.7679628e-03  6.8011490e-04 -6.7939912e-04\n",
      " -1.2760557e-03 -8.3443732e-04  3.8418849e-03 -1.5382431e-03\n",
      " -4.6656071e-03 -2.3612375e-03 -9.1492292e-04  5.9326412e-04\n",
      " -4.7833077e-03 -2.6112140e-04 -4.5117880e-03  3.8066083e-03\n",
      " -4.4272309e-03 -1.2666084e-03 -2.5107788e-03 -3.3297569e-03\n",
      " -2.9788711e-03  5.1322590e-05 -3.0751615e-03  3.8424246e-03\n",
      " -4.2473576e-03 -2.2195748e-03 -3.0289595e-03  3.2748515e-03\n",
      " -4.2068991e-03  4.5537008e-03 -3.3340557e-03 -1.4640221e-03\n",
      " -4.1550398e-03 -1.1336937e-04 -2.7506705e-03  3.2971252e-03\n",
      "  1.8923190e-04  2.5725567e-03  3.9257393e-03 -4.4109087e-04\n",
      " -6.5472233e-04  2.1880111e-03 -1.1236900e-03  2.9885198e-03\n",
      "  4.0860851e-03 -3.8348574e-03  2.5136715e-03  2.4066048e-03\n",
      "  3.8678455e-03 -1.5143570e-03 -3.3197419e-03 -2.5779619e-03\n",
      " -4.4755903e-03  2.6223219e-03  1.2582382e-03  2.5528786e-03\n",
      "  1.7335679e-04  4.5177499e-03  2.8375883e-03 -4.5547369e-03\n",
      " -3.1846594e-03 -3.1775183e-03  7.6870149e-04 -1.4684054e-03]\n",
      "=====================\n",
      "100.0 % similar\n"
     ]
    }
   ],
   "source": [
    "a = 3\n",
    "a_doctag = vectors.index2entity[a]\n",
    "print(\"A: \")\n",
    "print(a_doctag)\n",
    "print(corpus_ko[a])\n",
    "print(vectors[a])\n",
    "\n",
    "print(\"---------------------\")\n",
    "\n",
    "b = 3\n",
    "print(\"B: \")\n",
    "b_doctag = vectors.index2entity[b]\n",
    "print(b_doctag)\n",
    "print(corpus_ko[b])\n",
    "print(vectors[b])\n",
    "\n",
    "print(\"=====================\")\n",
    "\n",
    "sim = distance(a_doctag, b_doctag, vectors)\n",
    "\n",
    "print(round(sim,2),'% similar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: \n",
      "DNA\n",
      "['DNA', '내', 'DNA', '우리', '우주', '운명', '내', '내', '운명', '우주', '함께', '운명', 'DNA', '더', 'DNA', '우리', '자꾸', '이상', '사랑', '내', '운명', '우주', '함께', '운명', 'DNA', '운명', '우리', '함께', '운명', 'DNA']\n",
      "[ 2.1743893e-03  1.9190835e-03 -1.6003055e-04 -1.6911449e-03\n",
      "  1.5877279e-03  1.2325135e-03 -1.7678938e-05 -3.3111908e-03\n",
      "  1.2617743e-04  3.6203719e-03 -4.0152287e-03 -2.3789858e-03\n",
      "  4.2678546e-03 -1.4107731e-03  4.6450345e-04 -4.9895192e-05\n",
      "  4.5410113e-04 -3.5198582e-03 -3.0533033e-03  4.9649063e-03\n",
      " -4.1650683e-03  5.0260895e-03  3.6281052e-03  3.5725636e-05\n",
      " -1.0537121e-03 -1.9334556e-04  5.2469433e-04  4.6473122e-03\n",
      " -4.3411455e-05 -3.3767954e-03 -4.1622366e-03 -2.9033867e-03\n",
      "  1.1157270e-03 -2.2784849e-03 -2.0726982e-03  4.4518290e-03\n",
      "  4.3244445e-04 -1.1764695e-03 -4.4763046e-03 -2.3522745e-04\n",
      "  8.7763081e-05 -4.3056728e-03 -2.1030386e-03 -3.4990835e-03\n",
      " -1.3246523e-03 -8.3456485e-04 -6.9528533e-04 -3.6976128e-03\n",
      "  4.4456453e-04  1.1081741e-03  3.9693872e-03 -4.7257491e-03\n",
      "  1.6538034e-03  2.0426919e-03 -3.5236005e-03 -8.7330624e-04\n",
      " -3.0201587e-03  4.5933840e-03 -1.9171779e-03  4.1666529e-03\n",
      "  1.8639291e-03 -4.1361842e-03 -1.2490394e-03  9.5874036e-04\n",
      " -1.0404533e-03  1.3165459e-03  2.3396634e-03 -1.3324240e-03\n",
      "  1.0426460e-03  2.4032004e-03  2.6282487e-03  7.9900119e-04\n",
      " -1.1819526e-04 -4.3615657e-03  3.9177528e-03  2.9410382e-03\n",
      "  3.0193352e-03 -4.8316396e-03 -1.3853569e-03 -3.6319394e-03\n",
      "  3.1284462e-03 -4.3592211e-03 -6.1784021e-04 -2.8712109e-03\n",
      "  2.4284655e-04 -2.8048479e-03 -4.2012413e-03 -7.0676376e-04\n",
      "  4.7412999e-03 -3.1438249e-03 -1.0501756e-03 -7.9782418e-04\n",
      "  8.8466576e-04 -6.2396302e-04  2.9962547e-03  6.4995734e-04\n",
      " -4.1498598e-03 -1.8147704e-03 -2.1732617e-03 -3.5196696e-03]\n",
      "---------------------\n",
      "B: \n",
      "봄날\n",
      "['행복', '내', '머리', '네', '하늘', '있어', '날개', '너무', '내', '네', '전부', '함께', '조금', '내', '너', '사소', '사소', '특별', '사소', '너무', '운명', '처음', '내', '하늘', '있어', '날개', '너무', '내', '네', '전부', '함께', '조금', '상처', '상처', '때', '날개', '네', '전부', '함께', '조금']\n",
      "[ 3.46761546e-03  2.18992391e-05 -3.65320942e-03 -2.37685023e-03\n",
      "  1.89304887e-03  3.15011072e-04 -4.09068353e-03 -4.33888286e-03\n",
      "  3.44724348e-03 -4.94013766e-05  4.24597273e-03 -3.89909372e-03\n",
      "  5.22752351e-04  4.46271850e-03  2.09632050e-03  1.29287469e-03\n",
      " -2.72028707e-03  5.05689997e-03  2.90918723e-03  4.49196901e-03\n",
      " -4.95789992e-03  1.93239921e-05  3.56471818e-03 -4.28691367e-03\n",
      " -7.16024311e-04  3.92268272e-03  4.82445816e-03  3.16579686e-03\n",
      " -4.18306468e-03 -2.01790710e-03 -4.16060304e-03  1.06867950e-03\n",
      " -4.31689154e-03 -4.03033616e-03 -2.28817877e-03 -2.38058652e-04\n",
      "  4.74633649e-03 -1.74056122e-03 -3.60277342e-03  3.51140997e-03\n",
      " -4.30795830e-03 -3.68019333e-03 -4.43927431e-03 -4.22413787e-03\n",
      "  4.45938995e-03 -1.89431629e-03  7.74321496e-04 -3.05026979e-03\n",
      "  2.99795158e-03  3.88204353e-03 -2.44211545e-03 -1.97624555e-03\n",
      "  2.48081028e-03 -3.88829154e-03 -3.50965594e-04 -1.66209729e-03\n",
      " -3.12336651e-03  2.24848278e-03  4.98456834e-03 -2.63438025e-03\n",
      "  2.04300834e-03  2.50874902e-03  9.06882109e-04  1.09067478e-03\n",
      " -2.83839554e-03  2.16615634e-04 -2.70111300e-03 -6.72370312e-04\n",
      " -1.17272243e-03 -3.49775446e-03  3.97109473e-03 -2.98836012e-03\n",
      " -3.02249449e-03 -6.19506522e-04  3.46977636e-03  5.37870685e-04\n",
      "  4.88634594e-03 -2.12090113e-03  3.10351956e-03 -3.91813973e-03\n",
      " -4.98124259e-03 -2.65340810e-03 -3.67459106e-05  4.95160371e-03\n",
      " -1.64452731e-03 -4.51333541e-03  4.69700573e-03  1.34158181e-03\n",
      " -1.45084763e-04  1.48567068e-03  3.38935410e-03  2.18659104e-03\n",
      "  3.64297885e-03  1.03434210e-03  2.73084641e-03  4.24774969e-03\n",
      " -1.08575914e-04 -1.60998767e-04 -4.04793565e-04 -4.90443222e-03]\n",
      "=====================\n",
      "6.64 % similar\n"
     ]
    }
   ],
   "source": [
    "a = 4\n",
    "a_doctag = vectors.index2entity[a]\n",
    "print(\"A: \")\n",
    "print(a_doctag)\n",
    "print(corpus_ko[a])\n",
    "print(vectors[a])\n",
    "\n",
    "print(\"---------------------\")\n",
    "\n",
    "b = 8\n",
    "print(\"B: \")\n",
    "print(b_doctag)\n",
    "print(corpus_ko[b])\n",
    "print(vectors[b])\n",
    "\n",
    "print(\"=====================\")\n",
    "\n",
    "sim = distance(a_doctag, b_doctag, vectors)\n",
    "\n",
    "print(round(sim,2),'% similar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### plotly로 시각화 함수 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "from sklearn.decomposition import IncrementalPCA    # inital reduction\n",
    "from sklearn.manifold import TSNE                   # final reduction\n",
    "import numpy as np                                  # array handling\n",
    "\n",
    "from plotly.offline import init_notebook_mode, iplot, plot\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "def visualize_embeddings(vectors, labels, plot_in_notebook = True):\n",
    "\n",
    "    num_dimensions = 2  # final num dimensions (2D, 3D, etc)\n",
    "\n",
    "    # convert both lists into numpy vectors for reduction\n",
    "    vectors = np.asarray(vectors)\n",
    "    labels = np.asarray(labels)\n",
    "    \n",
    "    # reduce using t-SNE\n",
    "    logging.info('starting tSNE dimensionality reduction. This may take some time.')\n",
    "    tsne = TSNE(n_components=num_dimensions, random_state=0)\n",
    "    vectors = tsne.fit_transform(vectors)\n",
    "\n",
    "    x_vals = [v[0] for v in vectors]\n",
    "    y_vals = [v[1] for v in vectors]\n",
    "        \n",
    "    # Create a trace\n",
    "    trace = go.Scatter(\n",
    "        x=x_vals,\n",
    "        y=y_vals,\n",
    "        mode='text',\n",
    "        text=labels\n",
    "        )\n",
    "    \n",
    "    data = [trace]\n",
    "    \n",
    "    logging.info('All done. Plotting.')\n",
    "    \n",
    "    if plot_in_notebook:\n",
    "        init_notebook_mode(connected=True)\n",
    "        iplot(data, filename='word-embedding-plot')\n",
    "    else:\n",
    "        plot(data, filename='word-embedding-plot.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No More Dream\n",
      "[-7.8998436e-04  7.2224741e-04  7.5279403e-04  3.3110820e-03\n",
      " -1.6554599e-04 -5.9665216e-04 -2.2180914e-03 -5.0128563e-03\n",
      "  1.1621257e-03  2.0022118e-03 -4.8898552e-03  7.3794735e-04\n",
      "  4.1524386e-03 -4.3140757e-03 -4.0426860e-03  3.7782415e-04\n",
      "  2.1112207e-03  2.6266535e-03  2.0632350e-03  4.5367228e-03\n",
      " -1.2671099e-03  4.9808430e-03 -3.1151571e-03 -3.5587098e-03\n",
      "  1.4732762e-03 -4.1772700e-03 -1.4950533e-03 -2.4870571e-04\n",
      " -3.3947878e-04  2.2091547e-03  1.7591557e-03  3.6670095e-03\n",
      "  1.7021375e-03 -4.1979896e-03  2.9724082e-03 -4.3629901e-03\n",
      " -4.1979076e-03 -1.4266249e-03 -1.8863283e-03 -3.3406063e-03\n",
      "  4.7751768e-03 -4.0315883e-03 -1.0805968e-03  4.5295265e-03\n",
      " -4.2952015e-03  4.7686393e-03  1.2336487e-03 -2.6753121e-03\n",
      "  1.5505255e-03 -8.0554985e-04  2.1500492e-03 -4.8299232e-03\n",
      " -1.9262416e-03  1.4880367e-03 -3.5560040e-03  7.2937971e-04\n",
      " -4.7101676e-03 -1.6871268e-03 -4.3555470e-03  1.9187623e-03\n",
      "  3.0014194e-03  3.5826860e-03  1.5113975e-03  1.1298138e-04\n",
      "  1.9675798e-03  5.0486955e-03  3.7281148e-03  4.9514892e-03\n",
      "  1.6653826e-04 -8.6212158e-04 -3.5220277e-03 -7.1266823e-04\n",
      " -1.2839524e-03  2.4878862e-04 -1.6545451e-03 -2.8902467e-03\n",
      "  3.0122499e-03 -1.7452137e-04  2.4725806e-03  3.5558604e-03\n",
      " -3.9999932e-03  2.7950730e-03  3.7495613e-03 -2.7036096e-03\n",
      " -1.7613155e-03  5.9870385e-05 -1.9295951e-03 -1.7735887e-03\n",
      "  1.7448518e-03 -3.5504568e-03  5.2410695e-03  1.2768143e-03\n",
      "  3.4357600e-03  1.2233979e-03  1.9711624e-03  3.2813912e-03\n",
      "  1.2602648e-03 -5.3995539e-04 -3.8766033e-05  4.4148448e-03]\n"
     ]
    }
   ],
   "source": [
    "labels = titles\n",
    "vectors = [d2v.docvecs[title] for title in labels]\n",
    "\n",
    "print(labels[0])\n",
    "print(vectors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-23 21:09:09,324 : INFO : starting tSNE dimensionality reduction. This may take some time.\n",
      "2019-04-23 21:09:09,629 : INFO : All done. Plotting.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": false
       },
       "data": [
        {
         "mode": "text",
         "text": [
          "No More Dream",
          "I NEED U",
          "쩔어",
          "봄날",
          "DNA",
          "피 땀 눈물",
          "불타오르네",
          "IDOL",
          "작은 것들을 위한 시"
         ],
         "type": "scatter",
         "uid": "924f7ba9-6f57-4819-93f1-ce01a4b55fc2",
         "x": [
          133.53268432617188,
          326.6684265136719,
          269.13763427734375,
          -381.7850341796875,
          -107.82098388671875,
          -37.70338821411133,
          69.02787780761719,
          -262.59246826171875,
          -151.4911346435547
         ],
         "y": [
          -238.9559783935547,
          -29.38593101501465,
          283.1435852050781,
          -87.244873046875,
          -64.3395004272461,
          364.44622802734375,
          97.64485168457031,
          189.64996337890625,
          -335.9482116699219
         ]
        }
       ],
       "layout": {}
      },
      "text/html": [
       "<div id=\"7cfe8040-a77f-4966-a7f6-7ec460633881\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";\n",
       "if (document.getElementById(\"7cfe8040-a77f-4966-a7f6-7ec460633881\")) {\n",
       "    Plotly.newPlot(\"7cfe8040-a77f-4966-a7f6-7ec460633881\", [{\"mode\": \"text\", \"text\": [\"No More Dream\", \"I NEED U\", \"\\uca54\\uc5b4\", \"\\ubd04\\ub0a0\", \"DNA\", \"\\ud53c \\ub540 \\ub208\\ubb3c\", \"\\ubd88\\ud0c0\\uc624\\ub974\\ub124\", \"IDOL\", \"\\uc791\\uc740 \\uac83\\ub4e4\\uc744 \\uc704\\ud55c \\uc2dc\"], \"x\": [133.53268432617188, 326.6684265136719, 269.13763427734375, -381.7850341796875, -107.82098388671875, -37.70338821411133, 69.02787780761719, -262.59246826171875, -151.4911346435547], \"y\": [-238.9559783935547, -29.38593101501465, 283.1435852050781, -87.244873046875, -64.3395004272461, 364.44622802734375, 97.64485168457031, 189.64996337890625, -335.9482116699219], \"type\": \"scatter\", \"uid\": \"84921c77-b153-4cab-9df9-766cdb189b5f\"}], {}, {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"}); \n",
       "}\n",
       "});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){if (document.getElementById(\"7cfe8040-a77f-4966-a7f6-7ec460633881\")) {window._Plotly.Plots.resize(document.getElementById(\"7cfe8040-a77f-4966-a7f6-7ec460633881\"));};})</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"7cfe8040-a77f-4966-a7f6-7ec460633881\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";\n",
       "if (document.getElementById(\"7cfe8040-a77f-4966-a7f6-7ec460633881\")) {\n",
       "    Plotly.newPlot(\"7cfe8040-a77f-4966-a7f6-7ec460633881\", [{\"mode\": \"text\", \"text\": [\"No More Dream\", \"I NEED U\", \"\\uca54\\uc5b4\", \"\\ubd04\\ub0a0\", \"DNA\", \"\\ud53c \\ub540 \\ub208\\ubb3c\", \"\\ubd88\\ud0c0\\uc624\\ub974\\ub124\", \"IDOL\", \"\\uc791\\uc740 \\uac83\\ub4e4\\uc744 \\uc704\\ud55c \\uc2dc\"], \"x\": [133.53268432617188, 326.6684265136719, 269.13763427734375, -381.7850341796875, -107.82098388671875, -37.70338821411133, 69.02787780761719, -262.59246826171875, -151.4911346435547], \"y\": [-238.9559783935547, -29.38593101501465, 283.1435852050781, -87.244873046875, -64.3395004272461, 364.44622802734375, 97.64485168457031, 189.64996337890625, -335.9482116699219], \"type\": \"scatter\", \"uid\": \"84921c77-b153-4cab-9df9-766cdb189b5f\"}], {}, {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"}); \n",
       "}\n",
       "});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){if (document.getElementById(\"7cfe8040-a77f-4966-a7f6-7ec460633881\")) {window._Plotly.Plots.resize(document.getElementById(\"7cfe8040-a77f-4966-a7f6-7ec460633881\"));};})</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_embeddings(vectors, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding\n",
    "* Word2Vec\n",
    "* Glove\n",
    "* FastText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 출처 - models.word2vec – Deep learning with word2vec - https://radimrehurek.com/gensim/models/word2vec.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 영어 예시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['my', 'name', 'is', 'jamie'], ['jamie', 'is', 'cute']]\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "corpus_en = [[\"my\", \"name\", \"is\", \"jamie\"], [\"jamie\", \"is\", \"cute\"]]\n",
    "\n",
    "pprint(corpus_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 초기화\n",
    "w2v_en = gensim.models.Word2Vec(min_count=0) # 예) Word2Vec(sentences, size=100, window=5, min_count=5, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-23 21:10:32,693 : INFO : collecting all words and their counts\n",
      "2019-04-23 21:10:32,694 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-04-23 21:10:32,695 : INFO : collected 5 word types from a corpus of 7 raw words and 2 sentences\n",
      "2019-04-23 21:10:32,696 : INFO : Loading a fresh vocabulary\n",
      "2019-04-23 21:10:32,697 : INFO : min_count=0 retains 5 unique words (100% of original 5, drops 0)\n",
      "2019-04-23 21:10:32,698 : INFO : min_count=0 leaves 7 word corpus (100% of original 7, drops 0)\n",
      "2019-04-23 21:10:32,698 : INFO : deleting the raw counts dictionary of 5 items\n",
      "2019-04-23 21:10:32,699 : INFO : sample=0.001 downsamples 5 most-common words\n",
      "2019-04-23 21:10:32,700 : INFO : downsampling leaves estimated 0 word corpus (7.5% of prior 7)\n",
      "2019-04-23 21:10:32,701 : INFO : estimated required memory for 5 words and 100 dimensions: 6500 bytes\n",
      "2019-04-23 21:10:32,702 : INFO : resetting layer weights\n"
     ]
    }
   ],
   "source": [
    "# 모델 사전 만들기\n",
    "w2v_en.build_vocab(corpus_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-23 21:10:33,685 : INFO : training model with 3 workers on 5 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-04-23 21:10:33,688 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-23 21:10:33,689 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-23 21:10:33,690 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-23 21:10:33,691 : INFO : EPOCH - 1 : training on 7 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2019-04-23 21:10:33,693 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-23 21:10:33,694 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-23 21:10:33,695 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-23 21:10:33,696 : INFO : EPOCH - 2 : training on 7 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2019-04-23 21:10:33,698 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-23 21:10:33,699 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-23 21:10:33,700 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-23 21:10:33,701 : INFO : EPOCH - 3 : training on 7 raw words (1 effective words) took 0.0s, 397 effective words/s\n",
      "2019-04-23 21:10:33,703 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-23 21:10:33,704 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-23 21:10:33,704 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-23 21:10:33,705 : INFO : EPOCH - 4 : training on 7 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2019-04-23 21:10:33,708 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-23 21:10:33,709 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-23 21:10:33,710 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-23 21:10:33,710 : INFO : EPOCH - 5 : training on 7 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2019-04-23 21:10:33,713 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-23 21:10:33,713 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-23 21:10:33,714 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-23 21:10:33,715 : INFO : EPOCH - 6 : training on 7 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2019-04-23 21:10:33,717 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-23 21:10:33,718 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-23 21:10:33,718 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-23 21:10:33,719 : INFO : EPOCH - 7 : training on 7 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2019-04-23 21:10:33,721 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-23 21:10:33,722 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-23 21:10:33,723 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-23 21:10:33,724 : INFO : EPOCH - 8 : training on 7 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2019-04-23 21:10:33,726 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-23 21:10:33,727 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-23 21:10:33,728 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-23 21:10:33,729 : INFO : EPOCH - 9 : training on 7 raw words (1 effective words) took 0.0s, 362 effective words/s\n",
      "2019-04-23 21:10:33,731 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-23 21:10:33,731 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-23 21:10:33,732 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-23 21:10:33,733 : INFO : EPOCH - 10 : training on 7 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2019-04-23 21:10:33,734 : INFO : training on a 70 raw words (2 effective words) took 0.0s, 42 effective words/s\n",
      "2019-04-23 21:10:33,735 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2, 70)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습\n",
    "w2v_en.train(corpus_en, total_examples=len(corpus_en), epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-23 21:10:34,413 : INFO : saving Word2Vec object under model_w2v_en.wv, separately None\n",
      "2019-04-23 21:10:34,414 : INFO : not storing attribute vectors_norm\n",
      "2019-04-23 21:10:34,415 : INFO : not storing attribute cum_table\n",
      "2019-04-23 21:10:34,417 : INFO : saved model_w2v_en.wv\n"
     ]
    }
   ],
   "source": [
    "# 생성된 모델 저장 및 불러오기 - 이것은 나중에 이 모델을 다시 활용하려할 때 써보기. \n",
    "fname = 'model_w2v_en.wv'\n",
    "w2v_en.save(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_w2v_en.wv\r\n"
     ]
    }
   ],
   "source": [
    "%ls model_w2v_en.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-23 21:10:36,085 : INFO : loading Word2Vec object from model_w2v_en.wv\n",
      "2019-04-23 21:10:36,087 : INFO : loading wv recursively from model_w2v_en.wv.wv.* with mmap=None\n",
      "2019-04-23 21:10:36,088 : INFO : setting ignored attribute vectors_norm to None\n",
      "2019-04-23 21:10:36,089 : INFO : loading vocabulary recursively from model_w2v_en.wv.vocabulary.* with mmap=None\n",
      "2019-04-23 21:10:36,090 : INFO : loading trainables recursively from model_w2v_en.wv.trainables.* with mmap=None\n",
      "2019-04-23 21:10:36,091 : INFO : setting ignored attribute cum_table to None\n",
      "2019-04-23 21:10:36,092 : INFO : loaded model_w2v_en.wv\n"
     ]
    }
   ],
   "source": [
    "# 저장된 모델 로드해서 사용하기\n",
    "my_w2v_en = gensim.models.Word2Vec.load(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어의 벡터값 얻기\n",
    "vectors_en = my_w2v_en.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.5927573e-03,  4.4859448e-03, -2.2817224e-03, -3.9224941e-03,\n",
       "        3.6792208e-03, -1.4695083e-03,  1.9569823e-03,  1.1353698e-03,\n",
       "        2.6722791e-04, -2.1971969e-03,  3.7626526e-03,  3.7397007e-03,\n",
       "       -3.0215716e-03,  1.5770639e-03, -4.6257027e-03, -1.6846821e-03,\n",
       "        3.1876427e-03, -1.5796910e-03, -3.0271267e-03,  3.6565969e-03,\n",
       "        4.8914663e-03,  2.0221453e-03,  3.9439914e-03, -3.9060025e-03,\n",
       "       -4.7563524e-03, -4.7203777e-03, -6.6261768e-05, -2.8719442e-04,\n",
       "       -2.1669192e-03,  3.8900014e-03, -1.1963459e-03, -4.8108720e-03,\n",
       "        3.4040292e-03, -3.9582141e-03, -2.3571749e-03,  3.9369878e-03,\n",
       "       -1.6443490e-03,  7.4489706e-04, -1.5425759e-03,  3.1522007e-03,\n",
       "       -4.5602308e-03,  2.5334940e-03, -1.0126929e-03, -2.4590590e-03,\n",
       "        2.9935597e-03,  4.0025646e-03, -1.7761880e-03,  1.7690011e-03,\n",
       "        3.3243871e-04, -8.7835989e-04,  2.8865228e-03, -2.0090507e-03,\n",
       "        1.0992971e-03, -3.8388525e-03,  3.7692061e-03, -4.5965267e-03,\n",
       "        9.3148754e-04,  4.4851084e-03,  2.0924264e-03,  4.8112385e-03,\n",
       "        3.5505071e-03, -4.0699597e-03, -2.5647329e-03, -4.7033061e-03,\n",
       "       -2.7779827e-03, -1.3945694e-03,  4.3403800e-03, -3.6310635e-03,\n",
       "        3.1043962e-03,  1.3989244e-03,  4.9683889e-03,  4.3774457e-03,\n",
       "       -2.7722386e-03,  2.4450759e-03,  1.2397397e-03, -4.9499623e-03,\n",
       "        3.2048186e-03,  3.8908713e-03,  4.3369117e-03,  4.5578618e-04,\n",
       "       -3.7710024e-03, -3.2995662e-03, -3.1788775e-03,  4.4255103e-03,\n",
       "       -3.0947595e-03, -1.1456305e-03, -1.2928650e-03, -2.7942194e-03,\n",
       "        3.2090961e-03, -1.6639917e-03,  1.6776223e-03, -4.1603385e-03,\n",
       "       -1.6987801e-04,  2.6769151e-03, -1.4954369e-03, -1.6889946e-03,\n",
       "       -2.8399292e-03,  1.3256003e-03,  3.0920319e-03,  2.3168453e-03],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors_en['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-23 21:10:39,313 : INFO : starting tSNE dimensionality reduction. This may take some time.\n",
      "2019-04-23 21:10:39,426 : INFO : All done. Plotting.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": false
       },
       "data": [
        {
         "mode": "text",
         "text": [
          "my",
          "name",
          "is",
          "jamie",
          "cute"
         ],
         "type": "scatter",
         "uid": "79e80e76-5a88-4e13-9e99-43818ce8d6f8",
         "x": [
          74.40828704833984,
          37.319427490234375,
          54.9611701965332,
          7.289608955383301,
          -15.492201805114746
         ],
         "y": [
          -116.82817840576172,
          -155.46975708007812,
          -64.77223205566406,
          -89.53218841552734,
          -138.18067932128906
         ]
        }
       ],
       "layout": {}
      },
      "text/html": [
       "<div id=\"c8ab4507-0cb9-4121-b03f-5862692a3e44\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";\n",
       "if (document.getElementById(\"c8ab4507-0cb9-4121-b03f-5862692a3e44\")) {\n",
       "    Plotly.newPlot(\"c8ab4507-0cb9-4121-b03f-5862692a3e44\", [{\"mode\": \"text\", \"text\": [\"my\", \"name\", \"is\", \"jamie\", \"cute\"], \"x\": [74.40828704833984, 37.319427490234375, 54.9611701965332, 7.289608955383301, -15.492201805114746], \"y\": [-116.82817840576172, -155.46975708007812, -64.77223205566406, -89.53218841552734, -138.18067932128906], \"type\": \"scatter\", \"uid\": \"dde100c8-1ed2-4823-a1c4-9968bd148bec\"}], {}, {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"}); \n",
       "}\n",
       "});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){if (document.getElementById(\"c8ab4507-0cb9-4121-b03f-5862692a3e44\")) {window._Plotly.Plots.resize(document.getElementById(\"c8ab4507-0cb9-4121-b03f-5862692a3e44\"));};})</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"c8ab4507-0cb9-4121-b03f-5862692a3e44\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";\n",
       "if (document.getElementById(\"c8ab4507-0cb9-4121-b03f-5862692a3e44\")) {\n",
       "    Plotly.newPlot(\"c8ab4507-0cb9-4121-b03f-5862692a3e44\", [{\"mode\": \"text\", \"text\": [\"my\", \"name\", \"is\", \"jamie\", \"cute\"], \"x\": [74.40828704833984, 37.319427490234375, 54.9611701965332, 7.289608955383301, -15.492201805114746], \"y\": [-116.82817840576172, -155.46975708007812, -64.77223205566406, -89.53218841552734, -138.18067932128906], \"type\": \"scatter\", \"uid\": \"dde100c8-1ed2-4823-a1c4-9968bd148bec\"}], {}, {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"}); \n",
       "}\n",
       "});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){if (document.getElementById(\"c8ab4507-0cb9-4121-b03f-5862692a3e44\")) {window._Plotly.Plots.resize(document.getElementById(\"c8ab4507-0cb9-4121-b03f-5862692a3e44\"));};})</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 시각화\n",
    "labels = [word for word in vectors_en.vocab]\n",
    "vectors = [vectors_en[word] for word in labels]\n",
    "\n",
    "visualize_embeddings(vectors, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 한국어 예시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 단위로 입력 코퍼스를 바꿔야 한다.\n",
    "\n",
    "def tokenize(x, tokenizer) :\n",
    "    for token in tokenizer.tokenize(x) :\n",
    "        if nouns.get(token) :\n",
    "            yield token\n",
    "            \n",
    "sentences_ko = []\n",
    "for doc in documents_ko :\n",
    "    sentences = [sent for sent in doc.split('\\n') if len(sent)]\n",
    "    tokens = [list(tokenize(sent, tokenizer)) for sent in sentence]\n",
    "    sentences_ko.extend([token for token in tokens if len(token)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['네'], ['네']]\n"
     ]
    }
   ],
   "source": [
    "print(sentences_ko[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-24 03:04:01,909 : INFO : collecting all words and their counts\n",
      "2019-04-24 03:04:01,910 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-04-24 03:04:01,911 : INFO : collected 18 word types from a corpus of 441 raw words and 378 sentences\n",
      "2019-04-24 03:04:01,911 : INFO : Loading a fresh vocabulary\n",
      "2019-04-24 03:04:01,913 : INFO : min_count=0 retains 18 unique words (100% of original 18, drops 0)\n",
      "2019-04-24 03:04:01,914 : INFO : min_count=0 leaves 441 word corpus (100% of original 441, drops 0)\n",
      "2019-04-24 03:04:01,914 : INFO : deleting the raw counts dictionary of 18 items\n",
      "2019-04-24 03:04:01,915 : INFO : sample=0.001 downsamples 18 most-common words\n",
      "2019-04-24 03:04:01,916 : INFO : downsampling leaves estimated 57 word corpus (13.1% of prior 441)\n",
      "2019-04-24 03:04:01,917 : INFO : estimated required memory for 18 words and 100 dimensions: 23400 bytes\n",
      "2019-04-24 03:04:01,918 : INFO : resetting layer weights\n",
      "2019-04-24 03:04:01,920 : INFO : training model with 3 workers on 18 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-04-24 03:04:01,922 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-24 03:04:01,923 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-24 03:04:01,924 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-24 03:04:01,925 : INFO : EPOCH - 1 : training on 290 raw words (17 effective words) took 0.0s, 7548 effective words/s\n",
      "2019-04-24 03:04:01,925 : WARNING : EPOCH - 1 : supplied example count (9) did not equal expected count (378)\n",
      "2019-04-24 03:04:01,928 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-24 03:04:01,929 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-24 03:04:01,930 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-24 03:04:01,931 : INFO : EPOCH - 2 : training on 290 raw words (20 effective words) took 0.0s, 7697 effective words/s\n",
      "2019-04-24 03:04:01,931 : WARNING : EPOCH - 2 : supplied example count (9) did not equal expected count (378)\n",
      "2019-04-24 03:04:01,934 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-24 03:04:01,935 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-24 03:04:01,936 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-24 03:04:01,937 : INFO : EPOCH - 3 : training on 290 raw words (14 effective words) took 0.0s, 4584 effective words/s\n",
      "2019-04-24 03:04:01,937 : WARNING : EPOCH - 3 : supplied example count (9) did not equal expected count (378)\n",
      "2019-04-24 03:04:01,940 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-24 03:04:01,940 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-24 03:04:01,941 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-24 03:04:01,942 : INFO : EPOCH - 4 : training on 290 raw words (12 effective words) took 0.0s, 4555 effective words/s\n",
      "2019-04-24 03:04:01,943 : WARNING : EPOCH - 4 : supplied example count (9) did not equal expected count (378)\n",
      "2019-04-24 03:04:01,945 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-24 03:04:01,946 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-24 03:04:01,947 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-24 03:04:01,948 : INFO : EPOCH - 5 : training on 290 raw words (20 effective words) took 0.0s, 7421 effective words/s\n",
      "2019-04-24 03:04:01,948 : WARNING : EPOCH - 5 : supplied example count (9) did not equal expected count (378)\n",
      "2019-04-24 03:04:01,951 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-24 03:04:01,952 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-24 03:04:01,953 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-24 03:04:01,954 : INFO : EPOCH - 6 : training on 290 raw words (16 effective words) took 0.0s, 5151 effective words/s\n",
      "2019-04-24 03:04:01,954 : WARNING : EPOCH - 6 : supplied example count (9) did not equal expected count (378)\n",
      "2019-04-24 03:04:01,957 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-24 03:04:01,958 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-24 03:04:01,959 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-24 03:04:01,959 : INFO : EPOCH - 7 : training on 290 raw words (12 effective words) took 0.0s, 4060 effective words/s\n",
      "2019-04-24 03:04:01,960 : WARNING : EPOCH - 7 : supplied example count (9) did not equal expected count (378)\n",
      "2019-04-24 03:04:01,963 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-24 03:04:01,963 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-24 03:04:01,964 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-24 03:04:01,965 : INFO : EPOCH - 8 : training on 290 raw words (20 effective words) took 0.0s, 8239 effective words/s\n",
      "2019-04-24 03:04:01,966 : WARNING : EPOCH - 8 : supplied example count (9) did not equal expected count (378)\n",
      "2019-04-24 03:04:01,968 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-24 03:04:01,969 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-24 03:04:01,969 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-24 03:04:01,970 : INFO : EPOCH - 9 : training on 290 raw words (14 effective words) took 0.0s, 6192 effective words/s\n",
      "2019-04-24 03:04:01,971 : WARNING : EPOCH - 9 : supplied example count (9) did not equal expected count (378)\n",
      "2019-04-24 03:04:01,974 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-24 03:04:01,974 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-24 03:04:01,975 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-24 03:04:01,976 : INFO : EPOCH - 10 : training on 290 raw words (14 effective words) took 0.0s, 4383 effective words/s\n",
      "2019-04-24 03:04:01,977 : WARNING : EPOCH - 10 : supplied example count (9) did not equal expected count (378)\n",
      "2019-04-24 03:04:01,978 : INFO : training on a 2900 raw words (159 effective words) took 0.1s, 2781 effective words/s\n",
      "2019-04-24 03:04:01,979 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-04-24 03:04:01,980 : INFO : saving Word2Vec object under model_w2v_ko.wv, separately None\n",
      "2019-04-24 03:04:01,981 : INFO : not storing attribute vectors_norm\n",
      "2019-04-24 03:04:01,981 : INFO : not storing attribute cum_table\n",
      "2019-04-24 03:04:01,983 : INFO : saved model_w2v_ko.wv\n"
     ]
    }
   ],
   "source": [
    "# 모델 초기화\n",
    "w2v_ko = gensim.models.Word2Vec(min_count=0) # 예) Word2Vec(sentences, size=100, window=5, min_count=5, workers=4)\n",
    "# 모델 사전 만들기\n",
    "w2v_ko.build_vocab(sentences_ko)\n",
    "# 학습\n",
    "w2v_ko.train(corpus_ko, total_examples=len(sentences_ko), epochs=10)\n",
    "# 모델 저장\n",
    "fname = 'model_w2v_ko.wv'\n",
    "w2v_ko.save(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어의 벡터값 얻기\n",
    "vectors_ko = w2v_ko.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.3770596e-03, -2.3819695e-03, -4.7503787e-04, -1.4193344e-03,\n",
       "        4.7977436e-03, -9.8665361e-04, -1.8427230e-03,  7.7493320e-04,\n",
       "       -1.3317795e-03,  1.6155151e-03,  2.3584550e-03,  6.5936468e-04,\n",
       "        2.6537247e-03, -5.7585782e-04, -1.9154645e-03,  3.3450872e-03,\n",
       "        1.3377591e-03, -2.3150735e-04,  3.3459403e-03, -3.8644946e-03,\n",
       "       -8.4396110e-05, -3.2811644e-03, -4.4795186e-03, -4.7085499e-03,\n",
       "        3.6030859e-05,  2.2660999e-03, -3.5197530e-03,  5.9939484e-04,\n",
       "       -1.1683941e-03, -2.3626836e-03,  3.0485927e-03,  3.2824073e-03,\n",
       "        9.8227989e-05,  3.8114837e-03, -2.5666174e-03, -5.2756368e-04,\n",
       "       -3.2863314e-03,  1.2689834e-03,  2.3157473e-03, -2.1149539e-03,\n",
       "        1.1043837e-03, -4.0839459e-03,  4.4506108e-03, -1.4522129e-03,\n",
       "        3.2256318e-03,  3.8084595e-03,  1.5904799e-03,  3.8842952e-03,\n",
       "        3.2019124e-03, -2.1111062e-03,  2.9661607e-03, -2.3745240e-03,\n",
       "        4.9216649e-03, -1.3289509e-03,  3.7913534e-03, -4.6393052e-03,\n",
       "        3.9333692e-03, -4.7752270e-03, -1.3708761e-04, -8.5129723e-04,\n",
       "       -4.2813169e-03, -3.1282129e-03, -1.7397169e-03, -3.3411044e-03,\n",
       "        3.9851768e-03,  3.1856010e-03,  4.5403419e-03, -4.2197136e-03,\n",
       "        3.1765727e-03,  2.4257290e-04,  3.1496293e-03, -1.9681833e-03,\n",
       "        3.2317908e-03, -3.3230998e-03,  3.5737229e-03, -1.5201620e-03,\n",
       "        1.6226908e-03,  9.3156402e-04, -1.2948391e-03, -2.7512235e-03,\n",
       "       -1.0459975e-04, -3.9912937e-03,  1.2264604e-03,  4.5612571e-03,\n",
       "       -9.8257535e-04, -8.5490046e-04, -2.7214666e-03, -1.4735524e-03,\n",
       "        4.8117274e-03,  1.9578151e-03, -1.0162051e-03, -3.1342716e-03,\n",
       "        3.7929080e-03,  4.5789881e-03, -1.8594638e-03, -2.6530623e-03,\n",
       "       -4.7675031e-03, -3.2223391e-03, -2.6416089e-03, -4.2163404e-03],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 472,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors_ko['꿈']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-24 03:04:06,038 : INFO : starting tSNE dimensionality reduction. This may take some time.\n",
      "2019-04-24 03:04:06,159 : INFO : All done. Plotting.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": false
       },
       "data": [
        {
         "mode": "text",
         "text": [
          "네",
          "꿈",
          "아무",
          "전부",
          "있어",
          "멀리",
          "하루",
          "말",
          "준비",
          "벌써",
          "너",
          "거짓말",
          "자꾸",
          "매일",
          "어른들",
          "시간",
          "특별",
          "꿔"
         ],
         "type": "scatter",
         "uid": "0ff56cba-6769-443b-9adb-b8f0364d353a",
         "x": [
          -139.33201599121094,
          -8.303340911865234,
          -100.01652526855469,
          94.89714813232422,
          229.22694396972656,
          187.9876708984375,
          -47.16599655151367,
          69.00191497802734,
          -11.249449729919434,
          -10.736228942871094,
          184.876708984375,
          143.365966796875,
          74.22679138183594,
          93.60678100585938,
          133.5697784423828,
          -12.989117622375488,
          -100.54280853271484,
          50.03727340698242
         ],
         "y": [
          33.07489776611328,
          -41.02456283569336,
          -67.20237731933594,
          214.87632751464844,
          38.90904235839844,
          -85.6405029296875,
          31.656824111938477,
          127.9787826538086,
          -134.89939880371094,
          105.32756805419922,
          152.0074920654297,
          -6.147220611572266,
          -52.31425857543945,
          -141.88914489746094,
          75.51778411865234,
          200.66246032714844,
          132.1851348876953,
          34.481380462646484
         ]
        }
       ],
       "layout": {}
      },
      "text/html": [
       "<div id=\"7a0cc5a0-d27b-4968-b15f-94d61325980e\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";\n",
       "if (document.getElementById(\"7a0cc5a0-d27b-4968-b15f-94d61325980e\")) {\n",
       "    Plotly.newPlot(\"7a0cc5a0-d27b-4968-b15f-94d61325980e\", [{\"mode\": \"text\", \"text\": [\"\\ub124\", \"\\uafc8\", \"\\uc544\\ubb34\", \"\\uc804\\ubd80\", \"\\uc788\\uc5b4\", \"\\uba40\\ub9ac\", \"\\ud558\\ub8e8\", \"\\ub9d0\", \"\\uc900\\ube44\", \"\\ubc8c\\uc368\", \"\\ub108\", \"\\uac70\\uc9d3\\ub9d0\", \"\\uc790\\uafb8\", \"\\ub9e4\\uc77c\", \"\\uc5b4\\ub978\\ub4e4\", \"\\uc2dc\\uac04\", \"\\ud2b9\\ubcc4\", \"\\uafd4\"], \"x\": [-139.33201599121094, -8.303340911865234, -100.01652526855469, 94.89714813232422, 229.22694396972656, 187.9876708984375, -47.16599655151367, 69.00191497802734, -11.249449729919434, -10.736228942871094, 184.876708984375, 143.365966796875, 74.22679138183594, 93.60678100585938, 133.5697784423828, -12.989117622375488, -100.54280853271484, 50.03727340698242], \"y\": [33.07489776611328, -41.02456283569336, -67.20237731933594, 214.87632751464844, 38.90904235839844, -85.6405029296875, 31.656824111938477, 127.9787826538086, -134.89939880371094, 105.32756805419922, 152.0074920654297, -6.147220611572266, -52.31425857543945, -141.88914489746094, 75.51778411865234, 200.66246032714844, 132.1851348876953, 34.481380462646484], \"type\": \"scatter\", \"uid\": \"51190ab8-a3ce-495f-aef5-f3425b09a854\"}], {}, {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"}); \n",
       "}\n",
       "});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){if (document.getElementById(\"7a0cc5a0-d27b-4968-b15f-94d61325980e\")) {window._Plotly.Plots.resize(document.getElementById(\"7a0cc5a0-d27b-4968-b15f-94d61325980e\"));};})</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"7a0cc5a0-d27b-4968-b15f-94d61325980e\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";\n",
       "if (document.getElementById(\"7a0cc5a0-d27b-4968-b15f-94d61325980e\")) {\n",
       "    Plotly.newPlot(\"7a0cc5a0-d27b-4968-b15f-94d61325980e\", [{\"mode\": \"text\", \"text\": [\"\\ub124\", \"\\uafc8\", \"\\uc544\\ubb34\", \"\\uc804\\ubd80\", \"\\uc788\\uc5b4\", \"\\uba40\\ub9ac\", \"\\ud558\\ub8e8\", \"\\ub9d0\", \"\\uc900\\ube44\", \"\\ubc8c\\uc368\", \"\\ub108\", \"\\uac70\\uc9d3\\ub9d0\", \"\\uc790\\uafb8\", \"\\ub9e4\\uc77c\", \"\\uc5b4\\ub978\\ub4e4\", \"\\uc2dc\\uac04\", \"\\ud2b9\\ubcc4\", \"\\uafd4\"], \"x\": [-139.33201599121094, -8.303340911865234, -100.01652526855469, 94.89714813232422, 229.22694396972656, 187.9876708984375, -47.16599655151367, 69.00191497802734, -11.249449729919434, -10.736228942871094, 184.876708984375, 143.365966796875, 74.22679138183594, 93.60678100585938, 133.5697784423828, -12.989117622375488, -100.54280853271484, 50.03727340698242], \"y\": [33.07489776611328, -41.02456283569336, -67.20237731933594, 214.87632751464844, 38.90904235839844, -85.6405029296875, 31.656824111938477, 127.9787826538086, -134.89939880371094, 105.32756805419922, 152.0074920654297, -6.147220611572266, -52.31425857543945, -141.88914489746094, 75.51778411865234, 200.66246032714844, 132.1851348876953, 34.481380462646484], \"type\": \"scatter\", \"uid\": \"51190ab8-a3ce-495f-aef5-f3425b09a854\"}], {}, {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"}); \n",
       "}\n",
       "});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){if (document.getElementById(\"7a0cc5a0-d27b-4968-b15f-94d61325980e\")) {window._Plotly.Plots.resize(document.getElementById(\"7a0cc5a0-d27b-4968-b15f-94d61325980e\"));};})</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 시각화\n",
    "labels = [word for word in vectors_ko.vocab]\n",
    "vectors = [vectors_ko[word] for word in labels]\n",
    "\n",
    "visualize_embeddings(vectors, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['네', '꿈', '아무', '전부', '있어', '멀리', '하루', '말', '준비', '벌써', '너', '거짓말', '자꾸', '매일', '어른들', '시간', '특별', '꿔'])"
      ]
     },
     "execution_count": 474,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_ko.wv.vocab.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-24 03:04:14,044 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('자꾸', 0.16839897632598877),\n",
       " ('벌써', 0.13236922025680542),\n",
       " ('멀리', 0.06959383189678192),\n",
       " ('아무', 0.05615692958235741),\n",
       " ('하루', 0.05258084461092949),\n",
       " ('말', 0.01975037157535553),\n",
       " ('꿔', -0.01000899076461792),\n",
       " ('네', -0.018199510872364044),\n",
       " ('거짓말', -0.025152131915092468),\n",
       " ('시간', -0.04419991001486778)]"
      ]
     },
     "execution_count": 475,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_ko.wv.most_similar(['꿈'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('있어', 0.21480423212051392),\n",
       " ('전부', 0.10614712536334991),\n",
       " ('꿔', 0.10569868236780167),\n",
       " ('꿈', 0.05258084461092949),\n",
       " ('어른들', 0.02084885537624359),\n",
       " ('자꾸', 0.01874619349837303),\n",
       " ('특별', 0.00026061758399009705),\n",
       " ('벌써', -0.0004631727933883667),\n",
       " ('준비', -0.01703988015651703),\n",
       " ('매일', -0.02649909257888794)]"
      ]
     },
     "execution_count": 476,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_ko.wv.most_similar(['하루'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('매일', 0.18942180275917053),\n",
       " ('네', 0.17278073728084564),\n",
       " ('특별', 0.11666610091924667),\n",
       " ('멀리', 0.04391506314277649),\n",
       " ('하루', 0.020848851650953293),\n",
       " ('너', -0.011678421869874),\n",
       " ('준비', -0.01792408525943756),\n",
       " ('시간', -0.036207232624292374),\n",
       " ('벌써', -0.04509039223194122),\n",
       " ('자꾸', -0.04902574419975281)]"
      ]
     },
     "execution_count": 477,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_ko.wv.most_similar(['어른들'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FastText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 영어 예시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_en = [[\"my\", \"name\", \"is\", \"jamie\"], [\"jamie\", \"is\", \"cute\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/moodern/anaconda3/envs/deepnlp/lib/python3.7/site-packages/ipykernel_launcher.py:2: DeprecationWarning:\n",
      "\n",
      "Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"word 'jamia' not in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-479-12139b88e337>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 워드투벡은 학습시 없었던 단어에 대해서는 계산해주지 못한다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mw2v_en\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"jamia\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/deepnlp/lib/python3.7/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36mnew_func1\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1396\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1397\u001b[0m                 )\n\u001b[0;32m-> 1398\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1400\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mnew_func1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deepnlp/lib/python3.7/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    694\u001b[0m         \u001b[0mRefer\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdocumentation\u001b[0m \u001b[0;32mfor\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeyedvectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWordEmbeddingsKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m         \"\"\"\n\u001b[0;32m--> 696\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestrict_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    697\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mdeprecated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Method will be removed in 4.0.0, use self.wv.wmdistance() instead\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deepnlp/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    363\u001b[0m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 365\u001b[0;31m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    366\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m                     \u001b[0mall_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deepnlp/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[0;34m(self, word, use_norm)\u001b[0m\n\u001b[1;32m    272\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"word 'jamia' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "# 워드투벡은 학습시 없었던 단어에 대해서는 계산해주지 못한다.\n",
    "w2v_en.most_similar(\"jamia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_en = documents_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['my', 'name', 'is', 'jamie'], ['jamie', 'is', 'cute']]\n"
     ]
    }
   ],
   "source": [
    "pprint(sentences_en[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "\n",
    "ft_en = FastText(size=4, window=3, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-24 03:04:28,023 : INFO : collecting all words and their counts\n",
      "2019-04-24 03:04:28,024 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-04-24 03:04:28,024 : INFO : collected 5 word types from a corpus of 7 raw words and 2 sentences\n",
      "2019-04-24 03:04:28,025 : INFO : Loading a fresh vocabulary\n",
      "2019-04-24 03:04:28,027 : INFO : min_count=1 retains 5 unique words (100% of original 5, drops 0)\n",
      "2019-04-24 03:04:28,027 : INFO : min_count=1 leaves 7 word corpus (100% of original 7, drops 0)\n",
      "2019-04-24 03:04:28,028 : INFO : deleting the raw counts dictionary of 5 items\n",
      "2019-04-24 03:04:28,029 : INFO : sample=0.001 downsamples 5 most-common words\n",
      "2019-04-24 03:04:28,030 : INFO : downsampling leaves estimated 0 word corpus (7.5% of prior 7)\n",
      "2019-04-24 03:04:28,032 : INFO : estimated required memory for 5 words, 40 buckets and 4 dimensions: 3860 bytes\n",
      "2019-04-24 03:04:28,033 : INFO : resetting layer weights\n",
      "2019-04-24 03:04:28,050 : INFO : Total number of ngrams is 40\n"
     ]
    }
   ],
   "source": [
    "ft_en.build_vocab(sentences_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-24 03:04:28,897 : INFO : training model with 3 workers on 5 vocabulary and 4 features, using sg=0 hs=0 sample=0.001 negative=5 window=3\n",
      "2019-04-24 03:04:28,904 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-24 03:04:28,905 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-24 03:04:28,906 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-24 03:04:28,906 : INFO : EPOCH - 1 : training on 7 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2019-04-24 03:04:28,909 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-24 03:04:28,910 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-24 03:04:28,911 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-24 03:04:28,912 : INFO : EPOCH - 2 : training on 7 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2019-04-24 03:04:28,914 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-24 03:04:28,915 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-24 03:04:28,916 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-24 03:04:28,917 : INFO : EPOCH - 3 : training on 7 raw words (1 effective words) took 0.0s, 356 effective words/s\n",
      "2019-04-24 03:04:28,920 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-24 03:04:28,920 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-24 03:04:28,921 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-24 03:04:28,922 : INFO : EPOCH - 4 : training on 7 raw words (1 effective words) took 0.0s, 452 effective words/s\n",
      "2019-04-24 03:04:28,924 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-24 03:04:28,925 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-24 03:04:28,926 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-24 03:04:28,927 : INFO : EPOCH - 5 : training on 7 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2019-04-24 03:04:28,929 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-24 03:04:28,930 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-24 03:04:28,930 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-24 03:04:28,931 : INFO : EPOCH - 6 : training on 7 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2019-04-24 03:04:28,934 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-24 03:04:28,934 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-24 03:04:28,935 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-24 03:04:28,936 : INFO : EPOCH - 7 : training on 7 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2019-04-24 03:04:28,938 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-24 03:04:28,939 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-24 03:04:28,940 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-24 03:04:28,941 : INFO : EPOCH - 8 : training on 7 raw words (1 effective words) took 0.0s, 317 effective words/s\n",
      "2019-04-24 03:04:28,943 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-24 03:04:28,944 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-24 03:04:28,945 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-24 03:04:28,946 : INFO : EPOCH - 9 : training on 7 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2019-04-24 03:04:28,948 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-24 03:04:28,949 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-24 03:04:28,950 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-24 03:04:28,951 : INFO : EPOCH - 10 : training on 7 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2019-04-24 03:04:28,951 : INFO : training on a 70 raw words (3 effective words) took 0.1s, 56 effective words/s\n",
      "2019-04-24 03:04:28,952 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    }
   ],
   "source": [
    "ft_en.train(sentences=sentences_en, total_examples=len(sentences_en), epochs=10)  # train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-24 03:04:29,758 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-04-24 03:04:29,759 : INFO : precomputing L2-norms of ngram weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('my', 0.8944276571273804),\n",
       " ('jamie', 0.7871583104133606),\n",
       " ('name', 0.3816155791282654),\n",
       " ('is', 0.3451642394065857),\n",
       " ('cute', 0.30520403385162354)]"
      ]
     },
     "execution_count": 485,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# FastText는 학습시 없었던 단어에 대해서도 계산해준다.\n",
    "ft_en.wv.most_similar(\"jamia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-24 03:04:30,631 : INFO : starting tSNE dimensionality reduction. This may take some time.\n",
      "2019-04-24 03:04:30,755 : INFO : All done. Plotting.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": false
       },
       "data": [
        {
         "mode": "text",
         "text": [
          "my",
          "name",
          "is",
          "jamie",
          "cute"
         ],
         "type": "scatter",
         "uid": "99397e6a-b841-4491-82fd-f68c44c1b8af",
         "x": [
          58.887428283691406,
          50.34069061279297,
          129.97242736816406,
          125.4763412475586,
          29.455793380737305
         ],
         "y": [
          -138.66778564453125,
          9.897323608398438,
          -99.27764129638672,
          -21.076669692993164,
          -65.83253479003906
         ]
        }
       ],
       "layout": {}
      },
      "text/html": [
       "<div id=\"35a56352-3db4-490a-ab31-7155fab7fba0\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";\n",
       "if (document.getElementById(\"35a56352-3db4-490a-ab31-7155fab7fba0\")) {\n",
       "    Plotly.newPlot(\"35a56352-3db4-490a-ab31-7155fab7fba0\", [{\"mode\": \"text\", \"text\": [\"my\", \"name\", \"is\", \"jamie\", \"cute\"], \"x\": [58.887428283691406, 50.34069061279297, 129.97242736816406, 125.4763412475586, 29.455793380737305], \"y\": [-138.66778564453125, 9.897323608398438, -99.27764129638672, -21.076669692993164, -65.83253479003906], \"type\": \"scatter\", \"uid\": \"a176d7c2-8410-4046-826c-336a19ff55df\"}], {}, {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"}); \n",
       "}\n",
       "});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){if (document.getElementById(\"35a56352-3db4-490a-ab31-7155fab7fba0\")) {window._Plotly.Plots.resize(document.getElementById(\"35a56352-3db4-490a-ab31-7155fab7fba0\"));};})</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"35a56352-3db4-490a-ab31-7155fab7fba0\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";\n",
       "if (document.getElementById(\"35a56352-3db4-490a-ab31-7155fab7fba0\")) {\n",
       "    Plotly.newPlot(\"35a56352-3db4-490a-ab31-7155fab7fba0\", [{\"mode\": \"text\", \"text\": [\"my\", \"name\", \"is\", \"jamie\", \"cute\"], \"x\": [58.887428283691406, 50.34069061279297, 129.97242736816406, 125.4763412475586, 29.455793380737305], \"y\": [-138.66778564453125, 9.897323608398438, -99.27764129638672, -21.076669692993164, -65.83253479003906], \"type\": \"scatter\", \"uid\": \"a176d7c2-8410-4046-826c-336a19ff55df\"}], {}, {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"}); \n",
       "}\n",
       "});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){if (document.getElementById(\"35a56352-3db4-490a-ab31-7155fab7fba0\")) {window._Plotly.Plots.resize(document.getElementById(\"35a56352-3db4-490a-ab31-7155fab7fba0\"));};})</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels = [word for word in ft_en.wv.vocab]\n",
    "vectors = [ft_en.wv[word] for word in labels]\n",
    "\n",
    "visualize_embeddings(vectors, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 한국어 예시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'네': <gensim.models.keyedvectors.Vocab object at 0x1a1d4fe2b0>, '꿈': <gensim.models.keyedvectors.Vocab object at 0x1a1d4fe1d0>, '아무': <gensim.models.keyedvectors.Vocab object at 0x1a21d6bac8>, '전부': <gensim.models.keyedvectors.Vocab object at 0x1a21d6b9b0>, '있어': <gensim.models.keyedvectors.Vocab object at 0x1a21d57198>, '멀리': <gensim.models.keyedvectors.Vocab object at 0x1a21d57390>, '하루': <gensim.models.keyedvectors.Vocab object at 0x1a21d57358>, '말': <gensim.models.keyedvectors.Vocab object at 0x1a21d573c8>, '준비': <gensim.models.keyedvectors.Vocab object at 0x1a21d572e8>, '벌써': <gensim.models.keyedvectors.Vocab object at 0x1a21d57278>, '너': <gensim.models.keyedvectors.Vocab object at 0x1a21d57320>, '거짓말': <gensim.models.keyedvectors.Vocab object at 0x1a21d57400>, '자꾸': <gensim.models.keyedvectors.Vocab object at 0x1a21d57438>, '매일': <gensim.models.keyedvectors.Vocab object at 0x1a21d57470>, '어른들': <gensim.models.keyedvectors.Vocab object at 0x1a21d574a8>, '시간': <gensim.models.keyedvectors.Vocab object at 0x1a21d574e0>, '특별': <gensim.models.keyedvectors.Vocab object at 0x1a21d57518>, '꿔': <gensim.models.keyedvectors.Vocab object at 0x1a21d57550>}\n"
     ]
    }
   ],
   "source": [
    "print(w2v_ko.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_ko.wv.vocab.get(\"특별한\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/moodern/anaconda3/envs/deepnlp/lib/python3.7/site-packages/ipykernel_launcher.py:2: DeprecationWarning:\n",
      "\n",
      "Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"word '특별한' not in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-490-40ef64e84b67>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 워드투벡은 학습시 없었던 단어에 대해서는 계산해주지 못한다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mw2v_ko\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"특별한\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/deepnlp/lib/python3.7/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36mnew_func1\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1396\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1397\u001b[0m                 )\n\u001b[0;32m-> 1398\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1400\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mnew_func1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deepnlp/lib/python3.7/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    694\u001b[0m         \u001b[0mRefer\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdocumentation\u001b[0m \u001b[0;32mfor\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeyedvectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWordEmbeddingsKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m         \"\"\"\n\u001b[0;32m--> 696\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestrict_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    697\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mdeprecated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Method will be removed in 4.0.0, use self.wv.wmdistance() instead\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deepnlp/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    363\u001b[0m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 365\u001b[0;31m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    366\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m                     \u001b[0mall_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deepnlp/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[0;34m(self, word, use_norm)\u001b[0m\n\u001b[1;32m    272\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"word '특별한' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "# 워드투벡은 학습시 없었던 단어에 대해서는 계산해주지 못한다.\n",
    "w2v_ko.most_similar(\"특별한\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 단위로 입력 코퍼스를 바꿔야 한다.\n",
    "\n",
    "def tokenize(x, tokenizer) :\n",
    "    for token in tokenizer.tokenize(x) :\n",
    "        if nouns.get(token) :\n",
    "            yield token\n",
    "            \n",
    "sentences_ko = []\n",
    "for doc in documents_ko :\n",
    "    sentences = [sent for sent in doc.split('\\n') if len(sent)]\n",
    "    tokens = [list(tokenize(sent, tokenizer)) for sent in sentence]\n",
    "    sentences_ko.extend([token for token in tokens if len(token)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['네'], ['네']]\n"
     ]
    }
   ],
   "source": [
    "print(sentences_ko[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-24 03:25:42,599 : INFO : collecting all words and their counts\n",
      "2019-04-24 03:25:42,601 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-04-24 03:25:42,602 : INFO : collected 18 word types from a corpus of 441 raw words and 378 sentences\n",
      "2019-04-24 03:25:42,602 : INFO : Loading a fresh vocabulary\n",
      "2019-04-24 03:25:42,603 : INFO : min_count=1 retains 18 unique words (100% of original 18, drops 0)\n",
      "2019-04-24 03:25:42,604 : INFO : min_count=1 leaves 441 word corpus (100% of original 441, drops 0)\n",
      "2019-04-24 03:25:42,604 : INFO : deleting the raw counts dictionary of 18 items\n",
      "2019-04-24 03:25:42,605 : INFO : sample=0.001 downsamples 18 most-common words\n",
      "2019-04-24 03:25:42,606 : INFO : downsampling leaves estimated 57 word corpus (13.1% of prior 441)\n",
      "2019-04-24 03:25:42,608 : INFO : estimated required memory for 18 words, 50 buckets and 4 dimensions: 11640 bytes\n",
      "2019-04-24 03:25:42,609 : INFO : resetting layer weights\n",
      "2019-04-24 03:25:42,620 : INFO : Total number of ngrams is 50\n",
      "2019-04-24 03:25:42,627 : INFO : training model with 3 workers on 18 vocabulary and 4 features, using sg=0 hs=0 sample=0.001 negative=5 window=3\n",
      "2019-04-24 03:25:42,631 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-24 03:25:42,631 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-24 03:25:42,633 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-24 03:25:42,633 : INFO : EPOCH - 1 : training on 441 raw words (53 effective words) took 0.0s, 15211 effective words/s\n",
      "2019-04-24 03:25:42,636 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-24 03:25:42,637 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-24 03:25:42,638 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-24 03:25:42,639 : INFO : EPOCH - 2 : training on 441 raw words (50 effective words) took 0.0s, 15553 effective words/s\n",
      "2019-04-24 03:25:42,642 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-24 03:25:42,643 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-24 03:25:42,644 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-24 03:25:42,644 : INFO : EPOCH - 3 : training on 441 raw words (51 effective words) took 0.0s, 18185 effective words/s\n",
      "2019-04-24 03:25:42,648 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-24 03:25:42,648 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-24 03:25:42,649 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-24 03:25:42,650 : INFO : EPOCH - 4 : training on 441 raw words (56 effective words) took 0.0s, 18343 effective words/s\n",
      "2019-04-24 03:25:42,653 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-24 03:25:42,654 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-24 03:25:42,655 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-24 03:25:42,656 : INFO : EPOCH - 5 : training on 441 raw words (64 effective words) took 0.0s, 22846 effective words/s\n",
      "2019-04-24 03:25:42,659 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-24 03:25:42,660 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-24 03:25:42,661 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-24 03:25:42,661 : INFO : EPOCH - 6 : training on 441 raw words (62 effective words) took 0.0s, 19826 effective words/s\n",
      "2019-04-24 03:25:42,664 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-24 03:25:42,665 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-24 03:25:42,666 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-24 03:25:42,666 : INFO : EPOCH - 7 : training on 441 raw words (50 effective words) took 0.0s, 17143 effective words/s\n",
      "2019-04-24 03:25:42,669 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-24 03:25:42,670 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-24 03:25:42,671 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-24 03:25:42,672 : INFO : EPOCH - 8 : training on 441 raw words (44 effective words) took 0.0s, 13184 effective words/s\n",
      "2019-04-24 03:25:42,675 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-24 03:25:42,676 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-24 03:25:42,677 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-24 03:25:42,677 : INFO : EPOCH - 9 : training on 441 raw words (50 effective words) took 0.0s, 19334 effective words/s\n",
      "2019-04-24 03:25:42,680 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-24 03:25:42,681 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-24 03:25:42,682 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-24 03:25:42,683 : INFO : EPOCH - 10 : training on 441 raw words (65 effective words) took 0.0s, 21288 effective words/s\n",
      "2019-04-24 03:25:42,684 : INFO : training on a 4410 raw words (545 effective words) took 0.1s, 9806 effective words/s\n",
      "2019-04-24 03:25:42,684 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import FastText\n",
    "\n",
    "ft_ko = FastText(size=4, window=3, min_count=1)\n",
    "ft_ko.build_vocab(sentences_ko)\n",
    "ft_ko.train(sentences=sentences_ko, total_examples=len(sentences_ko), epochs=10)  # train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-24 03:25:43,572 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-04-24 03:25:43,573 : INFO : precomputing L2-norms of ngram weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('있어', 0.9348163604736328),\n",
       " ('멀리', 0.9237874746322632),\n",
       " ('너', 0.9161602854728699),\n",
       " ('시간', 0.8657609224319458),\n",
       " ('아무', 0.848251461982727),\n",
       " ('꿈', 0.8391091227531433),\n",
       " ('말', 0.6695265769958496),\n",
       " ('어른들', 0.5211405754089355),\n",
       " ('거짓말', 0.4977935552597046),\n",
       " ('네', 0.40327689051628113)]"
      ]
     },
     "execution_count": 549,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# FastText는 학습시 없었던 단어에 대해서도 계산해준다.\n",
    "ft_ko.wv.most_similar(\"특별한\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'거짓말': <gensim.models.keyedvectors.Vocab object at 0x1a2208be10>,\n",
      " '꿈': <gensim.models.keyedvectors.Vocab object at 0x1a25a6aeb8>,\n",
      " '꿔': <gensim.models.keyedvectors.Vocab object at 0x1a2208be80>,\n",
      " '너': <gensim.models.keyedvectors.Vocab object at 0x1a2208bac8>,\n",
      " '네': <gensim.models.keyedvectors.Vocab object at 0x1a25786518>,\n",
      " '말': <gensim.models.keyedvectors.Vocab object at 0x1a2208bef0>,\n",
      " '매일': <gensim.models.keyedvectors.Vocab object at 0x1a2208bb38>,\n",
      " '멀리': <gensim.models.keyedvectors.Vocab object at 0x1a25a6ab70>,\n",
      " '벌써': <gensim.models.keyedvectors.Vocab object at 0x1a2208bfd0>,\n",
      " '시간': <gensim.models.keyedvectors.Vocab object at 0x1a2208beb8>,\n",
      " '아무': <gensim.models.keyedvectors.Vocab object at 0x1a25a6ae10>,\n",
      " '어른들': <gensim.models.keyedvectors.Vocab object at 0x1a2208b320>,\n",
      " '있어': <gensim.models.keyedvectors.Vocab object at 0x1a25a6af60>,\n",
      " '자꾸': <gensim.models.keyedvectors.Vocab object at 0x1a2208bc50>,\n",
      " '전부': <gensim.models.keyedvectors.Vocab object at 0x1a25a6af28>,\n",
      " '준비': <gensim.models.keyedvectors.Vocab object at 0x1a2208b978>,\n",
      " '특별': <gensim.models.keyedvectors.Vocab object at 0x1a2208be48>,\n",
      " '하루': <gensim.models.keyedvectors.Vocab object at 0x1a2208bcf8>}\n"
     ]
    }
   ],
   "source": [
    "# 한국어는 한 단어가 자음모음 복합이라서, 원하던 효과가 나오지 않는다.  \n",
    "pprint(ft_ko.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-24 03:25:45,313 : INFO : starting tSNE dimensionality reduction. This may take some time.\n",
      "2019-04-24 03:25:45,458 : INFO : All done. Plotting.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": false
       },
       "data": [
        {
         "mode": "text",
         "text": [
          "네",
          "꿈",
          "아무",
          "전부",
          "있어",
          "멀리",
          "하루",
          "말",
          "준비",
          "벌써",
          "너",
          "거짓말",
          "자꾸",
          "매일",
          "어른들",
          "시간",
          "특별",
          "꿔"
         ],
         "type": "scatter",
         "uid": "3e66ae83-c9e7-441f-ae30-96db2f99165e",
         "x": [
          22.396421432495117,
          -65.58354187011719,
          0.5146400928497314,
          -34.7031135559082,
          79.27902221679688,
          -26.24057960510254,
          137.841552734375,
          -89.54383087158203,
          22.839174270629883,
          58.50896072387695,
          127.87554168701172,
          112.42582702636719,
          -9.222892761230469,
          -2.35579776763916,
          -65.41390228271484,
          52.657928466796875,
          69.10633087158203,
          72.5157470703125
         ],
         "y": [
          90.00062561035156,
          -35.551170349121094,
          -22.689443588256836,
          16.578216552734375,
          23.947418212890625,
          69.06417083740234,
          24.961015701293945,
          38.73554992675781,
          32.52923583984375,
          -78.93362426757812,
          90.6126708984375,
          -35.452335357666016,
          -79.55457305908203,
          142.4522247314453,
          110.04637908935547,
          -20.931554794311523,
          136.51776123046875,
          74.92880249023438
         ]
        }
       ],
       "layout": {}
      },
      "text/html": [
       "<div id=\"984dd039-3b68-41b0-8ea9-1596f752b0d9\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";\n",
       "if (document.getElementById(\"984dd039-3b68-41b0-8ea9-1596f752b0d9\")) {\n",
       "    Plotly.newPlot(\"984dd039-3b68-41b0-8ea9-1596f752b0d9\", [{\"mode\": \"text\", \"text\": [\"\\ub124\", \"\\uafc8\", \"\\uc544\\ubb34\", \"\\uc804\\ubd80\", \"\\uc788\\uc5b4\", \"\\uba40\\ub9ac\", \"\\ud558\\ub8e8\", \"\\ub9d0\", \"\\uc900\\ube44\", \"\\ubc8c\\uc368\", \"\\ub108\", \"\\uac70\\uc9d3\\ub9d0\", \"\\uc790\\uafb8\", \"\\ub9e4\\uc77c\", \"\\uc5b4\\ub978\\ub4e4\", \"\\uc2dc\\uac04\", \"\\ud2b9\\ubcc4\", \"\\uafd4\"], \"x\": [22.396421432495117, -65.58354187011719, 0.5146400928497314, -34.7031135559082, 79.27902221679688, -26.24057960510254, 137.841552734375, -89.54383087158203, 22.839174270629883, 58.50896072387695, 127.87554168701172, 112.42582702636719, -9.222892761230469, -2.35579776763916, -65.41390228271484, 52.657928466796875, 69.10633087158203, 72.5157470703125], \"y\": [90.00062561035156, -35.551170349121094, -22.689443588256836, 16.578216552734375, 23.947418212890625, 69.06417083740234, 24.961015701293945, 38.73554992675781, 32.52923583984375, -78.93362426757812, 90.6126708984375, -35.452335357666016, -79.55457305908203, 142.4522247314453, 110.04637908935547, -20.931554794311523, 136.51776123046875, 74.92880249023438], \"type\": \"scatter\", \"uid\": \"a59dac51-5769-4bb4-932c-76eeb6934913\"}], {}, {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"}); \n",
       "}\n",
       "});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){if (document.getElementById(\"984dd039-3b68-41b0-8ea9-1596f752b0d9\")) {window._Plotly.Plots.resize(document.getElementById(\"984dd039-3b68-41b0-8ea9-1596f752b0d9\"));};})</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"984dd039-3b68-41b0-8ea9-1596f752b0d9\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";\n",
       "if (document.getElementById(\"984dd039-3b68-41b0-8ea9-1596f752b0d9\")) {\n",
       "    Plotly.newPlot(\"984dd039-3b68-41b0-8ea9-1596f752b0d9\", [{\"mode\": \"text\", \"text\": [\"\\ub124\", \"\\uafc8\", \"\\uc544\\ubb34\", \"\\uc804\\ubd80\", \"\\uc788\\uc5b4\", \"\\uba40\\ub9ac\", \"\\ud558\\ub8e8\", \"\\ub9d0\", \"\\uc900\\ube44\", \"\\ubc8c\\uc368\", \"\\ub108\", \"\\uac70\\uc9d3\\ub9d0\", \"\\uc790\\uafb8\", \"\\ub9e4\\uc77c\", \"\\uc5b4\\ub978\\ub4e4\", \"\\uc2dc\\uac04\", \"\\ud2b9\\ubcc4\", \"\\uafd4\"], \"x\": [22.396421432495117, -65.58354187011719, 0.5146400928497314, -34.7031135559082, 79.27902221679688, -26.24057960510254, 137.841552734375, -89.54383087158203, 22.839174270629883, 58.50896072387695, 127.87554168701172, 112.42582702636719, -9.222892761230469, -2.35579776763916, -65.41390228271484, 52.657928466796875, 69.10633087158203, 72.5157470703125], \"y\": [90.00062561035156, -35.551170349121094, -22.689443588256836, 16.578216552734375, 23.947418212890625, 69.06417083740234, 24.961015701293945, 38.73554992675781, 32.52923583984375, -78.93362426757812, 90.6126708984375, -35.452335357666016, -79.55457305908203, 142.4522247314453, 110.04637908935547, -20.931554794311523, 136.51776123046875, 74.92880249023438], \"type\": \"scatter\", \"uid\": \"a59dac51-5769-4bb4-932c-76eeb6934913\"}], {}, {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"}); \n",
       "}\n",
       "});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){if (document.getElementById(\"984dd039-3b68-41b0-8ea9-1596f752b0d9\")) {window._Plotly.Plots.resize(document.getElementById(\"984dd039-3b68-41b0-8ea9-1596f752b0d9\"));};})</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels = [word for word in ft_ko.wv.vocab]\n",
    "vectors = [ft_ko.wv[word] for word in labels]\n",
    "\n",
    "visualize_embeddings(vectors, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 초성/중성/종성으로 분해하는 코드\n",
    "from soynlp.hangle import decompose, compose\n",
    "\n",
    "def encode(s):\n",
    "    def process(c):\n",
    "        if c == ' ':\n",
    "            return c\n",
    "        jamo = decompose(c)\n",
    "        # 'a' or 모음 or 자음\n",
    "        if (jamo is None) or (jamo[0] == ' ') or (jamo[1] == ' '):\n",
    "            return ' '\n",
    "        base = jamo[0]+jamo[1]\n",
    "        if jamo[2] == ' ':\n",
    "            return base + '-'\n",
    "        return base + jamo[2]\n",
    "\n",
    "    s = ''.join(process(c) for c in s)\n",
    "    return s.strip() \n",
    "\n",
    "def decode(s):\n",
    "    def process(t):\n",
    "        assert len(t) % 3 == 0\n",
    "        t_ = t.replace('-', ' ')\n",
    "        chars = [tuple(t_[3*i:3*(i+1)]) for i in range(len(t_)//3)]\n",
    "        recovered = [compose(*char) for char in chars]\n",
    "        recovered = ''.join(recovered)\n",
    "        return recovered\n",
    "\n",
    "    return ' '.join(process(t) for t in s.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ㄱㅏ-ㄴㅏ-ㄷㅏ-ㄹㅏㄹ            ㅎㅏ-ㅎㅏㅅ\n",
      "가나다랄 하핫\n"
     ]
    }
   ],
   "source": [
    "s = '가나다랄  a2ㅗㅛㅠ ㅋㅋㅋ 하핫'\n",
    "print(encode(s))\n",
    "print(decode(encode(s)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 단위로 입력 코퍼스를 바꿔야 한다.\n",
    "# 초/중/종성을 분리해야 한다.\n",
    "\n",
    "def tokenize(x, tokenizer) :\n",
    "    for token in tokenizer.tokenize(x) :\n",
    "        if nouns.get(token) :\n",
    "            token_decom = encode(token)\n",
    "            if len(token_decom) :\n",
    "                yield token_decom\n",
    "            \n",
    "sentences_ko = []\n",
    "for doc in documents_ko :\n",
    "    sentences = [sent for sent in doc.split('\\n') if len(sent)]\n",
    "    tokens = [list(tokenize(sent, tokenizer)) for sent in sentence]\n",
    "    tokens_decom = [token for token in tokens if len(token)]\n",
    "    sentences_ko.extend(tokens_decom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['ㄴㅔ-'], ['ㄴㅔ-']]\n"
     ]
    }
   ],
   "source": [
    "print(sentences_ko[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-24 03:26:02,482 : INFO : collecting all words and their counts\n",
      "2019-04-24 03:26:02,483 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-04-24 03:26:02,484 : INFO : collected 18 word types from a corpus of 441 raw words and 378 sentences\n",
      "2019-04-24 03:26:02,484 : INFO : Loading a fresh vocabulary\n",
      "2019-04-24 03:26:02,485 : INFO : min_count=1 retains 18 unique words (100% of original 18, drops 0)\n",
      "2019-04-24 03:26:02,486 : INFO : min_count=1 leaves 441 word corpus (100% of original 441, drops 0)\n",
      "2019-04-24 03:26:02,486 : INFO : deleting the raw counts dictionary of 18 items\n",
      "2019-04-24 03:26:02,487 : INFO : sample=0.001 downsamples 18 most-common words\n",
      "2019-04-24 03:26:02,488 : INFO : downsampling leaves estimated 57 word corpus (13.1% of prior 441)\n",
      "2019-04-24 03:26:02,491 : INFO : estimated required memory for 18 words, 278 buckets and 4 dimensions: 17192 bytes\n",
      "2019-04-24 03:26:02,494 : INFO : resetting layer weights\n",
      "2019-04-24 03:26:02,505 : INFO : Total number of ngrams is 278\n",
      "2019-04-24 03:26:02,512 : INFO : training model with 3 workers on 18 vocabulary and 4 features, using sg=0 hs=0 sample=0.001 negative=5 window=3\n",
      "2019-04-24 03:26:02,516 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-24 03:26:02,517 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-24 03:26:02,518 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-24 03:26:02,518 : INFO : EPOCH - 1 : training on 441 raw words (53 effective words) took 0.0s, 16784 effective words/s\n",
      "2019-04-24 03:26:02,521 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-24 03:26:02,522 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-24 03:26:02,523 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-24 03:26:02,524 : INFO : EPOCH - 2 : training on 441 raw words (50 effective words) took 0.0s, 14271 effective words/s\n",
      "2019-04-24 03:26:02,527 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-24 03:26:02,528 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-24 03:26:02,529 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-24 03:26:02,530 : INFO : EPOCH - 3 : training on 441 raw words (51 effective words) took 0.0s, 14659 effective words/s\n",
      "2019-04-24 03:26:02,534 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-24 03:26:02,534 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-24 03:26:02,535 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-24 03:26:02,536 : INFO : EPOCH - 4 : training on 441 raw words (56 effective words) took 0.0s, 17464 effective words/s\n",
      "2019-04-24 03:26:02,539 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-24 03:26:02,540 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-24 03:26:02,541 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-24 03:26:02,542 : INFO : EPOCH - 5 : training on 441 raw words (64 effective words) took 0.0s, 22600 effective words/s\n",
      "2019-04-24 03:26:02,545 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-24 03:26:02,546 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-24 03:26:02,547 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-24 03:26:02,548 : INFO : EPOCH - 6 : training on 441 raw words (62 effective words) took 0.0s, 18607 effective words/s\n",
      "2019-04-24 03:26:02,551 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-24 03:26:02,552 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-24 03:26:02,553 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-24 03:26:02,554 : INFO : EPOCH - 7 : training on 441 raw words (50 effective words) took 0.0s, 20407 effective words/s\n",
      "2019-04-24 03:26:02,557 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-24 03:26:02,558 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-24 03:26:02,559 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-24 03:26:02,559 : INFO : EPOCH - 8 : training on 441 raw words (44 effective words) took 0.0s, 15159 effective words/s\n",
      "2019-04-24 03:26:02,562 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-24 03:26:02,563 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-24 03:26:02,564 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-24 03:26:02,565 : INFO : EPOCH - 9 : training on 441 raw words (50 effective words) took 0.0s, 14691 effective words/s\n",
      "2019-04-24 03:26:02,568 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-24 03:26:02,569 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-24 03:26:02,570 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-24 03:26:02,570 : INFO : EPOCH - 10 : training on 441 raw words (65 effective words) took 0.0s, 21453 effective words/s\n",
      "2019-04-24 03:26:02,571 : INFO : training on a 4410 raw words (545 effective words) took 0.1s, 9387 effective words/s\n",
      "2019-04-24 03:26:02,572 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import FastText\n",
    "\n",
    "ft_ko = FastText(size=4, window=3, min_count=1)\n",
    "ft_ko.build_vocab(sentences_ko)\n",
    "ft_ko.train(sentences=sentences_ko, total_examples=len(sentences_ko), epochs=10)  # train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ㄱㅓ-ㅈㅣㅅㅁㅏㄹ': <gensim.models.keyedvectors.Vocab object at 0x1a2208be10>,\n",
      " 'ㄲㅜㅁ': <gensim.models.keyedvectors.Vocab object at 0x1a25a5f3c8>,\n",
      " 'ㄲㅝ-': <gensim.models.keyedvectors.Vocab object at 0x1a2288bb38>,\n",
      " 'ㄴㅓ-': <gensim.models.keyedvectors.Vocab object at 0x1a2208bac8>,\n",
      " 'ㄴㅔ-': <gensim.models.keyedvectors.Vocab object at 0x1a25a5f400>,\n",
      " 'ㅁㅏㄹ': <gensim.models.keyedvectors.Vocab object at 0x1a2208bb38>,\n",
      " 'ㅁㅐ-ㅇㅣㄹ': <gensim.models.keyedvectors.Vocab object at 0x1a2288b9e8>,\n",
      " 'ㅁㅓㄹㄹㅣ-': <gensim.models.keyedvectors.Vocab object at 0x1a2208bbe0>,\n",
      " 'ㅂㅓㄹㅆㅓ-': <gensim.models.keyedvectors.Vocab object at 0x1a2208bcf8>,\n",
      " 'ㅅㅣ-ㄱㅏㄴ': <gensim.models.keyedvectors.Vocab object at 0x1a2288b668>,\n",
      " 'ㅇㅏ-ㅁㅜ-': <gensim.models.keyedvectors.Vocab object at 0x1a25786518>,\n",
      " 'ㅇㅓ-ㄹㅡㄴㄷㅡㄹ': <gensim.models.keyedvectors.Vocab object at 0x1a2288bd68>,\n",
      " 'ㅇㅣㅆㅇㅓ-': <gensim.models.keyedvectors.Vocab object at 0x1a2208be48>,\n",
      " 'ㅈㅏ-ㄲㅜ-': <gensim.models.keyedvectors.Vocab object at 0x1a2288b748>,\n",
      " 'ㅈㅓㄴㅂㅜ-': <gensim.models.keyedvectors.Vocab object at 0x1a2208be80>,\n",
      " 'ㅈㅜㄴㅂㅣ-': <gensim.models.keyedvectors.Vocab object at 0x1a2208bfd0>,\n",
      " 'ㅌㅡㄱㅂㅕㄹ': <gensim.models.keyedvectors.Vocab object at 0x1a2288bc50>,\n",
      " 'ㅎㅏ-ㄹㅜ-': <gensim.models.keyedvectors.Vocab object at 0x1a2208b978>}\n"
     ]
    }
   ],
   "source": [
    "# 초/중/종성을 분리된 단어들이 벡터화 됨.\n",
    "pprint(ft_ko.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-24 03:26:09,046 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-04-24 03:26:09,047 : INFO : precomputing L2-norms of ngram weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('특별', 0.9261425733566284)\n",
      "('멀리', 0.7501364350318909)\n",
      "('어른들', 0.4801279902458191)\n",
      "('거짓말', 0.3854236304759979)\n",
      "('매일', 0.1696784645318985)\n",
      "('준비', 0.14507009088993073)\n",
      "('꿔', 0.027564167976379395)\n",
      "('전부', 0.009496230632066727)\n",
      "('시간', -0.0006869323551654816)\n",
      "('아무', -0.07786288857460022)\n"
     ]
    }
   ],
   "source": [
    "# FastText는 학습시 없었던 단어에 대해서도 계산해준다.\n",
    "rst = ft_ko.wv.most_similar(encode(\"특별히\"))\n",
    "for word, score in rst : \n",
    "    print((decode(word),score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('너', 0.7622981667518616)\n",
      "('꿈', 0.7212961912155151)\n",
      "('하루', 0.6398354172706604)\n",
      "('네', 0.5679073929786682)\n",
      "('말', 0.4501268267631531)\n",
      "('벌써', 0.41831713914871216)\n",
      "('있어', 0.3568355441093445)\n",
      "('자꾸', 0.15550968050956726)\n",
      "('전부', 0.02571043372154236)\n",
      "('아무', -0.02309088408946991)\n"
     ]
    }
   ],
   "source": [
    "# FastText는 학습시 없었던 단어에 대해서도 계산해준다.\n",
    "rst = ft_ko.wv.most_similar(encode(\"꿈을\"))\n",
    "for word, score in rst : \n",
    "    print((decode(word),score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-24 03:26:27,610 : INFO : starting tSNE dimensionality reduction. This may take some time.\n",
      "2019-04-24 03:26:27,752 : INFO : All done. Plotting.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": false
       },
       "data": [
        {
         "mode": "text",
         "text": [
          "네",
          "꿈",
          "아무",
          "전부",
          "있어",
          "멀리",
          "하루",
          "말",
          "준비",
          "벌써",
          "너",
          "거짓말",
          "자꾸",
          "매일",
          "어른들",
          "시간",
          "특별",
          "꿔"
         ],
         "type": "scatter",
         "uid": "065a9541-d7cd-465d-9300-a03327f0cf86",
         "x": [
          -16.048851013183594,
          28.809106826782227,
          -22.386123657226562,
          28.079519271850586,
          -3.885739326477051,
          5.370452880859375,
          -17.77536964416504,
          22.29291534423828,
          4.606944561004639,
          15.411092758178711,
          -36.851566314697266,
          32.44180679321289,
          5.679172039031982,
          48.21770477294922,
          6.096363544464111,
          -12.85997200012207,
          44.91941833496094,
          -34.820167541503906
         ],
         "y": [
          19.654041290283203,
          -21.439050674438477,
          46.24052810668945,
          18.343921661376953,
          34.96875762939453,
          13.954302787780762,
          -21.6877384185791,
          -0.28755494952201843,
          -8.012155532836914,
          33.17001724243164,
          23.92094612121582,
          46.58428955078125,
          -29.85264778137207,
          25.526296615600586,
          55.613059997558594,
          0.4270806908607483,
          -0.8587400913238525,
          -1.364003300666809
         ]
        }
       ],
       "layout": {}
      },
      "text/html": [
       "<div id=\"d63ef3ad-f6eb-4144-ac23-62b43b70b99e\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";\n",
       "if (document.getElementById(\"d63ef3ad-f6eb-4144-ac23-62b43b70b99e\")) {\n",
       "    Plotly.newPlot(\"d63ef3ad-f6eb-4144-ac23-62b43b70b99e\", [{\"mode\": \"text\", \"text\": [\"\\ub124\", \"\\uafc8\", \"\\uc544\\ubb34\", \"\\uc804\\ubd80\", \"\\uc788\\uc5b4\", \"\\uba40\\ub9ac\", \"\\ud558\\ub8e8\", \"\\ub9d0\", \"\\uc900\\ube44\", \"\\ubc8c\\uc368\", \"\\ub108\", \"\\uac70\\uc9d3\\ub9d0\", \"\\uc790\\uafb8\", \"\\ub9e4\\uc77c\", \"\\uc5b4\\ub978\\ub4e4\", \"\\uc2dc\\uac04\", \"\\ud2b9\\ubcc4\", \"\\uafd4\"], \"x\": [-16.048851013183594, 28.809106826782227, -22.386123657226562, 28.079519271850586, -3.885739326477051, 5.370452880859375, -17.77536964416504, 22.29291534423828, 4.606944561004639, 15.411092758178711, -36.851566314697266, 32.44180679321289, 5.679172039031982, 48.21770477294922, 6.096363544464111, -12.85997200012207, 44.91941833496094, -34.820167541503906], \"y\": [19.654041290283203, -21.439050674438477, 46.24052810668945, 18.343921661376953, 34.96875762939453, 13.954302787780762, -21.6877384185791, -0.28755494952201843, -8.012155532836914, 33.17001724243164, 23.92094612121582, 46.58428955078125, -29.85264778137207, 25.526296615600586, 55.613059997558594, 0.4270806908607483, -0.8587400913238525, -1.364003300666809], \"type\": \"scatter\", \"uid\": \"afe9f301-7058-4298-8781-108df44add7f\"}], {}, {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"}); \n",
       "}\n",
       "});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){if (document.getElementById(\"d63ef3ad-f6eb-4144-ac23-62b43b70b99e\")) {window._Plotly.Plots.resize(document.getElementById(\"d63ef3ad-f6eb-4144-ac23-62b43b70b99e\"));};})</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"d63ef3ad-f6eb-4144-ac23-62b43b70b99e\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";\n",
       "if (document.getElementById(\"d63ef3ad-f6eb-4144-ac23-62b43b70b99e\")) {\n",
       "    Plotly.newPlot(\"d63ef3ad-f6eb-4144-ac23-62b43b70b99e\", [{\"mode\": \"text\", \"text\": [\"\\ub124\", \"\\uafc8\", \"\\uc544\\ubb34\", \"\\uc804\\ubd80\", \"\\uc788\\uc5b4\", \"\\uba40\\ub9ac\", \"\\ud558\\ub8e8\", \"\\ub9d0\", \"\\uc900\\ube44\", \"\\ubc8c\\uc368\", \"\\ub108\", \"\\uac70\\uc9d3\\ub9d0\", \"\\uc790\\uafb8\", \"\\ub9e4\\uc77c\", \"\\uc5b4\\ub978\\ub4e4\", \"\\uc2dc\\uac04\", \"\\ud2b9\\ubcc4\", \"\\uafd4\"], \"x\": [-16.048851013183594, 28.809106826782227, -22.386123657226562, 28.079519271850586, -3.885739326477051, 5.370452880859375, -17.77536964416504, 22.29291534423828, 4.606944561004639, 15.411092758178711, -36.851566314697266, 32.44180679321289, 5.679172039031982, 48.21770477294922, 6.096363544464111, -12.85997200012207, 44.91941833496094, -34.820167541503906], \"y\": [19.654041290283203, -21.439050674438477, 46.24052810668945, 18.343921661376953, 34.96875762939453, 13.954302787780762, -21.6877384185791, -0.28755494952201843, -8.012155532836914, 33.17001724243164, 23.92094612121582, 46.58428955078125, -29.85264778137207, 25.526296615600586, 55.613059997558594, 0.4270806908607483, -0.8587400913238525, -1.364003300666809], \"type\": \"scatter\", \"uid\": \"afe9f301-7058-4298-8781-108df44add7f\"}], {}, {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"}); \n",
       "}\n",
       "});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){if (document.getElementById(\"d63ef3ad-f6eb-4144-ac23-62b43b70b99e\")) {window._Plotly.Plots.resize(document.getElementById(\"d63ef3ad-f6eb-4144-ac23-62b43b70b99e\"));};})</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels = [word for word in ft_ko.wv.vocab]\n",
    "vectors = [ft_ko.wv[word] for word in labels]\n",
    "\n",
    "d_labels = [decode(word) for word in labels]\n",
    "visualize_embeddings(vectors, d_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-24 03:26:39,815 : INFO : starting tSNE dimensionality reduction. This may take some time.\n",
      "2019-04-24 03:26:39,958 : INFO : All done. Plotting.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": false
       },
       "data": [
        {
         "mode": "text",
         "text": [
          "네",
          "꿈",
          "아무",
          "전부",
          "있어",
          "멀리",
          "하루",
          "말",
          "준비",
          "벌써",
          "너",
          "거짓말",
          "자꾸",
          "매일",
          "어른들",
          "시간",
          "특별",
          "꿔"
         ],
         "type": "scatter",
         "uid": "9d56fc2b-26c8-46cf-bc47-c6f4efc4fdd7",
         "x": [
          -139.33201599121094,
          -8.303340911865234,
          -100.01652526855469,
          94.89714813232422,
          229.22694396972656,
          187.9876708984375,
          -47.16599655151367,
          69.00191497802734,
          -11.249449729919434,
          -10.736228942871094,
          184.876708984375,
          143.365966796875,
          74.22679138183594,
          93.60678100585938,
          133.5697784423828,
          -12.989117622375488,
          -100.54280853271484,
          50.03727340698242
         ],
         "y": [
          33.07489776611328,
          -41.02456283569336,
          -67.20237731933594,
          214.87632751464844,
          38.90904235839844,
          -85.6405029296875,
          31.656824111938477,
          127.9787826538086,
          -134.89939880371094,
          105.32756805419922,
          152.0074920654297,
          -6.147220611572266,
          -52.31425857543945,
          -141.88914489746094,
          75.51778411865234,
          200.66246032714844,
          132.1851348876953,
          34.481380462646484
         ]
        }
       ],
       "layout": {}
      },
      "text/html": [
       "<div id=\"397f7823-33a7-448e-99d1-ddfb221143a7\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";\n",
       "if (document.getElementById(\"397f7823-33a7-448e-99d1-ddfb221143a7\")) {\n",
       "    Plotly.newPlot(\"397f7823-33a7-448e-99d1-ddfb221143a7\", [{\"mode\": \"text\", \"text\": [\"\\ub124\", \"\\uafc8\", \"\\uc544\\ubb34\", \"\\uc804\\ubd80\", \"\\uc788\\uc5b4\", \"\\uba40\\ub9ac\", \"\\ud558\\ub8e8\", \"\\ub9d0\", \"\\uc900\\ube44\", \"\\ubc8c\\uc368\", \"\\ub108\", \"\\uac70\\uc9d3\\ub9d0\", \"\\uc790\\uafb8\", \"\\ub9e4\\uc77c\", \"\\uc5b4\\ub978\\ub4e4\", \"\\uc2dc\\uac04\", \"\\ud2b9\\ubcc4\", \"\\uafd4\"], \"x\": [-139.33201599121094, -8.303340911865234, -100.01652526855469, 94.89714813232422, 229.22694396972656, 187.9876708984375, -47.16599655151367, 69.00191497802734, -11.249449729919434, -10.736228942871094, 184.876708984375, 143.365966796875, 74.22679138183594, 93.60678100585938, 133.5697784423828, -12.989117622375488, -100.54280853271484, 50.03727340698242], \"y\": [33.07489776611328, -41.02456283569336, -67.20237731933594, 214.87632751464844, 38.90904235839844, -85.6405029296875, 31.656824111938477, 127.9787826538086, -134.89939880371094, 105.32756805419922, 152.0074920654297, -6.147220611572266, -52.31425857543945, -141.88914489746094, 75.51778411865234, 200.66246032714844, 132.1851348876953, 34.481380462646484], \"type\": \"scatter\", \"uid\": \"1333dd03-c172-4e9f-89eb-607447ea3ceb\"}], {}, {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"}); \n",
       "}\n",
       "});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){if (document.getElementById(\"397f7823-33a7-448e-99d1-ddfb221143a7\")) {window._Plotly.Plots.resize(document.getElementById(\"397f7823-33a7-448e-99d1-ddfb221143a7\"));};})</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"397f7823-33a7-448e-99d1-ddfb221143a7\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";\n",
       "if (document.getElementById(\"397f7823-33a7-448e-99d1-ddfb221143a7\")) {\n",
       "    Plotly.newPlot(\"397f7823-33a7-448e-99d1-ddfb221143a7\", [{\"mode\": \"text\", \"text\": [\"\\ub124\", \"\\uafc8\", \"\\uc544\\ubb34\", \"\\uc804\\ubd80\", \"\\uc788\\uc5b4\", \"\\uba40\\ub9ac\", \"\\ud558\\ub8e8\", \"\\ub9d0\", \"\\uc900\\ube44\", \"\\ubc8c\\uc368\", \"\\ub108\", \"\\uac70\\uc9d3\\ub9d0\", \"\\uc790\\uafb8\", \"\\ub9e4\\uc77c\", \"\\uc5b4\\ub978\\ub4e4\", \"\\uc2dc\\uac04\", \"\\ud2b9\\ubcc4\", \"\\uafd4\"], \"x\": [-139.33201599121094, -8.303340911865234, -100.01652526855469, 94.89714813232422, 229.22694396972656, 187.9876708984375, -47.16599655151367, 69.00191497802734, -11.249449729919434, -10.736228942871094, 184.876708984375, 143.365966796875, 74.22679138183594, 93.60678100585938, 133.5697784423828, -12.989117622375488, -100.54280853271484, 50.03727340698242], \"y\": [33.07489776611328, -41.02456283569336, -67.20237731933594, 214.87632751464844, 38.90904235839844, -85.6405029296875, 31.656824111938477, 127.9787826538086, -134.89939880371094, 105.32756805419922, 152.0074920654297, -6.147220611572266, -52.31425857543945, -141.88914489746094, 75.51778411865234, 200.66246032714844, 132.1851348876953, 34.481380462646484], \"type\": \"scatter\", \"uid\": \"1333dd03-c172-4e9f-89eb-607447ea3ceb\"}], {}, {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"}); \n",
       "}\n",
       "});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){if (document.getElementById(\"397f7823-33a7-448e-99d1-ddfb221143a7\")) {window._Plotly.Plots.resize(document.getElementById(\"397f7823-33a7-448e-99d1-ddfb221143a7\"));};})</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Word2Vec도 다시 그려서 위의 그림과 비교해보자\n",
    "labels = [word for word in w2v_ko.wv.vocab]\n",
    "vectors = [w2v_ko.wv[word] for word in labels]\n",
    "\n",
    "visualize_embeddings(vectors, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Embeding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 참고자료 \n",
    "* Applied Text Analysis with Python: Enabling Language-Aware Data Products with Machine Learning - https://www.amazon.com/Applied-Text-Analysis-Python-Language-Aware/dp/1491963042/\n",
    "* Natural Language Processing - https://www.coursera.org/learn/language-processing / Main approaches in NLP\n",
    "* http://git.ajou.ac.kr/open-source-2018-spring/python_Korean_NLP/blob/master/README.md\n",
    "* https://github.com/lovit/soynlp\n",
    "* https://github.com/lovit/textmining-tutorial\n",
    "* https://github.com/lovit/textmining-tutorial/blob/master/topics/topic4_embedding/word_document_embedding.pdf\n",
    "* https://github.com/lovit/python_ml4nlp\n",
    "* https://github.com/lovit/python_ml4nlp/blob/master/day5_embedding_and_visualizing/day_5_4_fasttext_gensim.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
