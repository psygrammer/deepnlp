{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01.Text Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 차례\n",
    "* Frequency Vectors\n",
    "* One-Hot Encoding\n",
    "* Term Frequency-Inverse Document Frequency\n",
    "* Distributed Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://nbviewer.jupyter.org/github/psygrammer/psyml/blob/master/nlp_ml/ch01/figures/cap1.3.png\" width=600 />\n",
    "<img src=\"https://nbviewer.jupyter.org/github/psygrammer/psyml/blob/master/nlp_ml/ch01/figures/cap1.4.png\" width=600 />\n",
    "\n",
    "* 출처 - Applied Text Analysis with Python: Enabling Language-Aware Data Products with Machine Learning - https://www.amazon.com/Applied-Text-Analysis-Python-Language-Aware/dp/1491963042/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/main_nlp_01.png\" width=600 />\n",
    "<img src=\"figures/main_nlp_02.png\" width=600 />\n",
    "<img src=\"figures/main_nlp_03.png\" width=600 />\n",
    "<img src=\"figures/main_nlp_04.png\" width=600 />\n",
    "<img src=\"figures/main_nlp_05.png\" width=600 />\n",
    "\n",
    "* 출처 - Natural Language Processing - https://www.coursera.org/learn/language-processing - Main approaches in NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://nbviewer.jupyter.org/github/psygrammer/psyml/blob/master/nlp_ml/ch04/figures/cap01.png\" width=600 />\n",
    "\n",
    "* 출처 - Applied Text Analysis with Python: Enabling Language-Aware Data Products with Machine Learning - https://www.amazon.com/Applied-Text-Analysis-Python-Language-Aware/dp/1491963042/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://slideplayer.com/slide/5270778/17/images/10/Distributed+Word+Representation.jpg\" width=600 />\n",
    "\n",
    "* 출처 - https://slideplayer.com/slide/5270778/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frequency Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://nbviewer.jupyter.org/github/psygrammer/psyml/blob/master/nlp_ml/ch04/figures/cap02.png\" width=600 />\n",
    "\n",
    "* 출처 - Applied Text Analysis with Python: Enabling Language-Aware Data Products with Machine Learning - https://www.amazon.com/Applied-Text-Analysis-Python-Language-Aware/dp/1491963042/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gensim을 이용하려면 문서 목록인 documents를 corpus 클래스로 바꿔야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가상의 문서 4개\n",
    "documents = [\n",
    "    \"a b c a\",\n",
    "    \"c b c\",\n",
    "    \"b b a\",\n",
    "    \"a c c\",\n",
    "    \"c b a\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['a', 'b', 'c', 'a'],\n",
      " ['c', 'b', 'c'],\n",
      " ['b', 'b', 'a'],\n",
      " ['a', 'c', 'c'],\n",
      " ['c', 'b', 'a']]\n"
     ]
    }
   ],
   "source": [
    "# 단어(토큰) 단위로 분할\n",
    "def tokenize(x) :\n",
    "    for token in x.split() :\n",
    "        yield token\n",
    "        \n",
    "corpus = [list(tokenize(doc)) for doc in  documents]\n",
    "\n",
    "pprint(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary 객체. 구체적으로는 단어에 id 할당 (그 외에도 여러가지 기능은 튜토리얼 참조)\n",
    "id2word = gensim.corpora.Dictionary(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.corpora.dictionary.Dictionary at 0x1a174cd198>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 0, 'b': 1, 'c': 2}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2word.token2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = [id2word.doc2bow(doc) for doc in corpus]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 2), (1, 1), (2, 1)],\n",
      " [(1, 1), (2, 2)],\n",
      " [(0, 1), (1, 2)],\n",
      " [(0, 1), (2, 2)],\n",
      " [(0, 1), (1, 1), (2, 1)]]\n"
     ]
    }
   ],
   "source": [
    "pprint(vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://nbviewer.jupyter.org/github/psygrammer/psyml/blob/master/nlp_ml/ch04/figures/cap03.png\" width=600 />\n",
    "\n",
    "* 출처 - Applied Text Analysis with Python: Enabling Language-Aware Data Products with Machine Learning - https://www.amazon.com/Applied-Text-Analysis-Python-Language-Aware/dp/1491963042/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['a', 'b', 'c', 'a'],\n",
       " ['c', 'b', 'c'],\n",
       " ['b', 'b', 'a'],\n",
       " ['a', 'c', 'c'],\n",
       " ['c', 'b', 'a']]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 2), (1, 1), (2, 1)],\n",
       " [(1, 1), (2, 2)],\n",
       " [(0, 1), (1, 2)],\n",
       " [(0, 1), (2, 2)],\n",
       " [(0, 1), (1, 1), (2, 1)]]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[id2word.doc2bow(doc) for doc in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 2), (1, 1), (2, 1)],\n",
       " [(1, 1), (2, 2)],\n",
       " [(0, 1), (1, 2)],\n",
       " [(0, 1), (2, 2)],\n",
       " [(0, 1), (1, 1), (2, 1)]]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[token for token in id2word.doc2bow(doc)] \n",
    "                         for doc in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 2), (1, 1), (2, 1)],\n",
       " [(1, 1), (2, 2)],\n",
       " [(0, 1), (1, 2)],\n",
       " [(0, 1), (2, 2)],\n",
       " [(0, 1), (1, 1), (2, 1)]]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[(token[0], token[1]) for token in id2word.doc2bow(doc)] \n",
    "                         for doc in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = [[(token[0], 1) for token in id2word.doc2bow(doc)] \n",
    "                         for doc in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors  = np.array([\n",
    "        [(token[0], 1) for token in id2word.doc2bow(doc)]\n",
    "        for doc in corpus\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1)],\n",
       " [(1, 1), (2, 1)],\n",
       " [(0, 1), (1, 1)],\n",
       " [(0, 1), (2, 1)],\n",
       " [(0, 1), (1, 1), (2, 1)]]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Term Frequency-Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://nbviewer.jupyter.org/github/psygrammer/psyml/blob/master/nlp_ml/ch04/figures/cap04.png\" width=600 />\n",
    "\n",
    "* 출처 - Applied Text Analysis with Python: Enabling Language-Aware Data Products with Machine Learning - https://www.amazon.com/Applied-Text-Analysis-Python-Language-Aware/dp/1491963042/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a b c a', 'c b c', 'b b a', 'a c c', 'c b a']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['a', 'b', 'c', 'a'],\n",
       " ['c', 'b', 'c'],\n",
       " ['b', 'b', 'a'],\n",
       " ['a', 'c', 'c'],\n",
       " ['c', 'b', 'a']]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary 생성\n",
    "id2word = gensim.corpora.Dictionary(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.tfidfmodel.TfidfModel at 0x1a174d4940>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tfidf Model 생성\n",
    "tfidf = gensim.models.TfidfModel(dictionary=id2word, normalize=True)\n",
    "tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 0.816496580927726), (1, 0.408248290463863), (2, 0.408248290463863)],\n",
       " [(1, 0.447213595499958), (2, 0.894427190999916)],\n",
       " [(0, 0.447213595499958), (1, 0.894427190999916)],\n",
       " [(0, 0.447213595499958), (2, 0.894427190999916)],\n",
       " [(0, 0.5773502691896257), (1, 0.5773502691896257), (2, 0.5773502691896257)]]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors = [tfidf[id2word.doc2bow(vector)] for vector in corpus]\n",
    "vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary와 tfidf model을 저장해 놓으면 나중에 다시 로드해서 쓸 수 있어 편하다. \n",
    "id2word.save_as_text('test.txt')\n",
    "tfidf.save('tfidf.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01_text_features.ipynb  test.txt\r\n",
      "\u001b[34mfigures\u001b[m\u001b[m/                tfidf.pkl\r\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://nbviewer.jupyter.org/github/psygrammer/psyml/blob/master/nlp_ml/ch04/figures/cap05.png\" width=600 />\n",
    "\n",
    "* 출처 - Applied Text Analysis with Python: Enabling Language-Aware Data Products with Machine Learning - https://www.amazon.com/Applied-Text-Analysis-Python-Language-Aware/dp/1491963042/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 참고자료 \n",
    "* Applied Text Analysis with Python: Enabling Language-Aware Data Products with Machine Learning - https://www.amazon.com/Applied-Text-Analysis-Python-Language-Aware/dp/1491963042/\n",
    "* Natural Language Processing - https://www.coursera.org/learn/language-processing / Main approaches in NLP\n",
    "* http://git.ajou.ac.kr/open-source-2018-spring/python_Korean_NLP/blob/master/README.md\n",
    "* https://github.com/lovit/soynlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
